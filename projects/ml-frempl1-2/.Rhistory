knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(knitr)
if (knitr::is_latex_output()) {
knitr::opts_chunk$set(
fig.show = 'hide',
echo = TRUE,
warning=FALSE,
message = FALSE
)
} else {
knitr::opts_chunk$set(
warning=FALSE,
message = FALSE
)
}
theme_custom <- function() {
theme_minimal() %+replace%
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_rect(colour = "black", fill=NA),
axis.ticks = element_line(),
#axis.line.x = element_line(color = "#C5C5C5"),
axis.line = element_line(color = "black"),
#axis.ticks = element_blank(),
legend.position = "bottom",
legend.title = element_blank(),
#plot.background = element_rect(fill='transparent', color=NA),
plot.title = element_text(             #title
#family = font,            #set font family
size = 16,                #set font size
face = 'bold',            #bold typeface
hjust = 0,                #left align
vjust = 2,
color = "black"),               #raise slightly
plot.subtitle = element_text(          #subtitle
#family = font,            #font family
size = 12,
hjust = 0,
color = "black"),               #font size
plot.caption = element_text(           #caption
#family = font,            #font family
face = "italic",
size = 8,                 #font size
hjust = 1,
color = "black"),               #right align
axis.title = element_text(             #axis titles
#family = font,            #font family
size = 12,
color = "black"),               #font size
axis.text = element_text(              #axis text
#family = font,            #axis famuly
size = 12,
color = "black"),                #font size
axis.text.x = element_text(            #margin for axis text
margin=margin(5, b = 10))
)
}
library(CASdatasets)
library(lattice)
library(evmix)
library(ggplot2)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(dbarts)
library(mlr3mbo)
library(mlr3measures)
library(mlr3tuning)
library(ranger)
library(mlr3viz)
library(fastDummies)
library(dplyr)
library(patchwork)
library(xgboost)
library(glex)
library(treeshap)
## Feature selection and formatting
data("freMPL1",package = "CASdatasets")
#1: RecordBeg and RecordEnd discarded
freMPL1 <- freMPL1 %>%
select(-RecordBeg,-RecordEnd)
#2: Add id's
freMPL1[,"ID"] <- 1:dim(freMPL1)[1]
#3: Claim amount and indicator
freMPL1$ClaimAmount[freMPL1$ClaimAmount<0] <- 0
freMPL1$ClaimInd <- ifelse(freMPL1$ClaimAmount>0,1,0)
#4: Electric or GPL vehicals
freMPL1 <- freMPL1 %>%
filter(!(VehEngine %in% c("electric","GPL")))
#5: Combining max speed levels
levels(freMPL1$VehMaxSpeed)[1:2] <- "1-140 kmh"
#6: Bus set to sedan
levels(freMPL1$VehBody)[levels(freMPL1$VehBody) == "bus"] <- "sedan"
#Create df
df <- freMPL1
#Catagorical variables
cat_variables <- c("Gender","VehAge","MariStat","SocioCateg",
"VehUsage","HasKmLimit","VehBody","VehPrice","VehEngine",
"VehEnergy","VehMaxSpeed","VehClass","RiskVar","Garage")
for (col in cat_variables) {
#Get ordering
ord <- freMPL1 %>%
#Group by distinct values
group_by(!!as.name(col)) %>%
#Calculate frequency and mean claim
summarise(Frequency = sum(ClaimInd)/sum(Exposure),
Severity = sum(ClaimAmount)/sum(ClaimAmount > 0)) %>%
ungroup() %>%
#Order Frequency
arrange(Frequency) %>%
#Insert 0,...,n
mutate(Frequency = 0:(length(unique(!!as.name(col)))-1)) %>%
#Order Severity
arrange(Severity) %>%
#Insert 0,...,n
mutate(Severity = 0:(length(unique(!!as.name(col)))-1))
#Prepare ord for merging
colnames(ord)[2:3] <- paste0(col, c("_Freq","_Sev"))
#Merge new columns into df
df <- df %>%
merge(., ord, all.x = TRUE)
}
#Get relevant columns
sev_columns <- colnames(df)[!grepl("_Freq",colnames(df)) &
!(colnames(df) %in% cat_variables)]
df_sev <- df[,sev_columns] %>%
filter(ClaimInd == 1) %>%
select(-ClaimInd)
#Save ID in row names
row.names(df_sev) <- df_sev$ID
df_sev <- df_sev %>%
select(-ID)%>%
mutate_if(is.integer, as.numeric)
#Start a task
task_sev <- df_sev %>%
mutate(ClaimAmount = log(ClaimAmount)) %>%
#Start tast with target ClaimAmound
as_task_regr(.,
target = "ClaimAmount",
id= "Severity")
### XGBoost
sev_xgb_learner <- lrn("regr.xgboost",
eta = to_tune(0, 0.5),
nrounds = to_tune(75, 5000),
max_depth = to_tune(1, 3))
### XGBoost
set.seed(20230328) #We choose a seed
# 1: Estimating hyperparameters with 5-fold cross validation
sev_xgb_learner_instance = tune(
#method = tnr("random_search"), ### tuning method
method = mlr3tuning::tnr("mbo"), ### tuning method
task = task_sev,
learner = sev_xgb_learner,
resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
measures = msr("regr.rmse"), #### Mean Squared Log Error
terminator = trm("evals", n_evals = 200) #### terminator
)
#Save the instance
saveRDS(sev_xgb_learner_instance, file = "rds/sev_xgb_learner_instance.rds")
knitr::opts_chunk$set(echo = TRUE)
rm(list = ls())
library(knitr)
if (knitr::is_latex_output()) {
knitr::opts_chunk$set(
fig.show = 'hide',
echo = TRUE,
warning=FALSE,
message = FALSE
)
} else {
knitr::opts_chunk$set(
warning=FALSE,
message = FALSE
)
}
theme_custom <- function() {
theme_minimal() %+replace%
theme(
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
panel.border = element_rect(colour = "black", fill=NA),
axis.ticks = element_line(),
#axis.line.x = element_line(color = "#C5C5C5"),
axis.line = element_line(color = "black"),
#axis.ticks = element_blank(),
legend.position = "bottom",
legend.title = element_blank(),
#plot.background = element_rect(fill='transparent', color=NA),
plot.title = element_text(             #title
#family = font,            #set font family
size = 16,                #set font size
face = 'bold',            #bold typeface
hjust = 0,                #left align
vjust = 2,
color = "black"),               #raise slightly
plot.subtitle = element_text(          #subtitle
#family = font,            #font family
size = 12,
hjust = 0,
color = "black"),               #font size
plot.caption = element_text(           #caption
#family = font,            #font family
face = "italic",
size = 8,                 #font size
hjust = 1,
color = "black"),               #right align
axis.title = element_text(             #axis titles
#family = font,            #font family
size = 12,
color = "black"),               #font size
axis.text = element_text(              #axis text
#family = font,            #axis famuly
size = 12,
color = "black"),                #font size
axis.text.x = element_text(            #margin for axis text
margin=margin(5, b = 10))
)
}
library(CASdatasets)
library(lattice)
library(evmix)
library(ggplot2)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(dbarts)
library(mlr3mbo)
library(mlr3measures)
library(mlr3tuning)
library(ranger)
library(mlr3viz)
library(fastDummies)
library(dplyr)
library(patchwork)
library(xgboost)
library(glex)
library(treeshap)
## Feature selection and formatting
data("freMPL1",package = "CASdatasets")
#1: RecordBeg and RecordEnd discarded
freMPL1 <- freMPL1 %>%
select(-RecordBeg,-RecordEnd)
#2: Add id's
freMPL1[,"ID"] <- 1:dim(freMPL1)[1]
#3: Claim amount and indicator
freMPL1$ClaimAmount[freMPL1$ClaimAmount<0] <- 0
freMPL1$ClaimInd <- ifelse(freMPL1$ClaimAmount>0,1,0)
#4: Electric or GPL vehicals
freMPL1 <- freMPL1 %>%
filter(!(VehEngine %in% c("electric","GPL")))
#5: Combining price categories
levels(freMPL1$VehPrice)[1:3] <- "A-C"
n <- length(levels(freMPL1$VehPrice))
levels(freMPL1$VehPrice)[(n-5):n] <- "U-Z"
n <- length(levels(freMPL1$VehPrice))
levels(freMPL1$VehPrice)[(n-3):(n-1)] <- "R-T"
#6: Combining max speed levels
levels(freMPL1$VehMaxSpeed)[1:2] <- "1-140 kmh"
#7: Bus set to sedan
levels(freMPL1$VehBody)[levels(freMPL1$VehBody) == "bus"] <- "sedan"
#8: SocioCateg change levels
freMPL1 <- freMPL1 %>%
#Get numerical value of SocioCateg
mutate(helper = as.numeric(substr(SocioCateg,4,5))) %>%
#Overwrite SocioCateg
mutate(SocioCateg = factor(ifelse(helper > 50, "C",
ifelse( helper < 50, "A",
"B")),
levels = c("A","B","C"))) %>%
select(-helper)
#Create df
df <- freMPL1
#Catagorical variables
cat_variables <- c("Gender","VehAge","MariStat","SocioCateg",
"VehUsage","HasKmLimit","VehBody","VehPrice","VehEngine",
"VehEnergy","VehMaxSpeed","VehClass","RiskVar","Garage")
for (col in cat_variables) {
#Get ordering
ord <- freMPL1 %>%
#Group by distinct values
group_by(!!as.name(col)) %>%
#Calculate frequency and mean claim
summarise(Frequency = sum(ClaimInd)/sum(Exposure),
Severity = sum(ClaimAmount)/sum(ClaimAmount > 0)) %>%
ungroup() %>%
#Order Frequency
arrange(Frequency) %>%
#Insert 0,...,n
mutate(Frequency = 0:(length(unique(!!as.name(col)))-1)) %>%
#Order Severity
arrange(Severity) %>%
#Insert 0,...,n
mutate(Severity = 0:(length(unique(!!as.name(col)))-1))
#Prepare ord for merging
colnames(ord)[2:3] <- paste0(col, c("_Freq","_Sev"))
#Merge new columns into df
df <- df %>%
merge(., ord, all.x = TRUE)
}
#Get relevant columns
sev_columns <- colnames(df)[!grepl("_Freq",colnames(df)) &
!(colnames(df) %in% cat_variables)]
df_sev <- df[,sev_columns] %>%
filter(ClaimInd == 1) %>%
select(-ClaimInd,-Exposure)
#Save ID in row names
row.names(df_sev) <- df_sev$ID
df_sev <- df_sev %>%
select(-ID)%>%
mutate_if(is.integer, as.numeric)
#Start a task
task_sev <- df_sev %>%
mutate(ClaimAmount = log(ClaimAmount)) %>%
#Start tast with target ClaimAmound
as_task_regr(.,
target = "ClaimAmount",
id= "Severity")
### XGBoost
sev_xgb_learner <- lrn("regr.xgboost",
eta = to_tune(0, 1),
nrounds = to_tune(75, 10000),
max_depth = to_tune(1, 2))
### XGBoost
set.seed(20230328) #We choose a seed
# 1: Estimating hyperparameters with 5-fold cross validation
sev_xgb_learner_instance = tune(
#method = tnr("random_search"), ### tuning method
method = mlr3tuning::tnr("mbo"), ### tuning method
task = task_sev,
learner = sev_xgb_learner,
resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
measures = msr("regr.rmse"), #### Mean Squared Log Error
terminator = trm("evals", n_evals = 500) #### terminator
)
sev_xgb_learner_instance$result_learner_param_vals
#Save the instance
saveRDS(sev_xgb_learner_instance, file = "rds/sev_xgb_learner_instance.rds")
task_sev <- df_sev %>%
mutate(ClaimAmount = ClaimAmount) %>%
#Start tast with target ClaimAmound
as_task_regr(.,
target = "ClaimAmount",
id= "Severity")
### XGBoost
sev_xgb_learner <- lrn("regr.xgboost",
eta = to_tune(0, 0.5),
nrounds = to_tune(75, 5000),
max_depth = to_tune(1, 2))
### XGBoost
set.seed(20230328) #We choose a seed
# 1: Estimating hyperparameters with 5-fold cross validation
sev_xgb_learner_instance = tune(
#method = tnr("random_search"), ### tuning method
method = mlr3tuning::tnr("mbo"), ### tuning method
task = task_sev,
learner = sev_xgb_learner,
resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
measures = msr("regr.mape"), #### Mean Squared Error
terminator = trm("evals", n_evals = 500) #### terminator
)
sev_xgb_learner_instance$result_learner_param_vals
#Start a task
task_sev <- df_sev %>%
#Start tast with target ClaimAmount
as_task_regr(.,
target = "ClaimAmount",
id= "Severity")
### XGBoost
sev_xgb_learner <- lrn("regr.xgboost",
eta = to_tune(0, 0.5),
nrounds = to_tune(75, 3000),
max_depth = to_tune(1, 2))
### XGBoost
set.seed(20230328) #We choose a seed
# 1: Estimating hyperparameters with 5-fold cross validation
sev_xgb_learner_instance = tune(
#method = tnr("random_search"), ### tuning method
method = mlr3tuning::tnr("mbo"), ### tuning method
task = task_sev,
learner = sev_xgb_learner,
resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
measures = msr("regr.rmse"), #### Root Mean Squared Log Error
terminator = trm("evals", n_evals = 1000) #### terminator
)
sev_xgb_learner_instance$result_learner_param_vals
#Save the instance
saveRDS(sev_xgb_learner_instance, file = "rds/sev_xgb_learner_instance.rds")
#Start a task
task_sev <- df_sev %>%
mutate(ClaimAmount = log(ClaimAmount)) %>%
#Start tast with target ClaimAmount
as_task_regr(.,
target = "ClaimAmount",
id= "Severity")
### XGBoost
sev_xgb_learner <- lrn("regr.xgboost",
eta = to_tune(0, 0.5),
nrounds = to_tune(75, 3000),
max_depth = to_tune(1, 2))
### XGBoost
set.seed(20230328) #We choose a seed
# 1: Estimating hyperparameters with 5-fold cross validation
sev_xgb_learner_instance = tune(
#method = tnr("random_search"), ### tuning method
method = mlr3tuning::tnr("mbo"), ### tuning method
task = task_sev,
learner = sev_xgb_learner,
resampling = rsmp("cv", folds = 5), #### resampling method: 5-fold cross validation
measures = msr("regr.rmse"), #### Root Mean Squared Log Error
terminator = trm("evals", n_evals = 1000) #### terminator
)
sev_xgb_learner_instance$result_learner_param_vals
