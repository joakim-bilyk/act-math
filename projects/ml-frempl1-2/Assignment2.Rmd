---
title: "Comparing and Intereting Machine Learning Algorithms Estimating Technical Prices"
author:
  - name: "Joakim Bilyk, Sebastian Cramer and Teis Blem"
    affiliation: "University of Copenhagen"
date: "`r Sys.Date()`"
abstract: "This document provides a practical example of applications of machine learning algorithms to car insurance data using the `mlr3` package. A popular model used in pricing of non-life insurance policies is the frequency-severity model, where the price is decomposed into the product of the probability of a claim arrises and the expected claim size given a claim occurs. This model is fitted using machine learning algorithms such as penalized linear regression and random forests. This paper argues that the ranger model gives a particular well fit to both the frequency and severity model."
keywords: "mlr3, machine learning, regression, non-life insurance, estimating technical price, XGBoost, Ranger, Bart, Elastic net regression, Generalized Additive Models"
header-includes:
  - \usepackage{subfig}
  - \usepackage{wrapfig}
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    keep_tex: yes
    template: template.tex
  html_document:
    code_download: yes
    theme: cosmo
    toc_float: yes
    toc: yes
    toc_depth: 2
    highlight: pygments
---
```{r}
library(CASdatasets)
library(lattice)
library(evmix)
library(ggplot2)
library(mlr3)
library(mlr3learners)
library(mlr3extralearners)
library(dbarts)
library(mlr3mbo)
library(mlr3measures)
library(mlr3tuning)
library(ranger)
library(mlr3viz)
library(fastDummies)
library(dplyr)
library(xgboost)
library(treeshap) #devtools::install_github('ModelOriented/treeshap')
## Feature selection and formatting
data("freMPL1",package = "CASdatasets")
#1: RecordBeg and RecordEnd discarded
freMPL1 <- freMPL1 %>%
select(-RecordBeg,-RecordEnd)
#1.1: Adding ID to rows
freMPL1 <- cbind(freMPL1,ID = seq(nrow(freMPL1)))
#2: Claim amount and indicator
freMPL1$ClaimAmount[freMPL1$ClaimAmount<0] <- 0
freMPL1$ClaimInd <- ifelse(freMPL1$ClaimAmount>0,1,0)
#3: Electric or GPL vehicals
freMPL1 <- freMPL1 %>%
filter(!(VehEngine %in% c("electric","GPL")))
#4: Combining price categories
levels(freMPL1$VehPrice)[1:3] <- "A-C"
n <- length(levels(freMPL1$VehPrice))
levels(freMPL1$VehPrice)[(n-5):n] <- "U-Z"
n <- length(levels(freMPL1$VehPrice))
levels(freMPL1$VehPrice)[(n-3):(n-1)] <- "R-T"
#5: Combining max speed levels
levels(freMPL1$VehMaxSpeed)[1:2] <- "1-140 kmh"
#6: Bus set to sedan
levels(freMPL1$VehBody)[levels(freMPL1$VehBody) == "bus"] <- "sedan"
#8: SocioCateg change levels
freMPL1 <- freMPL1 %>%
#Get numerical value of SocioCateg
mutate(helper = as.numeric(substr(SocioCateg,4,5))) %>%
#Overwrite SocioCateg
mutate(SocioCateg = factor(ifelse(helper > 50, "C",
ifelse( helper < 50, "A",
"B")),
levels = c("A","B","C"))) %>%
select(-helper)
```

First we set up the XGBoost learners with the hyperparameters tuned from the previous assignment.

```{r}
df <- freMPL1 %>%
  dummy_cols(.,select_columns = c("VehAge","MariStat","VehUsage",
                                  "VehBody","VehPrice","VehEngine",
                                  "VehEnergy","VehMaxSpeed","VehClass",
                                  "Garage","SocioCateg","RiskVar","Gender"),
             remove_selected_columns = TRUE)
colnames(df) <- make.names(colnames(df))

df_sev <- df %>% 
  filter(ClaimInd == 1) %>%
  select(-Exposure,-ID)

df_freq <- df %>%
  #Make sure not to include ClaimAmount
  select(-ClaimAmount,-ID)

matrix(as.numeric(unlist(df_freq)),ncol = dim(df_freq)[2])
xgb_regr <- xgboost(data=as.matrix(df_sev), label=as.numeric(df_sev[,"ClaimAmount"]), eta = 0.246407, nrounds = 379, max_depth = 3)
xgb_freq <- xgboost(data=as.matrix(df_freq), label=as.numeric(df_freq[,"ClaimInd"]), eta = 0.196547, nrounds = 1747, max_depth = 3, objective="count:poisson")
```




```{r}
unified_xgb <- xgboost.unify(xgb_regr,data=as.matrix(df_sev %>% select(-ClaimAmount)))

treeshap_xgb <- treeshap(unified_xgb,  df_sev %>% select(-ClaimAmount), verbose = 0)

plot_contribution(treeshap_xgb, obs = 6257)  
plot_feature_importance(treeshap_xgb, max_vars = 6) 
plot_feature_dependence(treeshap_xgb, "Exposure")
plot_feature_dependence(treeshap_xgb, "BonusMalus")
plot_feature_dependence(treeshap_xgb, "DrivAge")
```




