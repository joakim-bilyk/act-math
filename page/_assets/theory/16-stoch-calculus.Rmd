# Stochastic calculus

## Stochastic Integrals

We want to formulate financial markets in continuous time and the most elegant theory is obtained from processes that can be defined in terms of **stochastic differential equations**\index{stochastic differential equations, SDE} or in other words by their dynamics. We may call them **diffusion processes**\index{diffusion processes}, as they may be approximated by a stochastic difference equation:

$$
X_{t+\Delta t}-X_t=\mu(t,X_t)\Delta t+\sigma(t,X_t)Z_t.\tag{4.1}
$$

Above $Z_t$ is a normally distributed random variable (a disturbance). In this formulation we say that $S_t$ is driven by two forces: on one hand a locally deterministic velocity or drift $\mu(t,X_t)$ and on the other hand a Gaussian term amplified by the deterministic factor $\sigma(t,X_t)$.

### Information

We consider a primary process $X_t$ and we introduce the notion of information generated by $X_t$ in terms of the natural filtration. The idea can be summed up in the following definition.

<blockquote class = "def">

**Definition 4.3. (Bjork)** _The symbol $\mathcal{F}^X_t\subseteq\mathcal{F}$ denotes "the information generated by $X_t$ on the interval $[0,t]$", or alternatively "what has happened to $X_t$ over ther interval $[0,t]$"._

  1. _If, based upon observations of the trajectory $\{X_s;\ 0\le s\le t\}$, it is possible to decide whether a given event $A$ has occurred or not, then we write this as_
  $$
  A\in\mathcal{F}^X_t
  $$
  _or say that "$A$ is $\mathcal{F}^X_t$-measurable"._
  2. _If the value of a given random variable $Z$ can be completely determined given observations of the tragectory $\{X_s;\ 0\le s\le t\}$, then we also write_
  $$
  Z\in\mathcal{F}^X_t.\ \text{(}Z\text{ is }\mathcal{F}^X_t\text{-measurable)}
  $$
  3. _If $Y$ is a stochastic process such that we have_
  $$
  Y_t\in\mathcal{F}^X_t
  $$
  _for all $t\ge0$ then we say that $Y_t$ is **adapted** to the **filtration** $\{\mathcal{F}^X_t\}_{t\ge 0}$. For brevity of notation, we will sometimes write the filtration as $\{\mathcal{F}^X_t\}_{t\ge 0}=\mathbf{F}$._

</blockquote>

### Stochastic Integrals

We will now formulate the theory of stochastic integrals, that is, processes written in terms of stochastic processes with stochastic integrator and/or stochastic integrant. We will consider some given standard Brownian motion $W_t$ and another stochastic process $X_t$. We need som integrability condition on $X_t$ in order to do the calculations. We therefore determine a selection of suitable stochastic processes $X$ must be contained in.

<blockquote class = "def">

**Definition 4.4. (Bjork)** _Let $X_t$ be a stochastic process, then_

  i. _We say that $X_t$ belongs to the class $\mathcal{L}^2[a,b]$ if $X_t$ is adapted to the filtration $\mathcal{F}^X_t$ and the following holds_
  $$\int_a^bE[X_s^2]\ ds<\infty$$
  ii. _We say that $X_t$ belongs to the class $\mathcal{L}^2$ if_ $X_t\in\mathcal{L}^2[0,t]$ for all $t>0$.

</blockquote>

We now want to define what we mean by 

$$
\int_a^bX_t\ dW_s
$$

for some process $X_t\in\mathcal{L}^2$. We see that a way to go about this problem is to start by defining the concept for a simple stochastic process $X_t$. By *simple* we mean a process $X_t$ that is constant on between some deterministic points in time $a=t_0<t_1<\cdots<t_n=b$. In that case we may define the integral as

$$
\int_a^bX_s\ dW_s = \sum_{k=0}^{n-1}X_{t_k}[W_{t_{k+1}}-W_{t_k}].\tag{4.8}
$$

In the more general setting we may follow the following approach:

  1. Approximate $X$ with a sequence $\{X^n\}_{n\in\mathbb{N}}$ of simple processes such that the following convergence criteria hold
  $$
  \int_a^bE[(X_s^n-X_s)^2]\ ds\to 0,\ n\to\infty
  $$
  2. For each $n$ the integral $\int_a^b X_s^n\ dW_s:=Z^n$ is well defined and it is possible to prove, using DCT, that a variable $Z$ exists such that $Z^n\to Z$ that is in $L^2$.
  3. We now define the stochastic integral by the limit
  $$
  \int_a^b X_s\ dW_s=\lim_{n\to \infty}\int_a^b X_s^n\ dW_s.\tag{4.9}
  $$

Obviously the hardest step is finding the processes $X^n$. This stochastic har some properties we will use.

<blockquote class = "prop">

**Proposition 4.5. (Bjork)** _Let $X_t\in\mathcal{L}^2$, then_
\begin{align*}
&E\left[\int_a^b X_s\ dW_s\right]=0.\tag{4.12}\\
&E\left[\left(\int_a^b X_s\ dW_s\right)^2\right]=\int_a^b E[ X_s^2]\ dW_s.\tag{4.13}\\
&\int_a^b X_s\ dW_s\ \text{ is }\mathcal{F}_b^W\text{-measurable.}\tag{4.14}
\end{align*}
</blockquote>

### Martingales

<blockquote class = "def">

**Definition 4.7. (Bjork)** _Let $M_t$ be a stochastic process defined on a background space $(\Omega,\mathcal{F},P)$. Let $(\mathcal{F}_t)_{t\ge 0}$ be a filtration. If $M_t$ is adapted to the filtration $\mathcal{F}_t$, $E\vert M_t\vert <\infty$ and_

$$E[M_t\vert \mathcal{F}_s]=M_s,\hspace{20pt}P-\text{a.s.}$$

_holds for any $t>s$ we say that $M_t$ is a martingale ($\mathbf{F}$-martingale). If the above has $\ge$ or $\le$ we say that $M_t$ is either a **submartingale** or **supermartingale** respectively._

</blockquote>

<blockquote class = "prop">

**Proposition 4.8. (Bjork)** _For any process $X_t\in\mathcal{L}^2[s,t]$ the following hold:_

$$
E\left[\left.\int_s^t X_s\ dW_s\right\vert\mathcal{F}_s^W\right]=0
$$

</blockquote>

<blockquote class = "prop">

**Corollary 4.9. (Bjork)** _For any process $X_t\in\mathcal{L}^2$ the process_

$$
M_t=\int_s^t X_s\ dW_s,
$$

_is an $(\mathcal{F}_t^W)$-martingale. In other words, modulo an integrability condition, **every stochastic integral is a martingale**._

</blockquote>

<blockquote class = "lem">

**Lemma 4.10. (Bjork)** _Let $M_t$ be a stochastic process with stochastic differential, then $M_t$ is a martingale if and only if the stochastic differential has the form $dM_t=X_t\ dW_t$ i.e. $M_t$ as no $dt$-term._

</blockquote>

### Stochastic Calculus and the Ito Formula

Given this breif introduction to stochastic integrals we may formulate som simple calculus revolving around Ito's formula. We consider the stochastic process $X_t$ and we suppose that there exist a real number $X_0$ and adapted processes $\mu$ and $\sigma$ wrt. $\mathcal{F}_t^W$ such that for all $t\ge0$ we have

$$
X_t=X_0+\int_0^t\mu_s\ ds+\int_0^t\sigma_s\ dW_s,\tag{4.16}
$$

where $W_t$ is a standard Brownian motion. We know from earlier courses that the above may be written in terms of the dynamics (pure notation):

$$
\left\{\begin{matrix}dX_t=\mu_t\ dt+\sigma_t\ dW_t,\tag{4.17/18}\\ X_0=X_0.\end{matrix}\right.
$$

Here we intepret the above as $X_t$ has boundary condition $X_0$ and evolves with a drift $\mu_t\ dt$ amplified and distorted by the drift $\sigma_t\ dW_t$. We say that $X_t$ has **stochastic differential**\index{stochastic differential} $dX_t$ and initial condition $X_0$.

We want to understand how transformation of such an integral behaves and therefore we introduce some calculus which will tell how for instance $f(t,X_t)$ (for some $C^{1,2}$-function) behaves. This insight is given by the important Ito's formula.

<blockquote class = "thm">

**Thoerem 4.11. (Bjork)** **(Ito's formula, one-dimensional)**\index{Ito's formula} _Assume that the process $X$ has a stochastic differential form given by_

$$
dX_t=\mu_t\ dt + \sigma_t\ dW_t,\tag{4.28}
$$

_where $\mu$ and $\sigma$ are adapted processes, and let $f:\mathbb{R}_+\times\mathbb{R}\to\mathbb{R}$ be a $C^{1,2}$-function. Define the process $Z$ by $Z_t=f(t,X_t)$. Then $Z$ has stochastic differential given by_

$$
df(t,X_t)=\left(\frac{\partial f}{\partial t}(t,X_t) + \mu_t\frac{\partial f}{\partial x}(t,X_t) + \frac{1}{2}\sigma^2_t\frac{\partial^2 f}{\partial x^2}(t,X_t)\right)\ dt+\sigma_t\frac{\partial f}{\partial x}(t,X_t)\ dW_t.\tag{4.29}
$$

</blockquote>

<blockquote class = "prop">

**Proposition 4.12. (Bjork)** **(Ito's formula, one-dimensional)** _With assumptions as in theorem 4.11, $df$ is given by_

$$
df=\frac{\partial f}{\partial t}\ dt + \frac{\partial f}{\partial x}\ dX_t + \frac{1}{2}\frac{\partial^2 f}{\partial x^2}\ (dX_t)^2,\tag{4.31}
$$

_where we use the following table_

$$
\left\{\begin{matrix}(dt)^2=0,\\ dt\cdot dW_t=0,\\ (dW_t)^2=dt.\end{matrix}\right.
$$

</blockquote>

<blockquote class = "lem">

**Lemma 4.18. (Bjork)** _Let $\sigma(t)$ be deterministic function of time and define the process $X$ by_

$$
X_t=\int_0^t \sigma(s)\ dW_s.\tag{4.37}
$$

_Then_

$$
X_t\sim\mathcal{N}\left(0,\int_0^t\sigma^2(s)\ ds\right).
$$

</blockquote>

### The multidimensional Ito Formula

Consider a vector process $X=(X^1,...,X^n)^\top$ where each component $X^i$ has stochastic differential

$$
d X_t^i=\mu_t^i\ dt+\sum_{j=1}^d\sigma^{ij}_t\ dW_t^j
$$

where $W^1,...,W^d$ is independent Brownian motions. Then we have respectively the drift vector process $\mu_t$ in $n$ dimensions, the vector Brownian motion in $d$ dimensions and a $n\times d$-dimensional **diffusion matrix** $\sigma_t$ given as below

$$
\mu_t=\begin{bmatrix}\mu^1_t\\ \vdots\\ \mu^n_t\end{bmatrix},\hspace{10pt}W_t=\begin{bmatrix}W^1_t\\ \vdots\\ W^d_t\end{bmatrix},\hspace{10pt}\sigma_t=\begin{bmatrix}\sigma^{11}_t & \cdots & \sigma^{1d}_t \\ \vdots & \ddots & \vdots\\ \sigma^{n1}_t &\cdots& \sigma^{nd}_t\end{bmatrix}.
$$

Given this we may write the dynamics of $X$ as

$$
d X_t=\mu_t\ dt+\sigma_t\ dW_t\in\mathbb{R}^n.
$$

Consider now a function $f:\mathbb{R}_+\times \mathbb{R}^n\to\mathbb{R}$ which is a $C^{1,2}$-mapping. We want to study the dynamics of the process

$$
Z_t=f(t,X_t).
$$

The dynamics is given in the multidimensional version of Ito's formula.

<blockquote class = "thm">

**Thoerem 4.19. (Bjork)** **(Ito's formula, multi-dimensional)**\index{Ito's formula, multi-dimensional} _Let $X$ be given as above. Then the following holds:_

  * _The process $f(t,X_t)$ has the stochastisc differential given by_
  $$
  df(t,X_t)=\left(\frac{\partial f}{\partial t}(t,X_t) + \sum_{i=1}^n\mu^i_t\frac{\partial f}{\partial x^i}(t,X_t) + \frac{1}{2}\sum_{i,j=1}^nC_t^{ij}\frac{\partial^2 f}{\partial x^i\partial x^j}(t,X_t)\right)\ dt+\sum_{i=1}^n\sigma^i_t\frac{\partial f}{\partial x^i}(t,X_t)\ dW_t.
  $$
  _Here the row vector $\sigma^i_t$ is the $i$'th row of the matrix $\sigma_t$ and the matrix $C$ is defined by $C=\sigma\sigma^\top$._
  * _Alternatively, the differential is given by the formula_
  $$
  df(t,X_t)=\frac{\partial f}{\partial t}(t,X_t)\ dt + \sum_{i=1}^n\frac{\partial f}{\partial x^i}(t,X_t)\ dX^i_t + \frac{1}{2}\sum_{i,j=1}^n\frac{\partial^2 f}{\partial x^i\partial x^j}(t,X_t)\ dX^i_tdX^j_t,
  $$
  _with the formal multiplication table_
  $$
  \left\{\begin{matrix}(dt)^2=0,\\  dt\cdot dW_t^i=0, & i = 1,...,d,\\ (dW_t^i)^2=dt, & i=1,...,d, \\ dW_t^i\cdot dW_t^i =0, & i\ne j.\end{matrix}\right.
  $$

</blockquote>

Obviously, one can write the differential in Ito's formula in many other ways including a matrix-wise version using the Hessian matrix $H_{ij}=\frac{\partial^2 f}{\partial x^i\partial x^j}$.

### The multidimensional Ito Formula with states

<blockquote class = "def">

**Definition. ($n$-dimensional Markov-jump process)** _Let $Z(t)$ be a Markov-jump process on a at most countable state space $E$. Let $\mathbf X(t)=(X_1(t),X_2(t),...,X_n(t))\in\mathcal X$ be a $n$-dimensional stochastic process. Let $f : \mathcal X\to \mathbb R$ be a twice differential function. We call the process_

$$
f^{Z(t)}(\mathbf X(t))=f^{Z(t)}(X_1(t),...,X_n(t))
$$

_a $n$-dimensional Markov-jump process on $E$._

</blockquote>

One could for instance have the 2 dimensional process $f^{Z(t)}(t,X(t))$ where $X$ has dynamics

$$
dX(t)=rX(t)\ dt + \sigma X(t)\ dW^\mathbb Q(t)
$$

i.e. a geometric brownian motion. For such af process $f$ we have the change of variables formula

<blockquote class = "thm">

**Theorem. (Ito's formula for $n$-dimensional Markov-jump process)** _Let $f^{Z(t)}(\mathbf X(t))$ be a $n$-dimensional Markov-jump process on $E$. The dynamics of $f$ is given by_
\begin{align*}
d \Big(f^{Z(t)}(\mathbf X(t))\Big)&=\sum_{i=1}^n f_{x_i}^{Z(t)}(\mathbf X(t))\ dX_i^c(t)\\
&+\frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n f_{x_ix_j}^{Z(t)}(\mathbf X(t))\ d(X_iX_j)^c(t)\\
&+\Big(f^{Z(t)}(\mathbf X(t))-f^{Z(t-)}(\mathbf X(t-))\Big)dN^Z(t)\\
&+\Big(f^{Z(t)}(\mathbf X(t))-f^{Z(t)}(\mathbf X(t-))\Big)dN^\mathbf X(t)
\end{align*}
_where $f^{Z(t)}_{x_i}$ denotes the derivative $\frac{\partial f^{Z(t)}}{\partial x_i}$ and $f^{Z(t)}_{x_ix_j}$ denotes the second derivative $\frac{\partial^2 f^{Z(t)}}{\partial x_i\partial x_j}$. In the above $dX^c(t)$ refers to the continuous part of the differential form of $X(t)$. Furthermore, the quadratic variation is given on differential form_

$$
d(XY)(t)=X(t-)dY(t)+Y(t-)dX(t)+dX(t)dY(t)
$$

_where_

$$
d(XY)^c(t)=dX^c(t)dY^c(t).
$$

_The processes $N^Z$ and $N^\mathbf X$ is defined as_
\begin{align*}
N^Z(t)&=\#\left\{ 0\le s\le t: Z(s)\ne Z(s-) \right\},\\
N^\mathbf X(t)&=\#\left\{ 0\le s\le t: \mathbf X(s)\ne \mathbf X(s-) \right\}.
\end{align*}

</blockquote>

Take the example $X_1(t)=t$ and $X_2=X$ is a geometric brownian motions with jumps according to $Z$ and deterministic jumps given $Z$ i.e.
\begin{align*}
dX(t)&=rX(t)\ dt+\sigma X(t)\ dW^\mathbb Q(t)+\Delta X^{Z(t)}(t)\\
&+\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-)\Big)dN^{k}(t)
\end{align*}
Then we know that
\begin{align*}
dX^c(t)&=rX(t)\ dt+\sigma X(t)\ dW^\mathbb Q(t)\\
&+\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-)\Big)\lambda_{Z(t-)k}(t)\ dt
\end{align*}
Using that $dN^k(t)=dM^k(t)+\lambda_k(t)\ dt$ for some martingale process $M^k$ and the predictable compensator. Furthermore, we see that the only non-zero quadratic term is $dW^\mathbb Q(t)dW^\mathbb Q(t)=dt$ hence

$$
\frac{1}{2}\sum_{i,j=1}^2d(X_iX_j)(t)=\frac{1}{2}\sigma^2X^2(t)\ dt.
$$

Combining this with

$$
\Big(f^{Z(t)}(\mathbf X(t))-f^{Z(t)}(\mathbf X(t-))\Big)dN^\mathbf X(t)=\Delta X^{Z(t)}(t)
$$

we obtain the differential form
\begin{align*}
d \Big(f^{Z(t)}(t,X(t))\Big)&=f_t^{Z(t)}(t,X(t))\ dt+f_x^{Z(t)}(t,X(t))\ dX^c(t)\\
&+\frac{1}{2}f^{Z(t)}_{xx}(t,X(t))\sigma^2X^2(t)\ dt\\
&+\sum_{k:k\ne Z(t-)}\Big(f^{Z(t)}(t,X(t))-f^{Z(t-)}(t,X(t-))\Big)\ dN^k(t)\\
&+\Delta X^{Z(t)}(t)\\
&=f_t^{Z(t)}(t,X(t))\ dt+\frac{1}{2}f^{Z(t)}_{xx}(t,X(t))\sigma^2X^2(t)\ dt\\
&+f_x^{Z(t)}(t,X(t))rX(t)\ dt+f_x^{Z(t)}(t,X(t))\sigma X(t)\ dW^\mathbb Q(t)\\
&+f_x^{Z(t)}(t,X(t))\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-)\Big)\lambda_{Z(t-)k}(t)\ dt\\
&+\sum_{k:k\ne Z(t-)}\Big(f^{Z(t)}(t,\chi^k(t,X(t-)))-f^{Z(t-)}(t,X(t-))\Big)\ dN^k(t)\\
&+\Delta X^{Z(t)}(t)
\end{align*}
Notice we use that on the event $Z(t)=k$ and $Z(t-)\ne k$ the variable $X$ jumps to the value $\chi^k(t,X(t-))$. If we choose $f^j(t,x)=x$ for all $j\in E$ we see that the above simplifies to
\begin{align*}
d \Big(f^{Z(t)}(t,X(t))\Big)&=rX(t)\ dt+\sigma X(t)\ dW^\mathbb Q(t)\\
&+\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-)\Big)\lambda_{Z(t-)k}(t)\ dt\\
&+\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-))\ dN^k(t)\\
&+\Delta X^{Z(t)}(t)\ne dX(t)
\end{align*}
And so we are left with an extra term $\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-)\Big)\lambda_{Z(t-)k}(t)\ dt$, where we should simply have had $dX(t)$ that is without the term $\sum_{k:k\ne Z(t-)}\Big(\chi^k(t,X(t-))-X(t-)\Big)\lambda_{Z(t-)k}(t)\ dt$.


### Correlated Brownian motions

In the previous section the $d$-dimensional Brownian was assumed to have independent Brownian motions. However we may instead consider a variation where we have some dependence between the Brownian motions.

This section has not been finished.


## Discrete Stochastic Integrals

This section has not been finished.


## Stochastic Differential Equations

We start the chapter by formalising some used objects. We consider the following objects.

  * $M(n,d)$ denotes the class of $n\times d$-matrices.
  * $W$ is a $d$-dimensional Brownian motion
  * $\mu$ is a $\mathbb{R}^n$-valued function with arguments $(t,X_t)$ with $X_t$ being a $n$-dimensional stochastic process.
  * $\sigma$ a $M(n,d)$-valued function with arguments as in $\mu$.
  * $x_0$ a $\mathbb{R}^n$-valued vector.

We want then to understand when the following has a solution

$$
dX_t=\mu(t,X_t)\ dt + \sigma(t,X_t)\ dW_t,\ \ X_0=x_0.\tag{5.1/2}
$$

We call such an equation the **stochastic differential equation**\index{stochastic differential equation, SDE} or simply SDE. We know that the above is loosely notation for the integral form as

$$
X_t=x_0+\int_0^t\mu(s,X_s)\ ds +\int_0^t\sigma(s,X_s)\ dW_s,\tag{5.3}
$$

for all $t\ge 0$. The following proposition tells us when an solution exist to the problem above. In the below $\Vert \cdot \Vert$ is usual euclidian norm

$$
\Vert x\Vert=\sqrt{\sum_{i=1}^nx_i^2}.
$$

<blockquote class = "prop">

**Proposition 5.1. (Bjork)** _Suppose that there existis a constant $K$ such that the following conditions are satisfied for all $x,y$ and $t$._
\begin{align*}
\Vert \mu(t,x) - \mu(t,y) \Vert &\le K\Vert x-y\Vert,\tag{5.6}\\
\Vert \sigma(t,x) - \sigma(t,y) \Vert &\le K\Vert x-y\Vert,\tag{5.7}\\
\Vert \mu(t,x) \Vert +\Vert \sigma(t,x) \Vert&\le K(1+\Vert x\Vert).\tag{5.8}
\end{align*}
_Then there exists a unique solution to the SDE above. Furthermore, the solution has the properties_

  1. _$X$ is $\mathcal{F}_t^W$-adapted._
  2. _$X$ has continuous trajectories._
  3. _$X$ is a Markov process._
  4. _There exists a constant $C$ such that_
  $$
  E[\Vert X_t\Vert^2]\le Ce^{Ct}(1+\Vert x_0\Vert^2).\tag{5.9}
  $$

</blockquote>
  
In genereal the solution to an SDE is so complicated, that it in practical terms is unsolvable and may only be approximated on a finely subdividet grid as jumps. There does however exist som nontrivial cases where we may infer a analytical solution. One is the rather important **Geometric Brownian motion**\index{Geometric Brownian motion}.

<blockquote class = "prop">

**Proposition 5.2. (Bjork)** _Consider the SDE_

$$
dX_t=\alpha X_t\ dt+\sigma X_t\ dW_t,\tag{5.13}
$$

_with $X_0=x_0$. Then the solution is given as_

$$
X_t=x_0\cdot \exp\left\{\left(\alpha- \frac{\sigma^2}{2}\right)t+\sigma W_t\right\}.\tag{5.15}
$$

_The expected value of $X$ is given as $E[X_t]=x_0e^{\alpha t}$ (eq. 5.16)._

</blockquote>

One other generalisation that is analytically solvable is the Linear SDE\index{Linear SDE}.

<blockquote class = "prop">

**Proposition 5.3. (Bjork)** _Consider the SDE_

$$
dX_t=(A X_t + b_t)\ dt+ \sigma_t\ dW_t,\tag{5.19}
$$

_with $X_0=x_0$ and $A\in M(n,n)$ and $b_t$ being a real-valued function. Then the solution is given as_

$$
X_t=e^{At}x_0+\int_0^te^{A(t-s)}b_s\ ds+\int_0^te^{A(t-s)}\sigma_s\ dW_s.\tag{5.20}
$$

_Where we define the exponential of a matrix as below_

$$
e^{At}=\sum_{k=0}^\infty A^k\frac{1}{k!}t^k.
$$

</blockquote>

In general with the SDE we have a partial differential operator $\mathcal{A}$ called the **infinitesimal operator**\index{infinitesimal operator} of $X$ which has some interesting analytical properties regarding $X$.

<blockquote class = "def">

**Definition 5.4. (Bjork)** _Consider the SDE_

$$
dX_t=\mu(t,X_t)\ dt+\sigma(t,X_t)\ dW_t.\tag{5.21}
$$

_The partial differential operator $\mathcal{A}$ is defined, for any function $h\in C^2(\mathbb{R}^n)$, by_

$$
\mathcal{A}h(t,x)=\sum_{i=1}^n\mu_i(t,x)\frac{\partial h}{\partial x_i}(x) + \frac{1}{2}\sum_{i,j=1}^n (\sigma(t,x)\sigma(t,x)^\top)_{ij}\frac{\partial^2h}{\partial x_i\partial x_j}(x).
$$

</blockquote>

We see that in terms of Ito's formula the operator is included as such

$$
df(t,X_t)=\left\{\frac{\partial f}{\partial t}(t,X_t)+\mathcal{A}f(t,x)\right\}\ dt+[\nabla_xf](t,X_t)\sigma(t,X_t)\ dW_t,
$$

where $\nabla_x$ is the gradient for function $h\in C^1(\mathbb{R}^n)$ as

$$
\nabla_xh(x)=\left[\frac{\partial h}{\partial x_1}(x),...,\frac{\partial h}{\partial x_n}(x)\right].
$$

## Partial differential equations

<blockquote class = "prop">

**Proposition 5.5. (Bjork)** **(Feynmann-Kac)**\index{Feynmann-Kac} _Assume that $F$ is a solution to the boundary value problem_

$$
\frac{\partial F}{\partial t}(t,x)+\mu(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sigma^2(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)=0,
$$

_with boundary condition $F(T,x)=\Phi(x)$. Assume furthermore that the process_

$$
\sigma(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
$$

_as per definition 4.4, where $X$ is defined below. Then $F$ has the representation_

$$
F(t,x)=E_{t,x}[\Phi(X_T)]=E[\Phi(X_T)\ \vert\ X_t=x],\tag{5.29}
$$

_where $X$ satisfies the SDE_

$$
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,\tag{5.30}
$$

_with boundary condition $X_t=x$._

</blockquote>

<blockquote class = "prop">

**Proposition 5.6. (Bjork)** **(Feynmann-Kac)** _Assume that $F$ is a solution to the boundary value problem_

$$
\frac{\partial F}{\partial t}(t,x)+\mu(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sigma^2(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)-rF(t,x)=0,\tag{5.34}
$$

_with boundary condition $F(T,x)=\Phi(x)$. Assume furthermore that the process_

$$
e^{-rs}\sigma(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
$$

_as per definition 4.4, where $X$ is defined below. Then $F$ has the representation_

$$
F(t,x)=e^{-r(T-t)}E_{t,x}[\Phi(X_T)]=e^{-r(T-t)}E[\Phi(X_T)\ \vert\ X_t=x],\tag{5.36}
$$

_where $X$ satisfies the SDE_

$$
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,\tag{5.37}
$$

_with boundary condition $X_t=x$._

</blockquote>

<blockquote class = "prop">

**Proposition 5.8. (Bjork)** **(Feynmann-Kac)** _Assume that $F$ is a solution to the boundary value problem_

$$
\frac{\partial F}{\partial t}(t,x)+\sum_{i=1}^n\mu_i(t,x)\frac{\partial F}{\partial x}(x,t)+\frac{1}{2}\sum_{i,j=1}^n C_{ij}(t,x)\frac{\partial^2 F}{\partial x^2}(t,x)-rF(t,x)=0,
$$

_with boundary condition $F(T,x)=\Phi(x)$ and $C_{ij}=\sigma \sigma^\top$. Assume furthermore that the process_

$$
e^{-rs}\sum_{i=1}^n\sigma_i(s,X_s)\frac{\partial F}{\partial x}(s,X_s) \in \mathcal{L}^2
$$

_as per definition 4.4, where $X$ is defined below. Then $F$ has the representation_

$$
F(t,x)=e^{-r(T-t)}E_{t,x}[\Phi(X_T)],\tag{5.39}
$$

_where $X$ satisfies the SDE_

$$
dX_s=\mu(s,X_s)\ ds+\sigma(s,X_s)\ dW_s,\tag{5.40}
$$

_with boundary condition $X_t=x$._

</blockquote>

<blockquote class = "prop">

**Proposition 5.9. (Bjork)** _Consider as given a vector process $X$ with generator $\mathcal{A}$, and a function $F(t,x)$. Then, modulo some integrability condition, the following hold:_

  * _The process $F(t,X_t)$ is a martingale relative to the filtration $\mathcal{F}^X$ if and only if $F$ satisfies the PDE_
  $$
  \frac{\partial F}{\partial t}+\mathcal{A}F=0.
  $$
  * _The process $F(t,X_t)$ is a martingale relative to the filtration $\mathcal{F}^X$ if and only if, for every $(t,x)$ and $T\ge t$, we have_
  $$
  F(t,x)=E_{t,x}[F(T,X_T)].
  $$

</blockquote>

## The Product Integral

Consider the (stochastic) function $Y : \mathbb{R} \to \mathbb{R}^{n\times n}$, where $\mathbb{R}^{n\times n}$ is the set of all $n\times n$ real-valued matrices, that is $Y$ has the matrix-representation

$$
Y(t)=\begin{bmatrix}
Y_{11}(t) & \cdots & Y_{1n}(t)\\
Y_{21}(t) & \cdots & Y_{2n}(t)\\
\vdots & \ddots & \vdots \\
Y_{n1}(t) & \cdots & Y_{nn}(t)
\end{bmatrix}.
$$

Assume furthermore that for each coordinate function $Y_{ij}(t)$ the equation

$$
\frac{dY_{ij}}{dt}(t)=Y_{i1}(t)A_{1j}(t)+\cdots+Y_{in}(t)A_{nj}(t),\hspace{15pt}Y_{ij}(s)=C_{ij},
$$

is satisfied. That is on matrix form the linear system of differential equation

$$
\frac{dY}{dt}(t)=Y(t)A(t),\hspace{15pt}Y(s)=C,
$$

for some function $A : \mathbb{R}\to\mathbb{R}^{n\times n}$ and initial condition $C\in \mathbb{R}^{n\times n}$. If $A$ is continuous we know that $Y$ is well-defined and absolutely continuous. We may then state the following theorem regarding uniqueness and existence of such a function above.

<blockquote class = "thm">

**Theorem 1.1. (Bladt)** **(Uniqueness)**\index{solution to linear differential equation system, uniqueness} _Consider the homogeneous system of linear differential equations_

$$
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt} \mathbf{Y}(s)=\mathbf{C},\tag{1}
$$

_where $\mathbf{Y}(t)$, $\mathbf{A}(t)$ and $\mathbf{C}$ are $n\times n$-matrices and $\mathbf{A}(t)$ is continuous on $[s,t]$. Then (1) has at most one solution._

</blockquote>

<blockquote class = "thm">

**Theorem 1.2. (Bladt)** **(Existence)**\index{solution to linear differential equation system, existence} _The matrix function_

$$
\mathbf{Y}(t)=\sum_{k=0}^\infty \mathbf{Y}_k(t),\tag{3}
$$

_converges uniformly and absolutely on finite intervals, and solves the differential equation_

$$
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt} \mathbf{Y}(s)=\mathbf{C}.
$$

</blockquote>

Now, we know that for any system as in (1) with a continuous function $\mathbf{A}(t)$ we can always construct the solution as some converging series as per theorem 1.2 then theorem 1.1 gives that the solution is unique. We can then with a piece of mind define a symbol for such a solution, without care for the *exact* solution.

<blockquote class = "def">

**Definition 1.3. (Bladt)** **(The Product Integral)**\index{Product Integral} _For any continuous matrix function $\mathbf{A}(t)$ we define the product integral as_

$$
\prod_{s}^t(\mathbf{I}+\mathbf{A}(x)\ dx)
$$

_as the unique solution $\mathbf{Y}(t)$ to_

$$
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt} \mathbf{Y}(s)=\mathbf{I}.
$$

</blockquote>

From a simple integral argument we may construct *a* converging series not containing $\mathbf{Y}$ itself. We see that
\begin{align*}
\prod_{s}^t(\mathbf{I}+\mathbf{A}(x)\ dx)&=\mathbf{I}+\int_s^t\mathbf{Y}'(x)\ dx=\mathbf{I}+\int_s^t\mathbf{Y}(x)\mathbf{A}(x)\ dx\\
&=\mathbf{I}+\int_s^t\left[ \mathbf{I}+\int_s^{x_1}\mathbf{Y}'(x_2)\ dx_2\right]\mathbf{A}(x_1)\ dx_1\\
&=\mathbf{I}+\int_s^t\left[ \mathbf{I}+\int_s^{x_1}\mathbf{Y}(x_2)\mathbf{A}(x_2)\ dx_2\right]\mathbf{A}(x_1)\ dx_1\\
&=\mathbf{I}+\int_s^t\mathbf{A}(x_1)\ dx_1+\int_s^t\int_s^{x_1}\mathbf{Y}(x_2)\mathbf{A}(x_2)\mathbf{A}(x_1)\ dx_2\ dx_1.
\end{align*}
One can continue indefinitely and see that we have the following representation.

<blockquote class = "prop">

**Corollary 1.4. (Bladt)** **(Peano Representation)**\index{Peano Representation} _The prduct integral has series representation given by_
\begin{align*}
\prod_{s}^t(\mathbf{I}+\mathbf{A}(x)\ dx)&=\mathbf{I}+\sum_{i=1}^\infty\int_s^t\int_s^{x_1}\cdots\int_s^{x_n}A(x_n)\cdots\mathbf{A}(x_2)\mathbf{A}(x_1)\ dx_n\cdots dx_2\ dx_1\\
&=\mathbf{I}+\int_s^t\mathbf{A}(x_1)\ dx_1+\sum_{i=2}^\infty\int_s^t\int_s^{x_1}\cdots\int_s^{x_n}A(x_n)\cdots\mathbf{A}(x_2)\mathbf{A}(x_1)\ dx_n\cdots dx_2\ dx_1.
\end{align*}

</blockquote>

### Properties of the Product Integral

The product integral is the fundamental solution in the sense that if

$$
\mathbf{Y}'(t)=\mathbf{Y}(t)\mathbf{A}(t),\hspace{15pt}\mathbf{Y}(s)=\mathbf{C},
$$

then

$$
\mathbf{Y}(t)=\mathbf{C}\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big).
$$

We furthermore have for any $s,t,u$ have

$$
\mathbf{Y}(t)=\mathbf{Y}(s)\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\prod_{u}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big).
$$

From uniqueness we then get the following theorem.

<blockquote class = "thm">

**Theorem 1.5. (Bladt)** _For any $s,t,u$ we have that_

$$
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)=\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\prod_{u}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big).
$$

</blockquote>

By choosing $u=t$ in the above we get the following corollary.

<blockquote class = "prop">

**Corollary 1.6. (Bladt)** _The inverse of the product integral is_

$$
\left[\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\right]^{-1}=\prod_{t}^s\big(\mathbf{I}+\mathbf{A}(x)\ dx\big).
$$

</blockquote>

<blockquote class = "thm">

**Theorem 1.7. (Bladt)** _If $\mathbf{A}(x)$ commutes withall $\mathbf{B}(x=$ i.e. $\mathbf{A}(x)\mathbf{B}(x)=\mathbf{B}(x)\mathbf{A}(x)$, then_

$$
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\prod_{s}^t\big(\mathbf{I}+\mathbf{B}(x)\ dx\big)=\prod_{s}^t\big(\mathbf{I}+\left(\mathbf{A}(x)+\mathbf{B}(x)\right)\ dx\big).
$$

</blockquote>

<blockquote class = "prop">

**Corollary 1.8. (Bladt)** _Let $r$ be a real-valued function, then_

$$
\exp\left(-\int_s^t r(x)\ dx\right)\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)=\prod_{s}^t\big(\mathbf{I}+\left(\mathbf{A}(x)-r(x)\mathbf{I}\right)\ dx\big).
$$

</blockquote>

<blockquote class = "thm">

**Theorem 1.9. (Bladt)** _If $\mathbf{A}(x)$ commutes for all $x$ i.e. $\mathbf{A}(y)\mathbf{A}(x)=\mathbf{A}(x)\mathbf{A}(y)$, then_

$$
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)=e^{\int_s^t\mathbf{A}(x)\ dx}.
$$

_In particular, if $\mathbf{A}(x)=\mathbf{A}\lambda(x)$ for some real-valued function $\lambda$ and a $n\times n$ matrix $\mathbf{A}$ then_

$$
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)=e^{\mathbf{A}\int_s^t\lambda(x)\ dx}.
$$

</blockquote>

Taking the function $\lambda=1$ we have that $\int_s^t\lambda(x)\ dx=(t-s)$ and so we get the result.

<blockquote class = "thm">

**Theorem 1.10. (Bladt)** _If $\mathbf{A}(x)=\mathbf{A}$ is constant then_

$$
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)=e^{\mathbf{A}(t-s)}.
$$

</blockquote>

<blockquote class = "thm">

**Theorem 1.11. (Bladt)** _The product integral satisfies_

$$
\frac{\partial}{\partial s}\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)=-\mathbf{A}(s)\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big).
$$

_On the other hand, the solution to the system of differential equations_

$$
\frac{\partial}{\partial s}\mathbf{X}(s)=-\mathbf{A}(s)\mathbf{X}(s)
$$

_with initial condition $\mathbf{X}(t)=\mathbf{I}$ is the function_

$$
s\mapsto\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)
$$

</blockquote>

We have a very useful result for insurance mathematics and markov chains given by the Van Loan's theorem giving the product integral of a block matrix with zero lower left block.

<blockquote class = "thm">

**Theorem 1.12. (Bladt)** _Let $\mathbf{A}(x)$, $\mathbf{B}(x)$ and $\mathbf{C}(x)$ be continuous matrix functions such that_

$$
\mathbf{D}(x)=\begin{bmatrix}
\mathbf{A}(x) & \mathbf{B}(x)\\
\mathbf{0} & \mathbf{C}(x)
\end{bmatrix},
$$

_is a square matrix. Then the product integral of $\mathbf{D}$ is_

$$
\prod_{s}^t\big(\mathbf{I}+\mathbf{D}(x)\ dx\big)=
\begin{bmatrix}
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big) & \int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\\
\mathbf{0} & \prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)
\end{bmatrix}.
$$

</blockquote>

<details>
<summary>**Proof.**</summary>

Start by defining $\mathbf{X}(t)$ as below
\begin{align*}
\mathbf{X}(t)&=
\begin{bmatrix}
\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big) & \int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\\
\mathbf{0} & \prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)
\end{bmatrix}\\
&:=\begin{bmatrix}
\mathbf{X}_{11}(t) & \mathbf{X}_{12}(t)\\
\mathbf{X}_{21}(t) & \mathbf{X}_{22}(t)
\end{bmatrix}.
\end{align*}
The derivative is then
\begin{align*}
\frac{d}{dt}\mathbf{X}(t)&=
\begin{bmatrix}
\frac{d}{dt}\mathbf{X}_{11}(t) & \frac{d}{dt}\mathbf{X}_{12}(t)\\
\frac{d}{dt}\mathbf{X}_{21}(t) & \frac{d}{dt}\mathbf{X}_{22}(t)
\end{bmatrix}.
\end{align*}
with
\begin{align*}
\frac{d}{dt}\mathbf{X}_{11}(t)&=\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{A}(t),\\
\frac{d}{dt}\mathbf{X}_{12}(t)&= \frac{d}{dt}\int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du,\\
\frac{d}{dt}\mathbf{X}_{21}(t)&=\mathbf{0},\\
\frac{d}{dt}\mathbf{X}_{22}(t)&=\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\mathbf{C}(t).
\end{align*}
Consider that
\begin{align*}
\prod_{u}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)&=\prod_{u}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\underbrace{\prod_{t}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)}_{=\mathbf{I}}\\
&=\prod_{u}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big),
\end{align*}
hence using af Riemann approximation of the integral $\mathbf{X}_{12}(t)$ we may write
\begin{align*}
\mathbf{X}_{12}(t)&=\int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\\
&=\int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\\
&=\lim_{n\to \infty}\frac{t-s}{n}\sum_{i=0}^n\prod_{s}^{s+(t-s)i/n}\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(s+(t-s)i/n)\prod_{s+(t-s)i/n}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\\
&\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\\
&=\left\{\lim_{n\to \infty}\frac{t-s}{n}\sum_{i=0}^n\prod_{s}^{s+(t-s)i/n}\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(s+(t-s)i/n)\prod_{s+(t-s)i/n}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\right\}\\
&\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\\
&=\left\{\int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\right\}\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big).
\end{align*}
We then get that
\begin{align*}
\frac{d}{dt}\mathbf{X}_{12}(t)&=\frac{d}{dt}\left\{\int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\right\}\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\\
&=\left\{\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(t)\prod_{t}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\right\}\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\\
&+\left\{\int_s^t\prod_{s}^u\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(u)\prod_{u}^s\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\ du\right\}\prod_{s}^t\big(\mathbf{I}+\mathbf{C}(x)\ dx\big)\mathbf{C}(t)\\
&=\prod_{s}^t\big(\mathbf{I}+\mathbf{A}(x)\ dx\big)\mathbf{B}(t)+\mathbf{X}_{12}(t)\mathbf{C}(t).
\end{align*}
We hope to show that $\frac{d}{dt}\mathbf{X}(t)=\mathbf{X}(t)\mathbf{D}(t)$. Calculating the right side we have
\begin{align*}
\mathbf{X}(t)\mathbf{D}(t)&=
\begin{bmatrix}
\mathbf{X}_{11}(t) & \mathbf{X}_{12}(t)\\
\mathbf{0} & \mathbf{X}_{22}(t)
\end{bmatrix}
\begin{bmatrix}
\mathbf{A}(x) & \mathbf{B}(x)\\
\mathbf{0} & \mathbf{C}(x)
\end{bmatrix}\\
&=\begin{bmatrix}
\mathbf{X}_{11}(t)\mathbf{A}(x) & \mathbf{X}_{11}(t)\mathbf{B}(x)+\mathbf{X}_{12}(t)\mathbf{C}(x)\\
\mathbf{0} & \mathbf{X}_{22}(t)\mathbf{C}(x)
\end{bmatrix},
\end{align*}
given that $\mathbf{X}$ satisfies the desired differential equation and so it follows that $\mathbf{X}$ is the product integral $\prod_{s}^t\big(\mathbf{I}+\mathbf{D}(x)\ dx\big)$ as desired.$\blacksquare$

</details>
