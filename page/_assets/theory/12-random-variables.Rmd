# Random Variables

## Introduction

<blockquote class = "def">
**Definition 1.1. (Hansen)** _A **real-valued random variable**\index{random variable} $X$ on a probability space $(\Omega, \mathbb{F},P)$ is a measurable map $X : (\Omega,\mathbb{F})\to (\mathbb{R},\mathbb{B})$._
</blockquote>

We never specify the background space $(\Omega, \mathbb{F},P)$ however we always assume $X$ is $\mathbb{F}-\mathbb{B}$ measurable. This assumption implies $(X\in A)\in \mathbb{F}$ for every $A\in \mathbb{B}$. We may want to show measurability for constructed variables and so it surfises to show measurability for generaters for $\mathbb{B}$ such as checking $(X\le a)\in\mathbb{F}$ for every $a\in\mathbb{R}$.

<blockquote class = "def">
**Definition 1.2. (Hansen)** _The **distribution**\index{distribution} of a real-valued random variable $X$, defined on a probability space $(\Omega,\mathbb{F},P)$, is the collection of probability values_
\begin{align*}
    P(X\in A)\hspace{15pt}\text{for}\ A\in \mathbb{B}.\tag{1.3}
\end{align*}
_In other words: the distribution of $X$ is the image measure $X(P)$ on $(\mathbb{R},\mathbb{B})$._
</blockquote>

<blockquote class = "lem">
**Lemma 1.3. (Hansen)** _Let $X$ and $X'$ be two real-valued random variables on a probability space $(\Omega,\mathbb{F},P)$. If_
\begin{align*}
    P(X=X')=1
\end{align*}
_then $X$ and $X'$ has the same distribution._
</blockquote>

An often used way of summarizing the distribution is through the **distribution function**\index{distribution function} $F(x)=P(X\le x)$ for some $x\in\mathbb{R}$.

<blockquote class = "def">
**Definition 1.4. (Hansen)** _A real-valued random variable $X$ has a **discrete** distribution\index{discrete distribution} if there is a countable set $S\subset\mathbb{R}$ such that $P(X\in S)=1$._
</blockquote>

Usually $S$ is one of $\mathbb{N},\mathbb{Z},\mathbb{Q}$ or a subset of these. We may in the discrete case define the distribution by the point probabilities $P(X=x)=p(x)$ for $x\in S$.

<blockquote class = "def">
**Definition 1.5. (Hansen)** _A real-valued random variable $X$ has a distribution with **density**\index{density} $f : \mathbb{R}\to [0,\infty)$ if_
\begin{align*}
    P(X\in A)=\int_Af(x)dx\hspace{15pt}\text{for}\ A\in \mathbb{B}.\tag{1.5}
\end{align*}
_If this is the case we will write $X(P)=f\cdot m$ or $X\sim f\cdot m$._
</blockquote>

<blockquote class = "def">
**Definition 1.6. (Hansen)** _A real-valued random variable $X$ defined on a probability space $(\Omega, \mathbb{F},P)$ is said to have $p$'th moment for som $p>0$ if_
\begin{align*}
    E\vert X\vert^p<\infty\tag{1.12}
\end{align*}
_The collection of all variables that satisfies (1.12) is denoted by $\mathcal{L}^p(\Omega,\mathbb{F},P)$._
</blockquote>

Recall the definition of the **expectation**\index{expectation} of $X$ by
\begin{align*}
    E\, X=\int XdP \in R\cup \{-\infty,+\infty\}.\tag{1.11}
\end{align*}
We recall that for any measurable function $f : \mathbb{R}\to \mathbb{R}$ that are continuous on a set $A\in\mathbb{B}$ such that $P(X\in A)=1$ we may change variable simply by computing
\begin{align*}
    E\, f(X)=\int f\circ XdP=\int f(x)dX(P)(x).
\end{align*}

<blockquote class = "lem">
**Lemma 1.7. (Hansen)** _(Markov's inequality)\index{Markov's inequality} Let $X$ be a non-negative random variable. For any $c>0$ it holds that_
\begin{align*}
    P(X\ge c)\le \frac{E\, X}{c}\left(\le \frac{E\ X^n}{c^n}\text{ or }\le \frac{E\left(\varphi(X)\right)}{\varphi(c)}\right).\tag{1.14}
\end{align*}
_for some $\varphi$ non-negative monotome increasing function._
</blockquote>

Some other versions of Markov's inequality can be found in the form of **Chebyshev's inequality**, **Chebyshev-Cantelli's inequality** or **Jensen's inequality**\index{Chebyshev's inequality}\index{Chebyshev-Cantelli's inequality}\index{Jensen's inequality} repectively: Let $X$ be a real-valued random variable in $\mathcal{L}^2(\Omega,\mathbb{F},P)$ it holds for any $\varepsilon>0$.
\begin{align*}
    P\left(\vert X-E\, X\vert \ge \varepsilon\right)&\le \frac{V\, X}{\varepsilon^2}\tag{1.15}\\
    P\left( X-E\, X \ge \varepsilon\right)&\le \frac{V\, X}{V\, X+\varepsilon^2}\tag{prob: 1.13(c)}\\
    \varphi\left(E\ X\right)&\le E\left( \varphi(X)\right)
\end{align*}
for some convex function $\varphi$.

<blockquote class = "lem">
**Lemma 1.8. (Hansen)** _Let $X$ be a non-negative random variable. It holds that_
\begin{align*}
    E\, X=\int_0^\infty P(X>t)dt.\tag{1.16}
\end{align*}
_where the integral on the right hand side is with respect to Lebesgue measure._
</blockquote>

<blockquote class = "def">
**Definition 1.9. (Hansen)** _The **joint distribution**\index{joint distribution} of real-valued random variables $X_1,...,X_k$, defined on a probability space $(\Omega, \mathbb{F},P)$, is the collection of probability values_
\begin{align*}
    P(\mathbf{X}\in A)\hspace{15pt}\text{for}\ A\in\mathbb{B}_k.\tag{1.21}
\end{align*}
_In other words: the joint distribution of $X_1,...,X_k$ (or simply: the distribution of $\mathbf{X}$) is the image measure $\mathbf{X}(P)$ on $\left(\mathbb{R}^k,\mathbb{B}_k\right)$._
</blockquote>

<blockquote class = "def">
**Definition 1.11. (Hansen)** _Real-valued random variables $X_1,...,X_k$, defined on a probability space $(\Omega, \mathbb{F},P)$, are **jointly independent**\index{independent, jointly} if_
\begin{align*}
    P\left(X_1\in A_1,...,X_k\in A_k\right)=\prod_{i=1}^kP(X_i\in A_i)\hspace{15pt}\text{for}\ A_1,...,A_k\in\mathbb{B}.\tag{1.23}
\end{align*}
_In other words: the variables are independent if the joint distribution $\mathbf{X}(P)$ equals the product measure $X_1(P)\otimes ... \otimes X_k(P)$._
</blockquote>

<blockquote class = "thm">
**Theorem 1.12. (Hansen)** _Let $X_1,...,X_k$ be real-valued random variables defined on a probability space $(\Omega, \mathbb{F},P)$. If the variables are independent and if $E\vert X_i\vert<\infty$ for $i=1,...,k$, then the product $X_1\cdot ...\cdot X_k$ has first moment and_
\begin{align*}
    E\left(X_1\cdot ... \cdot X_k\right)=\prod_{i=1}^kE\, X_i\tag{1.24}
\end{align*}
</blockquote>

The equality only holds for two independent variables. However the **Cauchy-Schwarz inequality**\index{Cauchy-Schwarz inequality} which closely resembles (1.24) holds wether or not $X$ or $Y$ are independent:
\begin{align*}
    \left(E\vert XY\vert\right)^2\le E\, X^2\, E\, Y^2\text{ or }E\vert XY\vert \le \sqrt{E\ X^2}\sqrt{E\ Y^2}.\tag{1.25}
\end{align*}
Furthermore the theorem give rise to a measure for dependence i.e. the **covariance**\index{covariance} between two variables $X$ and $Y$
\begin{align*}
    \text{Cov}(X,Y)=E\left((X-E\,X)(Y-E\,Y)\right)=E(XY)-(E\, X)(E\, Y)\tag{1.26}
\end{align*}
with Cov$(X,Y)\ne 0$ if and only if $X$ and $Y$ are dependent. With Cov$(X,Y)=0$ the test is inconlusive. However independence implies Cov$(X,Y)=0$.

 

## Conditional expectation

The theory of conditional expectation is well-known from courses on the bachelor. Because of this we will only summarise the most important results.

We consider a background space $(\Omega,\mathcal{F},P)$ and a sub-sigma algebra $\mathcal{G}\subseteq \mathcal{F}$. We assume that some stochastic variable is $\mathcal{F}$-measurable, that is the mapping $X : (\Omega,\mathcal{F},P) \to (\mathbb{R},\mathbb{B},m)$ is $\mathcal{F}-\mathbb{B}$-measurable i.e. $\forall B\in\mathbb{B} : \{X\in B\}\in\mathcal{F}$. For some random variable $Z$ defined on the subspace $(\Omega,\mathcal{G},P)$, we say that $Z$ is the conditional expectation of $X$ given $\mathcal{G}$ if

$$
\forall G\in\mathcal{G} : \int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).
$$

This fact is summed up in the definition below.

<blockquote class = "def">

**Definition B.27. (Bjork)** **(Conditional expectation)** _Let $(\Omega,\mathcal{F},P)$ be a probability space and $X$ a random variable in $L^1(\Omega,\mathcal{F},P)$ ($\vert X\vert$ is integrable). Let furthermore $\mathcal{G}$ be a sigma-algebra such that $\mathcal{G}\subseteq \mathcal{F}$. If $Z$ is a random variable with the properties that:_

  i. _$Z$ is $\mathcal{G}$-measurable._
  ii. _For every $G\in\mathcal{G}$ it holds that_
  $$\int_G Z(\omega)\ dP(\omega)=\int_G X(\omega)\ dP(\omega).\tag{B.5}$$

_Then we say that $Z$ is the **conditional expectation of $X$ given the sigma-algebra $\mathcal{G}$**. In that case we denote $Z$ by the symbol $E[X\ \vert\ \mathcal{G}]$._

</blockquote>

We see that from the above it always holds that $X$ satisfies (ii). It does not, however, always hold that $X$ is $\mathcal{G}$-measurable. Given this fact it is not trivial that a random variable $E[X\ \vert\ \mathcal{G}]$ even exists. This nontriviality is fortunatly resolved by the Radon-Nikodym theorem.

<blockquote class = "thm">

**Theorem B.28. (Bjork)** **(Existance and uniqueness of Conditional expectation)** *Let $(\Omega,\mathcal{F},P)$, $X$ and $\mathcal{G}$ be given as in the definition above. Then the following holds:*

  * _There will always exist a random variable $Z$ satisfying conditions (i)-(ii) above._
  * _The variable $Z$ is unique, i.e. if both $Y$ and $Z$ satisfy (i)-(ii) then $Y=Z$ $P$-a.s._

</blockquote>

This result ensures that we may condition on any sigma-algebra for instance $\mathcal{G}=\sigma(Y)$ in that case we (pure notation) write

$$
E[X\ \vert\ \sigma(Y)]=E[X\ \vert\ Y],\hspace{20pt}\sigma(Y)=\sigma\left(\left\{ Y\in A,\ A\in\mathbb{B}\right\}\right).
$$

In the above $\sigma(Y)$ is simply the smallest sigma-algebra containing all the pre-images of $Y$, that is the smallest sigma-algebra making $Y$ measurable! Giving this foundation there are a few properties conditional expectation have which is rather useful (for instance the tower property).

Below we assume: Let $(\Omega,\mathcal{F},P)$ be a probability space and $X,Y$ be random variables in $L^1(\Omega,\mathcal{F},P)$.

<blockquote class = "prop">

**Proposition B.29.** **(Monotinicity/Linearity of Conditional expectation)** _The following holds:_

$$
X\le Y\ \Rightarrow\ E[X\ \vert\ \mathcal{G}]\le E[Y\ \vert\ \mathcal{G}],\hspace{20pt}P-\text{a.s.}\tag{B.6}
$$

$$
E[\alpha X + \beta Y\ \vert\ \mathcal{G}]=\alpha E[X\ \vert\ \mathcal{G}]+ \beta E[Y\ \vert\ \mathcal{G}],\hspace{20pt}\forall \alpha,\beta\in\mathbb{R}.\tag{B.7}
$$

</blockquote>

<blockquote class = "prop">

**Proposition B.30. (Bjork)** **(Tower property)** _Assume that it holds that $\mathcal{H}\subseteq\mathcal{G}\subseteq\mathcal{F}$. Then the following hold:_

$$
E[E[X\vert \mathcal{G}]\vert\mathcal{H}]=E[X\vert \mathcal{H}],\tag{B.8}
$$

$$
E[X]=E[E[X\vert \mathcal{G}]].\tag{B.9}
$$

</blockquote>

<blockquote class = "prop">

**Proposition B.31. (Bjork)** _Assume $X$ is $\mathcal{G}$ and that both $X,Y$ and $XY$ are in $L^1$ (only assuming $Y$ is $\mathcal{F}$-measurable), then_

$$
E[X\vert\mathcal{G}]=X,\hspace{20pt}P-\text{a.s.}\tag{B.11}
$$

$$
E[XY\vert\mathcal{G}]=XE[Y\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}\tag{B.12}
$$

</blockquote>

<blockquote class = "prop">

**Proposition B.32. (Bjork)** **(Jensen inequality)** _Let $f:\mathbb{R}\to\mathbb{R}$ be a convex (measurable) function and assume $f(X)$ is in $L^1$. Then_

$$
f(E[X\vert\mathcal{G}])\le E[f(X)\vert\mathcal{G}],\hspace{20pt}P-\text{a.s.}
$$

</blockquote>

<blockquote class = "prop">

**Proposition B.37. (Bjork)** _Let $(\Omega,\mathcal{F},P)$ be a given probability space, let $\mathcal{G}$ be a sub-sigma-algebra of $\mathcal{F}$, and let $X$ be a square integrable random variable.
Consider the problem of minimizing_

$$
E\left[(X-Z)^2\right]
$$

_where $Z$ is allowed to vary over the class of all square integrable $\mathcal{G}$ measurable random variables. The optimal solution $\hat{Z}$ is then given by._

$$
\hat{Z}=E[X\vert\mathcal{G}].
$$

</blockquote>
\noindent\makebox[\linewidth]{\rule{\textwidth}{0.4pt}}
<details>
<summary>**Proof.**</summary>

Let $X\in L^2(\Omega,\mathcal{F},P)$ be a random variable. Now consider an arbitrary $Z\in L^2(\Omega,\mathcal{G},P)$. Recall that $\mathcal{G}\subset \mathcal{F}$ and so $X$ is also in $Z\in L^2(\Omega,\mathcal{G},P)$, as it is bothe square integrable and $\mathcal{G}$-measurable. Then

$$
E\left[Z\cdot(X-E[X\vert\mathcal{G}])\right]=E\left[Z\cdot X\right]-E\left[Z\cdot E[X\vert\mathcal{G}]\right].
$$

Then by using the law of total expectation and secondly that $Z$ is $\mathcal{G}$-measurable we have that

$$
E\left[Z\cdot X\right]=E\left[E[Z\cdot X\vert\mathcal{G}]\right]=E\left[Z\cdot E[ X\vert\mathcal{G}]\right].
$$

Combining the two equations gives the desired result. Obviously, we have that

$$
X-Z=X-Z+E[X\vert\mathcal{G}]-E[X\vert\mathcal{G}]=(X-E[X\vert\mathcal{G}])+(E[X\vert\mathcal{G}]-Z).
$$

Then squaring the terms gives

$$
(X-Z)^2=(X-E[X\vert\mathcal{G}])^2+(E[X\vert\mathcal{G}]-Z)^2+2(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)
$$

Taking expectation on each side and using linearity of the expectation we have that

$$
E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right]+2E\left[(X-E[X\vert\mathcal{G}])(E[X\vert\mathcal{G}]-Z)\right].
$$

We can now use that $E[X\vert\mathcal{G}]-Z$ is $\mathcal{G}$-measurable with the above result on the last term.

$$
E[(X-Z)^2]=E\left[(X-E[X\vert\mathcal{G}])^2\right]+E\left[(E[X\vert\mathcal{G}]-Z)^2\right].
$$

Now since $X$ is given the term $E\left[(X-E[X\vert\mathcal{G}])^2\right]$ is simply a constant not depending on the choice og $Z$. The optimal choice of $Z$ is then $E[X\vert\mathcal{G}]$ since this minimizes the second term. The statement is then proved.

</details>

 

## Independence

<blockquote class = "def">
**Definition 3.1. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space. Two events $A,B\in\mathbb{F}$ are **independent**\index{independent} if_
\begin{align*}
    P(A\cap B)=P(A)P(B)\tag{3.1}
\end{align*}
</blockquote>

<blockquote class = "def">
**Definition 3.4. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space and let $\mathbb{G},\mathbb{H}\subset \mathbb{F}$ be two classes of measurable sets. We sat that $\mathbb{G}$ and $\mathbb{H}$ are independent, written $\mathbb{G}\perp \!\!\! \perp\mathbb{H}$, if_
\begin{align*}
    P(A\cap B)=P(A)P(B)\hspace{15pt}\text{for all}\ A\in\mathbb{G},B\in\mathbb{H}.\tag{3.2}
\end{align*}
</blockquote>

<blockquote class = "lem">
**Lemma 3.5. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space and let $\mathbb{G},\mathbb{H}\subset \mathbb{F}$ be two classes of measurable sets. Let $\mathbb{G}_1\subset \mathbb{G}$ and $\mathbb{H}_1\subset\mathbb{H}$ be two subclasses. If $\mathbb{G}\perp \!\!\! \perp \mathbb{H}$ then it holds that $\mathbb{G}_1\perp \!\!\! \perp \mathbb{H}_1$._
</blockquote>

<blockquote class = "def">
**Definition 3.6. (Hansen)** _A class $\mathbb{H}$ of subsets of $\Omega$ is a **Dynkin class**\index{Dynkin class} if_

  1. $\Omega \in\mathbb{H}$,
  2. $A,B\in\mathbb{H},A\subset B\hspace{15pt}\Rightarrow\hspace{15pt}B\setminus A\in\mathbb{H}$,
  3. $A_1,A_2,...\in\mathbb{H},A_1\subset A_2\subset ...\hspace{15pt}\Rightarrow\hspace{15pt}\bigcup_{n=1}^\infty A_n\in\mathbb{H}$.

</blockquote>

<blockquote class = "lem">
**Lemma 3.7. (Hansen)** _(Dynkin) Let $\mathbb{D}\subset \mathbb{H}_0\subset \mathbb{H}$ be three nested classes of subsets of $\Omega$. if_

  1. $\sigma(\mathbb{D})=\mathbb{H}$,
  2. $A,B\in\mathbb{D}\hspace{10pt}\Rightarrow\hspace{10pt}A\cap B\in\mathbb{D}$
  3. $\mathbb{H}_0$ _is a Dynkin class._

_then it holds that $\mathbb{H}_0=\mathbb{H}$._
</blockquote>

<blockquote class = "lem">
**Lemma 3.8. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $A\in\mathbb{F}$ be a fixed event. The class_
\begin{align*}
    \mathbb{H}=\{B\in \mathbb{F}\ \vert\ A\perp \!\!\! \perp B\}
\end{align*}
_is a Dynkin class._
</blockquote>

<blockquote class = "thm">
**Theorem 3.9. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,\mathbb{G}_2\subset \mathbb{F}$ be two sigma-algebras. Let $\mathbb{D}_1$ and $\mathbb{D}_2$ be two classes such that $\sigma(\mathbb{D}_i)=\mathbb{G}_i$ for $i=1,2$._
_If both $\mathbb{D}_1$ and $\mathbb{D}_2$ are $\cap$-stable then it holds that_
\begin{align*}
    \mathbb{D}_1\perp \!\!\! \perp\mathbb{D}_2\hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp\mathbb{G}_2.
\end{align*}
</blockquote>

<blockquote class = "def">
**Definition 3.10. (Hansen)** _Two real-valued random variable $X$ and $Y$ on a background space $(\Omega,\mathbb{F},P)$ are \textbf{independent}, written $X\perp \!\!\! \perp Y$, if the corresponding sigma-algebras $\sigma(X)$ and $\sigma(Y)$ are independent._
</blockquote>

<blockquote class = "def">
**Definition 3.15. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}$ be finitely many classes of measurable sets. We say that $\mathbb{G}_1,...,\mathbb{G}_n$ are **jointly independent**\index{jointly independent}, written $\mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_n$, if_
\begin{align*}
    P(A_1\cap ...\cap A_n=\prod_{i=1}^nP(A_i)\hspace{15pt}\text{for }A_1\in\mathbb{G}_1,...,A_n\in\mathbb{G}_n.\tag{3.8}
\end{align*}
</blockquote>

<blockquote class = "lem">
**Lemma 3.16. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}$ be finitely many classes of measurable sets. It holds that_
\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_n\hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp...\perp \!\!\! \perp\mathbb{G}_{n-1}
\end{align*}
_provided that $\Omega\in\mathbb{G}_n$._
</blockquote>

<blockquote class = "thm">
**Theorem 3.17. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}$ be sigma-algebras. Let $\mathbb{D}_1,...,\mathbb{D}_n$ be classes such that $\sigma(\mathbb{D}_i)=\mathbb{G}_1$ for $i=1,...,n$. Suppose that for all lengths $k=2,...,n$ an all choices of indices $\le j_1<...<j_k\le n$ it holds that_
\begin{align*}
    \mathbb{D}_{j_1}\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{D}_{j_k}\tag{3.9}
\end{align*}
_If all the generators $\mathbb{D}_i$ are $\cap$-stable, then it holds that $\mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n$._
</blockquote>

<blockquote class = "lem">
**Lemma 3.18. (Hansen)** _(Grouping) Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,...,\mathbb{G}_n\subset \mathbb{F}$ be sigma-algebras. It holds that_
\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n \hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_{n-2}\perp \!\!\! \perp\sigma(\mathbb{G}_{n-1},\mathbb{G}_n).
\end{align*}
</blockquote>

<blockquote class = "def">
**Definition 3.19. (Hansen)** _The real-valued random variables $X_1,...,X_n$ on a background space $(\Omega,\mathbb{F},P)$ are **jointly independent**, written $X_1\perp \!\!\! \perp ... \perp \!\!\! \perp X_n$, if the corresponding sigma-algebras $\sigma(X_1),...,\sigma(X_n)$ are jointly independent._
</blockquote>

<blockquote class = "def">
**Definition 3.20. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $(\mathbb{G}_i)_{i\in I}$ be a family of classes of measurable sets. We say that the family $(\mathbb{G}_i)_{i\in I}$ is **jointly independent** if any finite subfamily is jointly independent._
</blockquote>

<blockquote class = "thm">
**Theorem 3.21. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,\mathbb{G}_2,...\subset \mathbb{F}$ be sigma-algebras. Let $\mathbb{D}_1,\mathbb{D}_2,...$ be classes such that $\sigma(\mathbb{D}_n)=\mathbb{G}_n$ for all $n\in\mathbb{N}$. Suppose that for all lengths $k\in\mathbb{N}$ and all choices of indices $1\le j_1< ... < j_k$ it holds that_
\begin{align*}
    \mathbb{D}_{j_1}\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{D}_{j_k}.\tag{3.14}
\end{align*}
_If all the generators $\mathbb{D}_n$ are $\cap$-stable, then it holds that $\mathbb{G}_1\perp \!\!\! \perp \mathbb{G}_2 \perp \!\!\! \perp ...$._
</blockquote>

<blockquote class = "lem">
**Lemma 3.22. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $\mathbb{G}_1,\mathbb{G}_2,...\subset \mathbb{F}$ be sigma-algebras. It holds that_
\begin{align*}
    \mathbb{G}_1\perp \!\!\! \perp \mathbb{G}_2 \perp \!\!\! \perp ... \hspace{10pt}\Rightarrow \hspace{10pt} \mathbb{G}_1\perp \!\!\! \perp ... \perp \!\!\! \perp \mathbb{G}_n \perp \!\!\! \perp \sigma(\mathbb{G}_{n+1},\mathbb{G}_{n+2}, ... ).
\end{align*}
</blockquote>

<blockquote class = "def">
**Definition 3.23. (Hansen)** _The real-valued random variables $(X_i)_{i\in I}$ on a background space $(\Omega,\mathbb{F},P)$ are **jointly independent** if the corresponding sigma-algebras $(\sigma(X_i))_{i\in I}$ are jointly independent._
</blockquote>

<blockquote class = "def">
**Definition 3.28. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space. A sigma-algebra $\mathbb{G}\subset \mathbb{F}$ satisfies a **zero-one law**\index{zero-one law} if_
\begin{align*}
    P(A)\in\{0,1\}\hspace{15pt}\text{for all}\ A\in\mathbb{G}.
\end{align*}
</blockquote>

<blockquote class = "thm">
**Theorem 3.29. (Hansen)** _Let $(\Omega,\mathbb{F},P)$ be a probability space and let $\mathbb{G}\subset \mathbb{F}$ be a sigma-algebra. The following three conditions are equivalent:_

  1. _For any sigma-algebra $\mathbb{H}\subset \mathbb{F}$ it holds that $\mathbb{G} \perp \!\!\! \perp \mathbb{H}$,_
  2. _It holds that $\mathbb{G}\perp \!\!\! \perp\mathbb{G}$,_
  3. _$\mathbb{G}$ satisfies a 0-1 law._
</blockquote>

<blockquote class = "def">
**Definition 3.30. (Hansen)** _Let $X_1,X_2,...$ be real-valued random variables on a background space $(\Omega,\mathbb{F},P)$. The **tail sigma-algebra**\index{tail sigma-algebra} of the process is defined as_
\begin{align*}
    \mathbb{J}(X_1,X_2,...)=\bigcap_{n=1}^\infty \sigma(X_n,X_{n+1}, ... ).
\end{align*}
</blockquote>

<blockquote class = "thm">
**Theorem 3.32. (Hansen)** _(Kolmogorov's zero-one law)\index{Kolmogorov's zero-one law} Let $X_1,X_2,...$ be real-valued random variables on a background space $(\Omega,\mathbb{F},P)$. If $X_1 \perp \!\!\! \perp X_2 \perp \!\!\! \perp ...$ then the tail-algebra $\mathbb{J}(X_1,X_2,...)$ satisfies a 0-1 law._
</blockquote>

<blockquote class = "lem">
**Lemma 3.35. (Hansen)** _(2nd half of Borel-Cantelli)\index{Borel-Cantelli, 2nd half} Let $(\Omega,\mathbb{F},P)$ be a probability space, and let $A_1,A_2,...$ be a sequence of $\mathbb{F}$-measurable sets. If $A_1 \perp \!\!\! \perp A_2 \perp \!\!\! \perp ...$ then it holds that_
\begin{align*}
    \sum_{n=1}^\infty P(A_n)<\infty \iff P(A_n\text{ i.o.})=0.
\end{align*}
</blockquote>

 

## Moment generating function

Let $X$ be a random variable with distribution function $F(x)=P(X\le x)$ and $Y$ be a random variable with distribution function $G(y)=P(Y\le y)$.

<blockquote class = "def">

**Definition. (Ex. FinKont)** _The moment generating function or Laplace transform of $X$ is_

$$
\psi_X(\lambda)=E\left[e^{\lambda X}\right]=\int_{-\infty}^\infty e^{\lambda x}dF(x)
$$

_provided the expectation is finite for $\vert\lambda\vert<h$ for some $h>0$._

</blockquote>

The MGF uniquely determine the distribution of a random variable, due to the following result.

<blockquote class = "thm">

**Theorem. (Ex. FinKont)** **(Uniqueness)** _If $\psi_X(\lambda)=\psi_Y(\lambda)$ when $\vert\lambda\vert<h$ for some $h>0$, then $X$ and $Y$ has the same distribution, that is, $F=G$._

</blockquote>

There is also the following result of independence for Moment generating functions.

<blockquote class = "thm">

**Theorem. (Ex. FinKont)** **(Independence)** _If_ 

$$
E\left[e^{\lambda_1X+\lambda_2Y}\right]=\psi_X(\lambda_1)\psi_Y(\lambda_2)
$$

_for $\vert\lambda_i\vert<h$ for $i=1,2$ for some $h>0$, then $X$ and $Y$ are independent random variables._

</blockquote>

 

## Standard distributions

### Normal disribution

The following gives a comprehensive table of the standard some properties.\index{Normal disribution}

| Description | Symbol | Normal distribution |
| :---------- | :----: | :-----------------: |
| Definition | $\sim$ | $X\sim\mathcal{N}(\mu,\sigma^2)$ |
| Parameters | $\theta\in \Theta$ | $\theta=(\mu,\sigma^2)\in \mathbb{R}\times \mathbb{R}_+$ |
| Support | $\text{Im}(X)$ | $x\in \mathbb{R}$ |
| Density | $f$ | $\frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)^2}$ |
| Distribution | $F$ | $\frac{1}{2}\left(1+N\left(\frac{x-\mu}{\sqrt{2\sigma^2}}\right)\right)$|
| Mean value | $E[X]$ | $\mu$ |
| Variance | $\text{Var}(X)$ | $\sigma^2$ |
| MGF* | $\psi_X=E[e^{\lambda X}]$ | $e^{\mu \lambda+\frac{1}{2}\lambda^2\sigma^2}$ |
| Characteristic function | $\varphi_X(t)=E[e^{itX}]$ | $e^{it\mu-\frac{1}{2}\sigma^2 t^2}$ |

In the table above we used the abbreviations: *MGF = Moment Generating function.

We also used the shorthand: $N$ being the distribution of a standard normal distributed variable $\mathcal{N}(0,1)$.