# Projects in the Mathematics of Life Insurance

Below is the used material.

  - *Life insurance cash flows with policyholder behavior* by Kristian Buchardt and Thomas Moller (2015),
  - *Transaction time models in multi-state life insurance* by Kristian Buchardt, Christian Furrer and Oliver Lunding Sandqvist (2023),
  - *Note on parametric estimation of Markov chains* by Oliver Lunding Sandqvist (2023),
  - *Non-parametric estimation in Markov models* by Christian Furrer (2023).

## Cash flows, Markov models, and policyholder behavior

When we model life-insurance contracts we are faced with a list of choices around how the payments are handled, the behavior of the underlying statewise jump process and the information on which we base our valuation. The most central thing to consider is the payment process $B$ and hence under which events the insurance expect to receive premiums and pay out benefits. We model the payments by the dynamics of $B$ and assume $B(0-)=0$:

$$
dB(t)=\sum_{i\in \mathcal J}1\{Z(t)=i\}\Big(b^{i}(t)\ dt+ \Delta B^i(t)\Big)+\sum_{k,j\in \mathcal J, k\ne j}b^{kj}(t)\ dN^{kj}(t),
$$

where we assume $b^i$, $\Delta B^i$ and $b^{ij}$ are deterministic FV-functions for all $i,j\in  \mathcal J$. This is indeed the most important choice for the insurance contract.

Next, we should assume some jump process for $Z(t)$ that governs the payment process. In general we assume that $Z=(Z(t))_{t\ge 0}$ is a non-explosive pure jump process, that is for any bounded interval $[t,s]$ for $t\ne s$ the counting process, defined as

$$
N(t)=\#\{0\le s\le t : Z(s)\ne Z(s-)\},
$$

does not tend to infinity. Therefore since we assumed that the payment processes $b^i$, $\Delta B^i$ and $b^{ij}$ are FV-functions then it follows that $B$ is a CADLAG process of finite variation (FV). Notice that we did not assume any Markov properties on $Z$.

We could assume some underlying process for $Z$ but we could just as well assume something about the predictable compensators of $B$ and $Z$ that is $\Lambda^B$ and $(\Lambda^{N^{ij}})_{i,j\in E}$. Recall that for all FV-processes $X$ there exist a predictable compensator such that

$$
M^X(t)=X(t)-\Lambda^X(t),
$$

is a Martingale wrt. $\sigma(X(t))$. We determine the predictable compensator through its dynamics

$$
d\Lambda^X(t)=\mathbb E[dX(t)\ \vert\ \sigma(X(t-))].
$$

Lastly, we define the information which is used when modelling the payments and valuating the contract. This is a filtration $\mathcal F=(\mathcal F_t)_{t\ge 0}$ which may be chosen according to the needed information for the payment stream $B$. In the most simple example we would have

$$
\mathcal F_t=\sigma\left(\mathcal F_t^{Z},\mathcal F_t^{r}\right),
$$

where $r$ is the short rate on the investments and $\mathcal F_t^X=\sigma(X(t))$ is the smallest sigma algebra that makes $X$ measurable. In more elaborate models the information could also contain state duration or other information.

### Cash flows

Reserving is an important topic in life insurance as it ensures that the insurance company reserve adequate cash to pay customers in the future. The liabilities at a timepoint $t\ge 0$ is given by the reserve denoted by $V$:

$$
V(t)=\mathbb E^{\mathbb Q\otimes \mathbb P}\left[\left.\int_t^\infty e^{-\int_t^sr(u)\ du}dB(s)\ \right\vert\ \mathcal F_t\right],
$$

where we use the product measure that valuate outcomes from $B$ that is $Z$ under the objective probability measure $\mathbb P$ and the short rate under the martingale measure $\mathbb Q\sim\mathbb P$. We will furthermore assume that $r$ and $Z$ are independend and that $\mathbb Q\sim\mathbb P$ ensures this. We denote this product measure $\mathbb Q\otimes \mathbb P$ and the superscript indicates that this is used in the integral. If we use the predictable compensator for $B$ we may assume that it is absolutely continuous and so

$$
\Lambda^B(t)=\Lambda^B(0)+\int_0^t\lambda^B(s)\ ds,
$$

hence

$$
V(t)=\mathbb E^{\mathbb Q\otimes \mathbb P}\left[\left.\int_t^\infty e^{-\int_t^sr(u)\ du}\lambda^B(s)\ ds\ \right\vert\ \mathcal F_t\right]+\mathbb E^{\mathbb Q\otimes \mathbb P}\left[\left.\int_t^\infty e^{-\int_t^sr(u)\ du}d M^B(s)\ \right\vert\ \mathcal F_t\right],
$$

where of cause the martingale integral is almost surely zero and hence in expectation zero. We can furthermore move the expectation inside using Fubini and obtain.

$$
V(t)=\int_t^\infty E^{\mathbb Q}\left[\left. e^{-\int_t^sr(u)\ du}\ \right\vert\ \mathcal F_t\right]\mathbb E^{\mathbb P}\left[\left.\lambda^B(s)\ \right\vert\ \mathcal F_t\right]\ ds.
$$

We can now define the forward rate $f_t(s)$ according to

$$
E^{\mathbb Q}\left[\left. e^{-\int_t^sr(u)\ du}\ \right\vert\ \mathcal F_t\right]=e^{-\int_t^sf_t(u)\ du}.
$$

We also see that this is in fact the pricing function for the zero-coupon bond maturing at time $s\ge t$. We can furthermore define the **expected cash flow** during $[t,s]$ by

$$
A(t,s)=\int_t^s\mathbb E^{\mathbb P}\left[\left.\lambda^B(u)\ \right\vert\ \mathcal F_t\right]\ du.
$$

If we use the notation $A(t,ds)$ as the dynamics of $A(s):=A(t,s)$ we may write

$$
V(t)=\int_t^\infty e^{-\int_t^s f_t(u)\ du} A(t,ds).
$$

If one use this decomposition one can compute the expected cash flow from today until infinity and study independently the effects of changing the rate model and mortality model. Crucially we assumed that $r$ and $Z$ are independent which in the case of policy behavour may not hold (high returns may result in increased free policy intensity).

### Smooth Markov model

In classic life insurance mathematics we assume the following:

  * The functions $(\mu^{ij}(t))_{i,j\in\mathcal J}$ defined as
  $$\mu^{ij}(t)=\lim_{h\to 0}\frac{1}{h}p^{ij}(t,t+h)$$
  exists and are differentiable at all timepoints $t\ge 0$.
  * The predictable compensator of $B$ is given by
  \begin{align*}
  d\Big(\Lambda^B(s)\ \vert\ \mathcal F_t\Big)&=d\Big(\Lambda^B(s)\ \vert\ Z(t)\Big)\\
  &=\sum_{i\in \mathcal J}1\{Z(t)=i\}p^{ij}(t,s)\left(b^i(t)\ ds+\Delta B^i(t)+\sum_{j\in\mathcal J, j\ne i}b^{ij}(t)\mu^{ij}(t)\ ds\right).
  \end{align*}
  * The transition probabilities $p^{ij}(t)$ satisfies Kolmogorov's forward differential equation
  \begin{align*}
  \frac{\partial}{\partial s}p^{ij}(t,s)=\sum_{k\in\mathcal J,k\ne j}p^{ik}(t,s)\mu^{kj}(s)\ ds-p^{ij}(t,s)\sum_{k\in\mathcal J, k\ne j}\mu^{jk}(s)
  \end{align*}
  with boundary condition $p^{ij}(t,t)=\delta^{ij}$ with $\delta^{ij}=1\{i=j\}$.

We call this a smooth model becauce $\mu^{ij}$ are continuous and so $p^{ij}$ are smooth. There does exist models where $\mu^{ij}$ can be non-continuous. In this classic model the expected cash flow is given by
\begin{align*}
A(t,s)&=\mathbb E^\mathbb P\left[\left.B(s)-B(t)\ \right\vert\ \mathcal F_t\right]=\mathbb E^\mathbb P\left[\left.\Lambda^B(s)-\Lambda^B(t)\ \right\vert\  Z(t)\right]\\
&=\int_t^s\sum_{i\in \mathcal J}1\{Z(t)=i\}p^{ij}(t,u)\left(b^i(u)\ du+\Delta B^i(u)+\sum_{j\in\mathcal J, j\ne i}b^{ij}(u)\mu^{ij}(u)\ du\right)
\end{align*}
hence

$$
A(t,ds)=\sum_{i\in \mathcal J}1\{Z(t)=i\}p^{ij}(t,s)\left(b^i(s)\ ds+\Delta B^i(s)+\sum_{j\in\mathcal J, j\ne i}b^{ij}(s)\mu^{ij}(s)\ ds\right).
$$

If we calculate $p^{ij}(t,s)$ for all $i,j\in\mathcal J$ and $s\ge t$ for all $t\ge 0$ we easily obtain the vector functions

$$
\left(A^i(t,s)\right)_{i\in\mathcal J}=\left(\int_t^s p^{ij}(t,s)\left(b^i(s)\ ds+\Delta B^i(s)+\sum_{j\in\mathcal J, j\ne i}b^{ij}(s)\mu^{ij}(s)\ ds\right)\right)_{i\in\mathcal J}
$$

where $t$ is fixed and $s\ge t$. Using this result we can write the reserve as

$$
V(t)=\sum_{i\in \mathcal J}1\{Z(t)=i\}\int_t^\infty e^{-\int_t^s f_t(u)\ du}p^{ij}(t,s)\left(b^i(s)\ ds+\Delta B^i(s)+\sum_{j\in\mathcal J, j\ne i}b^{ij}(s)\mu^{ij}(s)\ ds\right).
$$

### Absorbing sub state space

One can construct models where that include policy behavior. This may include the free policy option, surrender option and/or early retirement option. By this we mean:

  * *The free policy option*; the option to at any point stop paying premiums but still be entitled to some benefits (though lower than expected),
  * *The surrender option*; the option to at any point cancel the contract and receive a compensatory payment,
  * *Early retirement option*; the option to at any point transition into the retirement state and start receiving benefits (though lower than by waiting to the agreed upon retirement date).

In any case, this leads to a subdivision of the statespace $\mathcal J$ into $\mathcal J_0$ and $\mathcal J_1$ such that $\mathcal J_0\cap \mathcal J_1=\emptyset$ and $\mathcal J_0\cup\mathcal J_1=\mathcal J$. We let the policy start in $\mathcal J_0$ i.e. $Z(0)\in\mathcal J_0$ almost surely and assume that $\mathcal J_1$ is absorbing i.e. for all $i\in \mathcal J_1$ and $j\in\mathcal J_0$ we have $p^{ij}(t,s)=0$ for all $t\le s$.

In the free policy option we would simply take a copy of $\mathcal J$ (for instance active, disabled and dead) and say that the policy may jump to a free policy version of $\mathcal J$ at any time but not back. In that case we would simply have

$$
\mathcal J=\{\text{active}(0),\text{disabled}(1),\text{dead}(2)\}\cup\{\text{active as fp}(3),\text{disabled as fp}(4),\text{dead as fp}(5)\},
$$

the so-called six state model. In general, we define the transition time from $\mathcal J_0$ to $\mathcal J_1$ as

$$
\tau =\inf\{t\ge 0 : Z(t)\in\mathcal J_1\}
$$

using the convention $\inf\emptyset =\infty$. As we would need to adjust the payment streams in $\mathcal J_1$ based on the time of transition $\tau$ we define the function $\rho(i,j,t):\mathcal J_0\times \mathcal J_1\times [0,\infty)\to\mathbb R$ typical taking values in $(0,1]$ as a adjustment factor to the payments in the sub statespace $\mathcal J_1$. The factor that is just is determined by the transition $Z(\tau -)\to Z(\tau)$, that is the factor $\rho(Z(\tau-),Z(\tau),\tau)$. In this case we define the payment stream as

$$
dB(t)=dB_0(t)+dB_1(t),
$$

with

$$
dB_0(t)=\sum_{i\in \mathcal J_0}1\{Z(t)=i\}\Big(b^{i}(t)\ dt+ \Delta B^i(t)\Big)+\sum_{k,j\in \mathcal J_0, k\ne j}b^{kj}(t)\ dN^{kj}(t)
$$

and

$$
dB_1(t)=\rho(Z(\tau-),Z(\tau),\tau)\left(\sum_{i\in \mathcal J_1}1\{Z(t)=i\}\Big(b^{i}(t)\ dt+ \Delta B^i(t)\Big)+\sum_{k,j\in \mathcal J_1, k\ne j}b^{kj}(t)\ dN^{kj}(t)\right).
$$

Notice that no payments are payed upon the transition $Z(\tau-)\to Z(\tau)$. Then we have the expected cash flow given by
\begin{align*}
A(t,s)&=\mathbb E^\mathbb P\left[\left.B(s)-B(t)\ \right\vert\ \mathcal F_t\right]=\mathbb E^\mathbb P\left[\left.\int_t^s dB_0(u)+ \int_t^s dB_1(u)\ \right\vert\ \mathcal F_t\right]\\
&=\mathbb E^\mathbb P\left[\left.\int_t^s \sum_{i\in \mathcal J_0}1\{Z(u)=i\}\Big(b^{i}(u)\ du+ \Delta B^i(u)\Big)+\sum_{k,j\in \mathcal J_0, k\ne j}b^{kj}(u)\ dN^{kj}(u)\ \right\vert\ \mathcal F_t\right]\\
&+\mathbb E^\mathbb P\left[\rho(Z(\tau-),Z(\tau),\tau)\int_t^s \sum_{i\in \mathcal J_1}1\{Z(u)=i\}\Big(b^{i}(u)\ du+ \Delta B^i(u)\Big)\right.\\
&+\left.\left.\sum_{k,j\in \mathcal J_1, k\ne j}b^{kj}(u)\ dN^{kj}(u)\ \right\vert\ \mathcal F_t\right]\\
&=\sum_{i\in \mathcal J_0}1\{Z(t)=i\}\int_t^s \sum_{j\in\mathcal J_0,j\ne i}p^{ij}(t,u)\left(\Big(b^{j}(u)\ du+ \Delta B^j(u)\Big)+\sum_{k\in \mathcal J_0, k\ne j}b^{jk}(u)\mu^{jk}(u)\ du \right)\\
&+\sum_{i\in \mathcal J}\sum_{j\in \mathcal J_1}\int_t^s\mathbb E^\mathbb P\Big[\left.1\{Z(t)=i\} 1\{Z(u)=j\}\rho(Z(\tau-),Z(\tau),\tau)\ \right\vert\ \mathcal F_t\Big]\\
&\left(\Big(b^{j}(u)\ du+ \Delta B^j(u)\Big)+\sum_{k\in \mathcal J_0, k\ne j}b^{jk}(u)\mu^{jk}(u)\ du \right),
\end{align*}
where the integral $\int_t^s dB_0(t)$ only sums over the starting states in $\mathcal J_0$ since one only receive payments in $\mathcal J_0$ at time $u\ge t$ if one starts in this subspace. If we define

$$
p_\rho^{ij}(t,s)=E^\mathbb P\Big[\left.1\{Z(s)=j\}\rho(Z(\tau-),Z(\tau),\tau)^{1\{Z(s)\in \mathcal J_1\}}\ \right\vert\ Z(t)=i\Big]
$$

then the above simplifies to

$$
A(t,ds)=p_\rho^{ij}(t,u)\left(\Big(b^{j}(u)\ du+ \Delta B^j(u)\Big)+\sum_{k\in \mathcal J_0, k\ne j}b^{jk}(u)\mu^{jk}(u)\ du \right).
$$


And so we may evaluate the reserves if we can compute $(p_\rho^{ij}(t,s))_{i,j\in\mathcal J}$.

### $\rho$-modified Kolmogorov

As we saw in the above section solving the cash flow in the policy behaviour model comes down to solving the quantities

$$
p_\rho^{ij}(t,s)=E^\mathbb P\Big[\left.1\{Z(s)=j\}\rho(Z(\tau-),Z(\tau),\tau)^{1\{Z(s)\in \mathcal J_1\}}\ \right\vert\ Z(t)=i\Big]\tag{1}
$$

we will henceforth call the $\rho$-modified transition probabilities. It follows that even though this expectations seems problematic due to the stopping time $\tau$. However, one can show that the $\rho$-modified transition probabilities follows a forward kolmogorov differential equation:

<blockquote class = "prop">

**Proposition 1.1. ($\rho$-modified Kolmogorov)**\index{$\rho$-modified Kolmogorov} _Let the $\rho$-modified transition probabilities be given by (1). For $i,j\in\mathcal J$ we have_

$$
\frac{d}{ds}p_\rho^{ij}(t,s)=\sum_{k\in\mathcal J,k\ne j}p_\rho^{ik}(t,s)\mu^{kj}(s)\rho(k,j,s)^{1_{\{j\in \mathcal J_1\}}}-p_\rho^{ij}(t,s)\sum_{k\in \mathcal J,k\ne j}\mu^{jk}(s),
$$

_with boundary condition $p_\rho^{ij}(t,t)=1$._

</blockquote>

Notice that $p_\rho^{ij}(t,s)=0$ and $\mu^{ij}(t)=0$ for all $t\le s$ with $i\in\mathcal J_1$ and $j\in\mathcal J_0$, hence the above may be simplified for some transitions.

## Transaction time modeling

This section gives a brief introduction to the real life effects of delay in the information flow and validity of the information present. In the classical Markov model we assume that the filtration $\mathcal F(s)$ is known for all $s\ge t$ and we can at time $t$ compute the reserve

$$
V(t)=\mathbb E\left[\left.\int_t^\infty e^{-\int_t^sr(v)\ dv}\ dB(s)\ \right\vert\ \mathcal F(t)\right].
$$

However, in real life the information at time $t$ is probably not enough to determine this since there are non-negative time between when an event is occurred, reported and eventually recognized. We study the terms transaction time and valid time (in danish transaktions tid and valør tid).

### The basic model

Let us lay the foundations for this study in the definition:

<blockquote class = "def">

**Definition 2.1. (Valid time model)** _Let $(X_t)_{t\ge 0}$ be a pure jump process on an finite state space $\mathcal J$ with bounded transition $(\mu_{jk}(t))_{t\ge 0,\ j,k\in\mathcal J}$ rates on compact sets and initial value $X_0=x_0$. We define the jump times $(\tau_n)_{n\in\mathbb N}$ recursively as $\tau_0=0$ and_

$$
\tau_n=\inf\{s\ge \tau_{n-1} : X_s\ne X_{\tau_{n-1}}\},\quad n\ge 1
$$

_and the associated counting process_

$$
\langle\langle t \rangle\rangle=\sum_{i=1}^\infty 1_{(\tau_i\le t)}.
$$

_We will further define $H_t$ as_

$$
H_t=\left(\tau_1,...,\tau_{\langle\langle t \rangle\rangle};X_{\tau_1},...,X_{\tau_{\langle\langle t \rangle\rangle}}\right)
$$

_and the mapping $s\mapsto f_{H_t}(s)$ given $t$ as_

$$
f_{H_t}(s)=\sum_{i=0}^{\langle\langle t \rangle\rangle}X_{\tau_i}1_{(\tau_{i}\le s< \tau_{i+1})}
$$

_with $\tau_{\langle\langle t \rangle\rangle + 1}=\infty$. Furthermore, we define $\mathcal F^X_t=\sigma(X_s : 0\le s\le t)$._

</blockquote>

Obviously, we see that

$$
f_{H_t}(t)=X_t.
$$

We further use the convention $\inf\ \emptyset=\infty$.

We can define the cash flow in the valid time model.

<blockquote class = "def">

**Definition 2.2. (Cash Flow - Valid time model)** _Let $A\subseteq \mathbb R^+$ and define the collection $x_A=(X_s)_{s\in A}$. We define the cash flow gained in $A$ as_

$$
B(x_A,dt)=1_A(t)\sum_{j\in \mathcal J}\left(1_{(X_{t-}=j)}B_j(dt)+\sum_{k\in \mathcal J, k\ne j}b_{jk}(t)n_{jk}(x_A,dt)\right),
$$

_with_

$$
n_{jk}(x_A,t)=\#\Big\{s\in \mathbb R^+\cap A : X_s=k,X_{s-}=j\Big\}.
$$

_We then define the usual cash flow model as_

$$
B(dt)\stackrel{\text{def}}{=}B(x_{\mathbb R^+},dt),\quad N_{jk}(t)\stackrel{\text{def}}{=}n_{jk}(x_{\mathbb R^+},t).
$$

</blockquote>

Furthermore, we define the rate enviroment as follows.

<blockquote class = "def">

**Definition 2.3. (Rate model)** _Let $(r_t)_{t\ge 0}$ be a real-valued random variable. We say that $r$ is the short (spot) rate and define the accumulated compounded payments payed during $A\subseteq \mathbb R^+$ as_

$$
B^\circ(x_A)=\int_0^\infty e^{-\int_0^sr(v)\ dv} B(x_A,ds).
$$

</blockquote>

Note that we will make the assumption that $r$ is deterministic or at least independent of the stochastic process $X_t$. The transaction time model is defined as follows.

<blockquote class = "def">

**Definition 2.4. (Transaction time model)** _Let $(Z_t)_{t\ge 0}$ be a pure jump process on the finite state space $\mathcal K$ as $X_t$ though with bounded transition rates $(\lambda_{jk}(t))_{t\ge 0,\ j,k\in \mathcal K}$ on any compact set on $\mathbb R^+$. We define the jump times $(T_n)_{n\in \mathbb N}$ and number of jumps at time $t$: $\langle t\rangle$ analogous to definition 2.1._

</blockquote>

The connection between $X_t$ and $Z_t$ is that we let $Z_t$ describe the flow of information the insurance company experience and so $Z_t$ determines the actual transactions between the insured. That is we need to construct some payment stream wrt. $Z_t$ such that when the insurance company knows the entire path of $X_t$ the contract will be settled with the appropriate rate accumulated. To this we define the following history process.

<blockquote class = "def">

**Definition 2.5. (History process)** _Let $(H^t_s)_{0\le s\le t}$ be a double-indexed stochastic process such that for each fixed $t$ the process $(H^t_s)_{0\le s\le t}$ is a pure jump process on $\mathcal J$ and that as $t \to \infty$ we have $(H^t_s)\to (X_s)$._

</blockquote>

The history process will be constructed in such a way that at time $t$ given the information gathered through the filtration $\mathcal F^Z_t=\sigma (Z_s : 0\le s\le t)$ up until this time point $H_s^t$ will be the assumed state of the process $X_t$ given $\mathcal F^Z_t$. We will consider the obvious construction called the *basic bi-temporal model*.

<blockquote class = "def">

**Definition 2.6. (Basic Bi-Temporal Model)**\index{Basic Bi-Temporal Model} _We say that $(X_t,Z_t,(H^t_s)_{0\le s\le t})_{t\ge 0}$ satisfy the basic bi-temporal assumptions if_

  (i) _$(H^t_s)=(H^{T_{\langle t\rangle}}_s)$ for all $t\ge 0$,_
  (ii) _there exists an $0\le \eta <\infty$ such that $Z_t=Z_\eta$ for all $t\ge \eta$ with probability one and_
  (iii) _$H^t_s$ is constant for all $\eta \le s\le t$ and we set $H_s=H^\eta_s$._

_We furthermore define $\mathcal Z_t=(Z_t,(H^t_s)_{0\le s\le t})$ and let $\mathcal F^{\mathcal Z}_t=\sigma(\mathcal Z_s : 0\le s\le t)$._

</blockquote>

The interpretations of the above is that (i) the assumed state of $X_t$ cannot change in between the jumps of $Z_t$ i.e. we gain no information outside the jumptimes $(T_n)$, (ii) we will at some finite time know for certain the entire path of $X_t$ and so be able to settle all payments and (iii) at this time the process $X_t$ is absorbed and we have no further jumps in the valid time model. Furthermore, at this time we will have the entire history of $X$. Notice, that this actually gives a framework such that the time $\eta$ may be a bankruptcy time or surrender time and so $H$ may differ from $X$ and only serve as the final settlement time for the contract.

From this construction we set

$$
H_s^t=\left( \tau_1^t,..., \tau_{\langle\langle s\rangle \rangle }^t; X_{\tau_1^t}^t,..., X_{\tau_{\langle\langle s\rangle \rangle }^t}^t\right),\quad X_s^t=f_{H^t_s}(s),
$$

where the superscript refer to the presumed history of $X$ given the information at time $t$. Notice that $\mathcal F^{\mathcal Z}_s$ is indeed known for all $s\le t$ at time $t$ where $\mathcal F^X_s$ is only known for all $0\le s\le t$ at some time $t\ge \eta$. Our endavours during the timespan $[0,\eta)$ is to transact such the payments paid up until time $t$ is consistent with $B((X_s^t)_{0\le s\le t},dt)$ on $[0,t]$. We therefore define the payments in transaction time as follows

<blockquote class = "def">

**Definition 2.7. (Cash Flow - Transaction time model)** _We define the cash flow gained at time $t$ in transaction time as_

$$
\tilde B(dt)= B(X^t_{[0,\infty)},dt)+d\left(\sum_{0\le s\le t}e^{\int_0^sr(v)\ dv}\Big(B^\circ(X^s_{[0,s]})-B^\circ(X^{-s}_{[0,s]})\Big)\right)
$$

_with $\tilde B(0)=B(0)$._

</blockquote>

The dynamics is read as follows: We pay out at time $t$ the payments in valid time as if we know the value of $X$ at time $t$ but use the assumed state of $X$ and then correct backwards in time whenever we gain new informations such that the sample path $X^{s}_{[0,s]}$ differs from $X^{s-}_{[0,s]}$. The accumulator in from ensures that the missing payment at time $u\le s$ is multiplied with the factor $e^{\int_0^sr(v)\ dv}e^{-\int_0^u r(v)\ dv}= e^{\int_u^sr(v)\ dv}$, so no arbitrage is possible. Notice that

$$
B(X^t_{[0,\infty)},dt)=\sum_{n=0}^\infty 1_{(\langle t\rangle=n)}B(X^{T_n}_{[0,\infty)},dt).
$$

### Present values and reserves

The present value in valid time is defined as

$$
PV(t)=\int_t^\infty e^{-\int_t^s r(v)\ dv}B(ds)
$$

with associated reserve

$$
V(t)=\mathbb E\left[PV(t)\ \vert\ \mathcal F^X_t\right].
$$

We may define the transaction time present value and reserve as

$$
\tilde{PV}(t)=\int_t^\infty e^{-\int_t^s r(v)\ dv}\tilde B(ds),\quad \tilde V(t)=\mathbb E\left[\tilde {PV}(t)\ \vert\ \mathcal F^{\mathcal Z}_t\right].
$$

The present value in valid time takes the form:

<blockquote class = "lem">

**Lemma 2.8.** _The valid time present value is_

$$
PV(t)=e^{\int_0^t r(v)\ dv}\Big(B^\circ(X^\eta_{[0,\infty)})-B^\circ(X^\eta_{[0,t]})\Big).
$$

</blockquote>
<details>
<summary>**Proof.**</summary>

We see that by definition
\begin{align*}
PV(t)&=\int_t^\infty e^{-\int_t^s r(v)\ dv}B(ds)\\
&=\int_0^\infty e^{-\int_t^s r(v)\ dv}B(ds)-\int_0^t e^{-\int_t^s r(v)\ dv}B(ds)\\
&=e^{\int_0^t r(v)\ dv}\left(\int_0^\infty e^{-\int_9^s r(v)\ dv}B(ds)-\int_0^t e^{-\int_0^s r(v)\ dv}B(ds)\right)\\
&=e^{\int_0^t r(v)\ dv}\Big(B^\circ(X^\eta_{[0,\infty)})-B^\circ(X^\eta_{[0,t]})\Big),
\end{align*}
as desired. $\blacksquare$

</details>

We can relate the present value in transaction time and valid time by first showing the lemma below.

<blockquote class = "lem">

**Lemma 2.9.** _The transaction time present value is_

$$
\tilde{PV}(t)=e^{\int_0^t r(v)\ dv}\Big(B^\circ(X^\eta_{[0,\infty)})-B^\circ(X^t_{[0,t]})\Big).
$$

</blockquote>
<details>
<summary>**Proof.**</summary>

Consider first that if $t\ge \eta$ we have
\begin{align*}
\tilde{PV(t)}&=PV(t)=e^{\int_0^t r(v)\ dv}\Big(B^\circ(X^\eta_{[0,\infty)})-B^\circ(X^\eta_{[0,t]})\Big)\\
&=e^{\int_0^t r(v)\ dv}\Big(B^\circ(X^\eta_{[0,\infty)})-B^\circ(X^t_{[0,t]})\Big),
\end{align*}
using that $B^\circ(X^t_{[0,t]})=B^\circ(X^\eta_{[0,t]})$ for all $t\ge \eta$. 

Next let $t<\eta$. By definition of $\tilde B$ we have
\begin{align*}
\tilde{PV}(t)&=\int_t^\infty e^{-\int_t^s r(v)\ dv}\tilde B(ds)\\
&= \int_t^\infty e^{-\int_t^s r(v)\ dv}B(X^t_{[0,\infty)},ds)+\sum_{t< s<\infty}\underbrace{\Big(B^\circ(X^s_{[0,s]})-B^\circ(X^{-s}_{[0,s]})\Big)}_{\beta_s}\\
&=\sum_{n=0}^\infty\int_t^\infty e^{-\int_t^s r(v)\ dv}1_{(\langle s\rangle = n)}B(X^{T_n}_{[0,\infty)},ds)+\sum_{n=\langle t\rangle +1}^\infty\beta_{T_n}\\
&=\sum_{n=\langle t\rangle}^\infty\int_t^\infty e^{-\int_t^s r(v)\ dv}1_{(\langle s\rangle = n)}B(X^{T_n}_{[0,\infty)},ds)+\sum_{n=\langle t\rangle +1}^\infty\beta_{T_n}\\
&=\int_t^{T_{\langle t\rangle +1}}e^{-\int_t^s r(v)\ dv}B(X^{T_{\langle t\rangle}}_{[0,\infty)},ds)+\beta_{T_{\langle t\rangle +1}}\\
&+\sum_{n=\langle t\rangle +1}^{\langle \eta\rangle-1}\left(\int_{T_n}^{T_{n +1}}e^{-\int_t^s r(v)\ dv}B(X^{T_n}_{[0,\infty)},ds)+\beta_{T_{n +1}}\right)\\
&+\int_\eta^\infty e^{-\int_t^s r(v)\ dv}B(X^{\eta}_{[0,\infty)},ds).
\end{align*}
Hence we can rewrite the present value as
\begin{align*}
\tilde{PV}(t)&=\underbrace{B^\circ\Big(X^{T_{\langle t\rangle}}_{[0,T_{\langle t\rangle})}\Big)-B^\circ\Big(X^{T_{\langle t\rangle}}_{[0,t]}\Big)}_{\int_t^{T_{\langle t\rangle +1}}e^{-\int_t^s r(v)\ dv}B(X^{T_{\langle t\rangle}}_{[0,\infty)},ds)}+\underbrace{B^\circ\Big(X^{T_{\langle t\rangle+1}}_{[0,T_{\langle t\rangle+1})}\Big)-B^\circ\Big(X^{T_{\langle t\rangle}}_{[0,T_{\langle t\rangle + 1})}\Big)}_{\beta_{T_{\langle t\rangle +1}}}\\
&+\sum_{n=\langle t\rangle +1}^{\langle \eta\rangle-1}\left(B^\circ\Big(X^{T_{n}}_{[0,T_{n+1})}\Big)-B^\circ\Big(X^{T_{n}}_{[0,T_n)}\Big)+B^\circ\Big(X^{T_{n+1}}_{[0,T_{n+1})}\Big)-B^\circ\Big(X^{T_{n}}_{[0,T_{n+1})}\Big)\right)\\
&+B^\circ\Big(X^{\eta}_{[0,\infty)}\Big)-B^\circ\Big(X^{\eta}_{[0,\eta)}\Big).
\end{align*}
This sum telescopes such that we obtain
\begin{align*}
\tilde{PV}(t)&=B^\circ\Big(X^{T_{\langle t\rangle +1}}_{[0,T_{\langle t\rangle +1})}\Big)-B^\circ\Big(X^{T_{\langle t\rangle }}_{[0,t]}\Big)+B^\circ\Big(X^{\eta}_{[0,\eta)}\Big)-B^\circ\Big(X^{T_{\langle t\rangle +1}}_{[0,T_{\langle t\rangle +1})}\Big)\\
&+B^\circ\Big(X^{\eta}_{[0,\infty)}\Big)-B^\circ\Big(X^{\eta}_{[0,\eta)}\Big)\\
&=B^\circ\Big(X^{\eta}_{[0,\infty)}\Big)-B^\circ\Big(X^{t}_{[0,t]}\Big)
\end{align*}
using that $X^{T_{\langle t\rangle }}_{[0,t]}=X^{t}_{[0,t]}$. Thus we obtain the desired result.$\blacksquare$

</details>

Combining lemma 2.8 and 2.9 we trivially obtain the relation.

<blockquote class = "thm">

**Theorem 2.10.** _The transaction and valid time present value relates as follows_

$$
\tilde{PV}(t)=PV(t) +e^{\int_0^t r(v)\ dv}\Big(B^\circ(X^\eta_{[0,t]})-B^\circ(X^t_{[0,t]})\Big).
$$

</blockquote>

The proof is trivial and therefore skipped.

The reserve in transaction time cannot be easily computed without making any independence assumptions on the filtrations $\mathcal F^X$ and $\mathcal F^{\mathcal Z}$. Furthermore, it is non-constructive to generalize the reserve given appropriate assumptions. In general, we need to make the assumption that $\mathcal F^X_\infty$, that is $\sigma\left(\bigcup_{t\ge 0} \mathcal F^X_t\right)$, is independent of $\mathcal F^{\mathcal Z}_t\ \vert\ \mathcal F^X_t$. In this case, we can compute explicitly the transaction time reserve using a weighing of the valid time reserve and a *mistake* term by transition probabilities in $X_t$ given $\mathcal F^{\mathcal Z}_t$. In general we would have something like

$$
\tilde V_{j}(t)=\mathbb E\left[\left.\tilde {PV}(t)\ \right\vert\ Z_t= j\right]=\sum_{i\in \mathcal J}\mathbb P(X_{\theta_j(t)}=i\ \vert\ \mathcal F^{\mathcal Z}_t)\left(V_i(t)+\int_{\theta_j(t)}^t e^{\int_s^t r(v)\ dv}(B_i-B_j)(ds)\right),
$$

where $\theta_j(t)=\sup\bigcup_{0\le s\le t}\inf\{0\le u\le s: Z_u=j, Z_{u-}\ne j\}$ is the latest jumptime to the state $j$. The interpretation is that we reserve in the sum of all statewise valid time reserves by the probability that the underlying jump process is at the jump time of $Z$ was in that state. We also add up the mistakes this would have caused during the time span since the jump and now and weigh these *mistake* terms by the same probability. It is however not clear how one may estimate the desired probability.

## Parametric estimation in Markov models

A way to represent a Markov chain is through a marked point process $(T_n,Y_n)_{n\in\mathbb N}$ where $T_n \in (0, \infty]$ is the $n$’th jump-time and $Y_n \in \mathcal J \cup \{\nabla\}$ is where the process jumps to at time $T_n$, also known as the $n$’th jump-mark. Here $\nabla$ is the mark corresponding to a jump that does not occur, that is $Y_n =\nabla$ if and only if $T_n =\infty$.

The distribution of $X$ is most easily characterized in terms of the marked point process representation. Since the primarily focus of this section is maximum likelihood estimation, and the likelihood is closely related to the distribution of $X$, we mainly work with the marked point process representation. Let $H_n = (T_1,Y_1,...,T_n,Y_n)$ be the first $n$ jump-times and jump-marks. We will make the conventions $T_0 = 0, Y_0 = x_0$ and $H_0 = (T_0,Y_0)$ as the case with no jumps then follows the same formulae as the case with jumps. The distribution of a marked point process is characterized by the conditional jump-time and jump-mark distributions
\begin{align*}
F_{H_n}^{(n+1)}(s)&=\mathbb P(T_{n+1} \le s\ \vert\ H_n),\\
G_{H_n,T_{n+1}}^{(n+1)}(j)&=\mathbb P(Y_{n+1} = j\ \vert\ H_n,T_{n+1}).
\end{align*}
Such a sequence of conditional distributions leads to a joint distribution of $(T_n,Y_n)_{n\in\mathbb N}$ with the desired conditional distributions follows from the Ionescu-Tulcea theorem (see e.g. Theorem 5.17 of Kallenberg (1997)). We will often denote this distribution with the letter $Q$.

Introduce non-negative measurable functions $\mu_{jk}$ with $j,k \in \mathcal J$ and $k\ne j$) which we shall assume bounded on any compact set. The marked point process corresponding to a Markov chain with transition rates $\mu_{jk}$ can be defined by the specification
\begin{align*}
\overline F_{H_n}^{(n+1)}(s)&=\exp\left(-\int_{T_n}^s\mu_{Y_n\cdot}(u)\ du \right),\\
\overline G_{H_n,T_{n+1}}^{(n+1)}(j)&=\frac{\mu_{Y_nj}(T_{n+1})}{\mu_{Y_n\cdot}(T_{n+1})}.
\end{align*}
with the usual definition $\mu_{j\cdot}(t)=\sum_{k\in \mathcal J, k\ne j}\mu_{jk}(t)$. Thus we can think of this as a in homogeneous Poisson arrival model with associated claim sizes $Y_n$ although with the major difference, that the renewal time depends on the most recent "*claim size*". We can formally define the markov chain as

$$
X_t=\sum_{n=1}^\infty Y_n1_{(T_n\le t <T_{n+1})}.
$$

### Likelihood without censoring

Let the background probability space be denoted $(\Omega,\mathcal F,\mathbb P)$ and let $\mathcal F_t \subseteq \mathcal F$ be the filtration generated by $(T_n, Y_n)_{n\in \mathbb N}$. Let $\mathcal P$ be a statistical model i.e. a collection of probability measures on $(\Omega,\mathcal F,\mathbb P)$. We think of these as candidates for the distribution of $(T_n, Y_n)_{n\in \mathbb N}$. For elements $Q, \tilde Q \in\mathcal P$ , we denote by $Q_t$ and $\tilde Q_t$ the restrictions of the measures $Q$, $\tilde Q$ to the sigma-algebra $\mathcal F_t$.

<blockquote class = "def">

**Definition 3.1.** _Let $Q$ and $\tilde Q$ be two probability measures. We say that $\tilde Q$ is absolutely locally continuous wrt. $Q$ if for all $t\ge 0$ it holds_

$$
\forall F\in\mathcal F_t: Q_t(F)=0\Rightarrow\tilde Q_t(F)=0.
$$

_If so we write $\tilde Q\stackrel{\text{loc}}{<<}Q$._

</blockquote>

If we have such two measurus then by the Radon-Nikodym Theorem we can define the *likelihood process*

$$
\mathcal L_t=\frac{d \tilde Q_t}{d Q_t}.
$$

Let $\tilde F$ and $\tilde G$ be the conditional jump-time and jump-mark distributions for $\tilde Q$ and likewise use $F$ and $G$ for $Q$. Denote by $h_n = (t_1, y_1, ..., t_n, y_n)$ a generic outcome of $H_n$ and let $\langle t\rangle = \sum_{n=1}^\infty 1_{(T_n\le t)}$ be the number of jumps that have occurred at time $t$.

<blockquote class = "prop">

**Proposition 3.2.** _Assume that the Radon-Nikodym derivatives_

$$
\frac{d\tilde F^{(n)}_{h_{n-1}}}{dF^{(n)}_{h_{n-1}}}(t)\quad \text{and}\quad \frac{d\tilde G^{(n)}_{h_{n-1},t_n}}{dG^{(n)}_{h_{n-1},t_n}}(k),
$$

_of the jump-time and jump-mark distributions exist. Then $\tilde Q$ is locally absolutely continuous wrt. $Q$ and the likelihood process takes the form_

$$
\mathcal L_t=\left(\prod_{n=1}^{\langle t\rangle} \frac{d\tilde F^{(n)}_{H_{n-1}}}{dF^{(n)}_{H_{n-1}}}(T_n) \frac{d\tilde G^{(n)}_{H_{n-1},T_n}}{dG^{(n)}_{H_{n-1},T_n}}(Y_n)\right)\frac{{\overline {\tilde F}}^{(\langle t\rangle +1)}_{H_{\langle t\rangle}}(t)}{{\overline F}^{(\langle t\rangle +1)}_{H_{\langle t\rangle}}(t)}
$$

_where ${\overline F}^{(\langle t\rangle +1)}_{H_{\langle t\rangle}}(t)=Q(T_{\langle t\rangle +1}\le t\ \vert\ H_{\langle t\rangle})$ and likewise for $\tilde Q$._

</blockquote>

Proposition 3.2 holds for a general marked point process, and may hence also be used for semi- Markov processes, processes with point-mass in their distributions etc. For Markov chains with transition rates, the likelihood simplifies. Let $\tilde \mu_{jk}$ be the transition rates for $\tilde Q$ and $\mu_{jk}$ be the transition rates for $Q$.

<blockquote class = "prop">

**Corollary 3.3.** _In the case where $Q$ and $\tilde Q$ are distributions of a Markov chain with transition rates, the likelihood $\mathcal L$ is given by_
\begin{align*}
\mathcal L_t&=\left(\prod_{n=1}^{\langle t\rangle} \frac{e^{-\int_{T_n-1}^{T_n}\tilde \mu_{Y_{n-1}\cdot}(s)\ ds}\tilde \mu_{Y_{n-1}\cdot}(T_n)}{e^{-\int_{T_n-1}^{T_n} \mu_{Y_{n-1}\cdot}(s)\ ds} \mu_{Y_{n-1}\cdot}(T_n)}\cdot \frac{\tilde \mu_{Y_{n-1}Y_n}(T_n)/\tilde \mu_{Y_{n-1}\cdot }(T_n)}{\mu_{Y_{n-1}Y_n}(T_n)/\mu_{Y_{n-1}\cdot }(T_n)}\right)\frac{e^{-\int_{T_{\langle t\rangle}}^t \tilde \mu_{Y_{\langle s-\rangle \cdot}}(s)\ ds}}{e^{-\int_{T_{\langle t\rangle}}^t  \mu_{Y_{\langle s-\rangle \cdot}}(s)\ ds}}\\
&=\frac{e^{-\int_0^t \tilde \mu_{Y_{\langle s-\rangle \cdot}}(s)\ ds}}{e^{-\int_0^t  \mu_{Y_{\langle s-\rangle \cdot}}(s)\ ds}}\left(\prod_{n=1}^{\langle t\rangle} \frac{\tilde \mu_{Y_{n-1}Y_n}(T_n)}{\mu_{Y_{n-1}Y_n}(T_n)}\right).
\end{align*}
_in other words_

$$
\mathcal L_t\propto e^{-\int_0^t \tilde \mu_{Y_{\langle s-\rangle \cdot}}(s)\ ds}\prod_{n=1}^{\langle t\rangle} \tilde \mu_{Y_{n-1}Y_n}(T_n).
$$

</blockquote>

We will henceforth switch measure notation and simply say that

$$
\mathcal L_t\propto \exp\left(-\int_0^t  \mu_{Y_{\langle s-\rangle \cdot}}(s)\ ds\right)\prod_{n=1}^{\langle t\rangle}  \mu_{Y_{n-1}Y_n}(T_n).
$$

### Likelihood with right-censoring

With our general results from the previous section, we are able to describe what is basically the most general form of right-censoring that still leaves the underlying transition rates identifiable from the observable data, namely independent right-censoring.

Let $(T_n^*,Y_n^*)_{n\in \mathbb N}$ be the marked point process corresponding to a Markov chain on the state-space $\mathcal J$ with transition rates $\mu^*_{jk}$. We will now extend the state-space with an extra absorbing state $(J + 1)$ which represents the censored state. Let $C > 0$ be a random variable which we think of as the censoring time. We construct a new marked point process $(T_n,Y_n)_{n\in\mathbb N}$ from $(T_n^*, Y_n^*)_{n\in\mathbb N}$ by deleting all jumps with $T_n^* > C$ and adding a jump $(C, J +1)$. We assume that the marked point process $(T_n, Y_n)_{n\in\mathbb N}$ leads to a Markov chain on the state-space $\mathcal J \cup \{J + 1\}$ with transition rates $\mu_{jk}$ with $j,k \in \mathcal J \cup \{J + 1\}$ and $j\ne k$. Note that we do not automatically have $\mu_{jk} = \mu_{jk}^*$ for $j,k \in\mathcal J$ since for the jump-time distribution, we now also condition on censoring not having occurred yet. We elaborate on this point after stating Assumption 1. In total, we specify the jump-time and jump-mark distributions of $(T_n,Y_n)_{n\in\mathbb N}$ as
\begin{align*}
\overline F_{H_n}^{(n+1)}(s)&=\exp\left(-\int_{T_n}^s\mu_{Y_n\cdot}(u)\ du \right),&\qquad\text{on}\ (s>T_n),\\
\overline G_{H_n,T_{n+1}}^{(n+1)}(k)&=\frac{\mu_{Y_nk}(T_{n+1})}{\mu_{Y_n\cdot}(T_{n+1})},&\qquad\text{on}\ (k\ne Y_n).
\end{align*}
We are interested in estimating the underlying transition rates $\mu^*_{jk}$, but only observe the marked point process $(T_n,Y_n)_{n\in\mathbb N}$. In order for this to be possible, we impose Assumption 1, called *independent right-censoring*.

**Assumption 1.** _For $j,k\in\mathcal J$ with $j\ne k$, it holds that $\mu_{jk}(s)=\mu^*_{jk}(s)$ for all $s\ge 0$._

In other words, the transition rates to the original states are unchanged by incorporating censoring. This means that the information that the subject has not yet been censored has no effect on the probability of making a given transition. One situation where this assumption would be violated would be a two-state life-death model $J = \{1, 2\}$ where frail people have a higher rate of censoring/drop-out, because the death-transition rate of the non-censored $\mu_{12}$ would then be lower than the population average $\mu^*_{12}$. Note also that the transition rate to the censored state $(J + 1)$ is allowed to depend on the current state i.e. $\mu_{k,J+1}$ may be different from $\mu_{j,J+1}$. Hence, the assumption of independent right-censoring is weaker than the assumption that the censoring time $C$ is independent of $(T_n^* ,Y_n^*)_{n\in\mathbb N}$. This latter assumption is called *entirely random right-censoring*.

As a final remark, note that independent right-censoring is only strictly needed if we really want to estimate $\mu^*_{jk}$ instead of $\mu_{jk}$. A life-insurance company will most often be interested in estimating the transition rates for the insureds that stay in the portfolio (i.e. $\mu_{jk}$), since these are the ones generating premium and benefit payments, and care less about whether these transition rates equal the transition rates of the group of people that originally entered the portfolio (i.e. $\mu^*_{jk}$). In life-insurance, one can think of the censoring as being explicitly modeled through policyholder options (freepolicy and surrender). For other biostatistical applications such as medical studies, the object of interest is however almost always $\mu^*_{jk}$.

We may factorize the likelihood into a part relating to censoring and one relating to the biometric transition rates as follows:
\begin{align*}
\mathcal L_t&\propto \exp\left(-\int_0^t  \sum_{k\in\mathcal J}\mu_{Y_{\langle s-\rangle k}}(s)\ ds\right)\left(\prod_{n=1,Y_n\ne J+1}^{\langle t\rangle}  \mu_{Y_{n-1}Y_n}(T_n)\right)\\
&\times\exp\left(-\int_0^t  \mu_{Y_{\langle s-\rangle J+1}}(s)\ ds\right)\left(\prod_{n=1,Y_n= J+1}^{\langle t\rangle}  \mu_{Y_{n-1}Y_n}(T_n)\right)
\end{align*}
and under Assumption 1, one may substitute $\mu^*_{jk}$ for $\mu_{jk}$ when $k\ne  J + 1$.

### Maximum likelihood estimation

We here present maximum likelihood under independent right-censoring, but the case where $\mu_{jk}$ are the objects of interest is completely analogous. To make the notation less cumbersome, we also only consider one path of the marked point process, but note that with $n$ i.i.d. subjects, the likelihood $\mathcal L_t$ becomes a product over the likelihood for each of the $n$ observed paths which can be handled analogously.

For parametric estimation, we assume that the statistical model is of the form $\mathcal P = \{P_\theta : \theta\in\Theta\}$ where $\Theta$ is a subset of some Euclidean space $\mathbb R^p$ and that the true distribution is in this family for some value $\theta_0$ of the parameter. We assume that the subject has some baseline covariates $W$ that are known at time 0 i.e. we extend the filtration with $W$ such that $W$ is $\mathcal F_0$-measurable. Then the arguments leading to the likelihood in Proposition 3.2 still hold, but now under the conditional measure given $\mathcal F_0$. We write $\mu^*_{jk}(s,W;\theta)$ and $\mu_{j,J+1}(s,W;\theta)$ for the transition rates corresponding to $P_\theta$ and $\mathcal L_t(\theta)$ for the likelihood. The maximum likelihood estimator is defined as $\hat θ= \arg\max_\theta \mathcal L_t(\theta)$ and under some standard regularity assumptions, this estimator is consistent and asymptotically normal for $\theta_0$. To avoid having to model the censoring mechanism, we impose Assumption 2, called *non-informative right-censoring*.

**Assumption 2.** _The parameter space factorizes into a product space $\Theta = \Theta_1 \times \Theta_2$. For $\theta = (\theta_1,\theta_2)$ with $\theta_1 \in\Theta_1$ and $\theta_2 \in\Theta_2$, the transition rates $\mu^*_{jk}(s,W;\theta)$ for $j,k \in\mathcal J$ and $j\ne k$ only depend on $\theta$ through $\theta_1$ and $\mu_{j,J+1}(s,W;\theta)$ for $j \in\mathcal J$ only depends on $\theta$ through $\theta_2$._

This assumption is quite weak, as it is usually unnatural to assume that the censoring and biometric transition hazards are not able to be specified separately. It would be violated if we e.g. assumed that one of the censoring transition rates was proportional to one of the biometric transition rates.

When Assumption 2 holds, the likelihood is on the form $\mathcal L_t(\theta) = \mathcal L_t(\theta_1) \mathcal L_t(\theta_2)$ with

$$
\mathcal L_t(\theta_1)= \exp\left(-\int_0^t  \sum_{k\in\mathcal J}\mu^*_{Y_{\langle s-\rangle k}}(s,W;\theta_1)\ ds\right)\left(\prod_{n=1,Y_n\ne J+1}^{\langle t\rangle}  \mu^*_{Y_{n-1}Y_n}(T_n,W;\theta_1)\right).
$$

The likelihood $\mathcal L_t(\theta)$ as a function of $\theta_1$ is thus proportional to $\mathcal L_t(\theta_1)$, which is the same as if there had been no censoring. Hence when Assumption 1 and 2 hold, one may ignore the right-censoring when making inference about $\theta 1$. Remarkably, the estimator based on $\mathcal L_t(\theta_1)$ stays consistent and asymptotically normal even if Assumption 2 does not hold, but it will have a larger asymptotic variance than the estimator based on $\mathcal L_t(\theta)$.

Write $\ell_t(\theta_1) = \log\mathcal L_t(\theta_1)$ for the log-likelihood of the biometric transition rates. Note that we can write
\begin{align*}
\ell_t(\theta_1)&= \sum_{n=1,Y_n\ne J+1}^{\langle t\rangle}  \log\left(\mu^*_{Y_{n-1}Y_n}(T_n,W;\theta_1)\right)-\int_0^t  \sum_{k\in\mathcal J}\mu^*_{Y_{\langle s-\rangle k}}(s,W;\theta_1)\ ds\\
&=\sum_{j\in \mathcal J}\sum_{k\in \mathcal J, k\ne j}\int_0^t\log\left(\mu^*_{Y_{n-1}Y_n}(T_n,W;\theta_1)\right)\ dN_{jk}(s)-\int_0^t 1_{(X_s=j)}\mu^*_{jk}(s,W;\theta_1)\ ds.
\end{align*}
A trick that allows one to use off-the-shelf statistical software to find the maximum likelihood estimator is to make a discrete approximation of the log-likelihood. Make a partition $0 = t_0 < t_1 < ... < t_M = t$ of the interval $[0,t]$ and write $O_{jk}(t_m) = N_{jk}(t_m) − N_{jk}(t_{m−1})$ and $E_j(t_m) = 1_{(X_{t_m} =j)}(t_m − t_{m−1})$ which we call the occurrences and exposures respectively. Then

$$
\ell_t(\theta_1)\approx \sum_{j\in \mathcal J}\sum_{k\in \mathcal J, k\ne j}\sum_{m=1}^M\log\left(\mu^*_{Y_{n-1}Y_n}(T_n,W;\theta_1)\right)O_{jk}(t_m)-\mu^*_{jk}(s,W;\theta_1)E_j(t_m).
$$

Now since $\log(\mu^*_{jk}(t_m,W;\theta_1)E_j(t_m)) = \log(\mu^*_{jk}(t_m,W;\theta_1))+\log(E_j(t_m))$ and $\log(E_j(t_m))O_{jk}(t_m)$ does not depend on $\theta_1$, the log-likelihood is up to an additive constant approximately equal to

$$
\sum_{j\in \mathcal J}\sum_{k\in \mathcal J, k\ne j}\sum_{m=1}^M\log\left(\mu^*_{Y_{n-1}Y_n}(T_n,W;\theta_1)E_j(t_m)\right)O_{jk}(t_m)-\mu^*_{jk}(s,W;\theta_1)E_j(t_m)
$$

The expression $\log\left(\mu^*_{Y_{n-1}Y_n}(T_n,W;\theta_1)E_j(t_m)\right)O_{jk}(t_m)-\mu^*_{jk}(s,W;\theta_1)E_j(t_m)$ can be recognized as the log-likelihood of a Poisson random variable with outcome $O_{jk}(t_m)$ and mean equal to $\mu^*_{jk}(t_m,W;\theta_1)E_j(t_m)$. Since independent random variables leads sums in the log-likelihood, one may input $(O_{jk}(t_m))_{j,k\in\mathcal J,m=1,...,M}$ as independent outcomes into a Poisson regression where the mean is modeled as $\mu^*_{jk}(t_m,W;\theta_1)E_j(t_m)$, treating $E_j(t_m)$ and $W$ as non-stochastic. If there are $n$ i.i.d. subjects, each subject will contribute with a vector of such outcomes $(O_{jk}(t_m))_{j,k\in\mathcal J,m=1,...,M}$, and the Poisson regression may still be used with these as the observations. Additionally, if any of these outcomes between subjects have the same mean (e.g. from having the same covariates $W$ at the same time $t_m$), we see from the form of the log-likelihood that the outcomes and exposures between these subjects may be summed before inputting into the Poisson regression as a single observation.

A final reparametrization makes this fit into the generalized linear model (GLM) framework: Since $\mu^*_{jk}$ and $E_j(t_m)$ are non-negative, we may write

$$
\mu^*_{jk}(t_m,W;\theta_1)E_j(t_m)=\exp\left(\log\left(\mu^*_{jk}(t_m,W;\theta_1)\right)+\log\left(E_j(t_m)\right)\right).
$$

In the language of generalized linear models, the outcome $O_{jk}(t_m)$ is modeled with the Poisson family, log-link, offset $\log(E_j(t_m))$ and mean $\log(\mu^*_{jk}(t_m,W;\theta_1))$.

As an example, take a two-state life-death model $\mathcal J = \{1, 2\}$. Let $W$ be the age at time 0 and let $\theta_1 =(\beta_0,\beta_1)$. We model the transition rate as $\mu^*_{12}(t,W;\theta_1)=\exp(\beta_0+\beta_1(t+W))$. In `R`, this could be fitted by executing `glm(occurrence ∼ 1+age+offset(log(exposure)), data, family = poisson(link = "log"))`.

We reiterate that all of this is purely a trick to make implemented statistical packages solve our likelihood equations; we are never actually assuming that $(O_{jk}(t_m))_{j,k\in\mathcal J,m=1,...,M}$ are independent Poisson random variables, but only exploit that the likelihood under this specification would be the same as a discretization of our marked point process likelihood.

## Non-parametric estimation in Markov models

We start by stating the important result *Glivenko-Cantelli theorem* that let us extend the almost surely pointwise convergence of the empirical distribution to a almost surely uniform convergence for all $x\in\mathbb R$. This result let us trust the non-parametric distributions we may fit and thus use them as benchmarks for parametric solutions.

<blockquote class = "thm">

**Theorem 4.1.** _Let $X_1,...,X_N$ be i.i.d with distribution function $F_X$, then the empirical distribution_

$$
F_N(x)=\frac{1}{N}\sum_{i=1}^N 1_{(X_i\le x)},
$$

_satisfy_

$$
\sup_{x\in\mathbb R}\left\vert F_N(x)-F_X(x)\right\vert\stackrel{\text{a.s.}}{\to}0,\qquad N\to \infty.
$$

</blockquote>
<details>
<summary>**Proof.**</summary>

Let $\varepsilon>0$ be given. Let $k\ge 2$ and define the increasing sequence $\mathbb K=(K_i)_{i\in \{0,1,...,k\}}$ as

$$
K_i=\sup_{x\in\mathbb R}\left \{F_X(x-)\le \frac{i}{k}\right\}
$$

with $K_0=-\infty$ and $K_k=\infty$. Thus we have a partition of the space $\mathbb R$. Note that some knotpoints may be equal if $F_X$ jumps a larger size than $1/k$. Given $\mathbb K$ we can define

$$
\Delta_N=\max_{j=1,..,k-1}\left\{\vert F_n(K_j)-F_X(K_j)\vert,\vert F_n(K_j-)-F_X(K_j-)\vert\right\}.
$$

One can show that $\Delta_N\stackrel{\text{a.s.}}{\to}0$ for $N\to\infty$. Then it follows that

$$
\sup_{x\in\mathbb R}\left\vert F_N(x)-F_X(x)\right\vert\stackrel{\text{a.s.}}{\to}\varepsilon
$$

as $N\to\infty$. Then by letting $\varepsilon \to 0$ the result follows. $\blacksquare$

</details>

### Modelling the counting process

Let $Z = (Z_t)_{t\ge 0}$ be a non-explosive jump process on a finite state space $\mathcal J$, and take $(\Omega, \mathcal F, \mathbb P)$ to be the underlying probability space. We also introduce a multivariate counting process $N$ with components $N_{jk} = (N_{jk}(t))_{t\ge 0}$ given by

$$
N_{jk}(t)=\#\{s\in (0,t] : Z_{s−} =j,Z_s =k\}
$$

for $j,k\in\mathcal J$ and $j\ne k$. Throughout, we assume that $\mathbb E[N_{jk}(t)]<\infty$, which especially guarantees that $Z$ is non-explosive. The Doob–Meyer decomposition of $N_{jk}$ is

$$
N_{jk}(t)=M_{jk}(t)+\Lambda_{jk}(t),
$$

where $M_{jk}$ is a mean-zero martingale and $\Lambda_{jk}$ is an increasing predictable process with initial value zero.

We focus on Markov models, that is we assume that $Z$ satisfies the Markov assumption. In this case,

$$
\Lambda_{jk}(t)=\int_0^t 1_{(Z_{s-}=j)}q_{jk}(ds).
$$

We let $q(t)=(q_{jk}(t))_{j,k\in\mathcal J}$ and setting $q_{jj}(t)=-\sum_{k\in\mathcal J, k\ne j}q_{jk}(t)$. These quantities are known as *cumulative transition rates*, and we recover the classic case of smooth Markov models whenever

$$
q_{jk}(t)=\int_0^t\mu_{jk}(s)\ ds
$$

for ordinary transition rates $\mu_{jk}$. A sufficient, but not necessary, condition for the existence of all moments of the counting processes is boundedness of the transition rates on compacts.

Recall that the transition probabilities $p_{jk}(t, s) =\mathbb P(Z_s = k\ \vert\ Z_t = j)$ can be represented as a multivariate product integral of the cumulative transition rates:

$$
(p_{jk}(t,s))_{j,k\in\mathcal J}=p(t,s)=\prod_t^s\Big(1+q(du)\Big).
$$

<blockquote class = "lem">

**Lemma 4.2.** _The function $q_{jk}$ is given by_

$$
q_{jk}(t)=\int_0^t\frac{1}{\mathbb P(Z_{s-}=j)}\ d\mathbb E[N_{jk}(s)].
$$

</blockquote>
<details>
<summary>**Proof.**</summary>

Note that by Tonelli’s theorem,

$$
\mathbb E[N_{jk}(s)]=\mathbb E[\Lambda_{jk}(s)]=\int_0^t \mathbb E[1_{(Z_{s-}=j)}] q_{jk}(ds)=\int_0^t \mathbb P(Z_{s-}=j) q_{jk}(ds)
$$

Thus we obtain

$$
\int_0^t\frac{1}{\mathbb P(Z_{s-}=j)}\ d\mathbb E[N_{jk}(s)]=\int_0^t\frac{\mathbb P(Z_{s-}=k)}{\mathbb P(Z_{s-}=k)}\ q_{jk}(ds)=q_{jk}(t)
$$

as desired. $\blacksquare$

</details>

We are interested in the case where observation of $Z$ is right-censored. Let $C$ be a strictly positive random variable describing right-censoring, so that we actually observe the pair

$$
\Big((Z_t)_{0\le t\le C}, \tau \wedge C\Big),
$$

where $\tau$ is the (possibly infinite) absorption time of $Z$. We introduce
\begin{align*}
p^c_j(t)&=\mathbb E\left[1_{(Z_t=j)}1_{(t<C)}\right],\\
p^c_{jk}(t)&=\mathbb E\left[ N_{jk}(t\wedge C)\right].
\end{align*}
<blockquote class = "prop">
**Proposition 4.3.** _Suppose that right-censoring is entirely random, that is $Z \perp \!\!\! \perp C$. Then_

$$
q_{jk}(t)=\int_0^t\frac{1}{p^c_j(s-)}p^c_{jk}(ds).
$$

</blockquote>
<details>
<summary>**Proof.**</summary>

Note that, since $Z \perp \!\!\! \perp C$, we have that

$$
p_j^c(t)=\mathbb P(t< C)\mathbb P(Z_t=j).
$$

Furthermore,

$$
N_{jk}(t\wedge C)=\int_0^t 1_{(s\le C)}N_{jk}(ds),
$$

so that by Tonelli’s theorem and similar to the proof of the previous lemma,

$$
p^c_{jk}(t)=\int_0^t \mathbb P(s\le C)\ d\mathbb E[N_{jk}(s)].
$$

Collecting results yields
\begin{align*}
\int_0^t\frac{1}{p^c_j(s-)}p^c_{jk}(ds)&=\int_0^t\frac{\mathbb P(s\le C)}{\mathbb P(s\le C)\mathbb P(Z_{s-} = j)}\ d\mathbb E[N_{jk}(s)]\\
&=\int_0^t\frac{1}{\mathbb P(Z_{s-} = j)}\ d\mathbb E[N_{jk}(s)]\\
&=q_{jk}(t)
\end{align*}
as desired. $\blacksquare$

</details>

Notice, since $Z$ satisfies the Markov assumption, the condition that right-censoring be entirely random may be replaced by a weaker condition of *independent* right-censoring.

### Estimators

Consider i.i.d. replicates $\Big((Z_t^\ell)_{0\le t\le C^\ell}, \tau^\ell \wedge C^\ell\Big)_{\ell\in \{1,...,n\}}$. We form a non-parametric estimator for the cumulative transition rates by suitably combining standard non-parametric estimators for the components $p^c_j$ and $p^c_{jk}$. To this end, define
\begin{align*}
\mathbb N_{jk}^{(n)}(t)=\frac{1}{n}\sum_{\ell = 1}^n N_{jk}^\ell(t\wedge C^\ell),\\
\mathbb I_{j}^{(n)}(t)=\frac{1}{n}\sum_{\ell = 1}^n 1_{(Z_t^\ell = j)}1_{(t<C^\ell)}.
\end{align*}
Note that, due to the strong law of large numbers,
\begin{align*}
\mathbb N_{jk}^{(n)}(t)&\stackrel{\text{a.s.}}{\to}p_{jk}^c(t),&\quad n\to\infty,\\
\mathbb I_{j}^{(n)}(t)&\stackrel{\text{a.s.}}{\to}p_{j}^c(t),&\quad n\to\infty.
\end{align*}
<blockquote class = "def">
**Definition 4.4.** _The **Nelson-Aalen estimator** for the cumulative transition rates is defined according to_

$$
\hat q_{jk}^{(n)}(t)=\int_0^t \frac{1}{\mathbb I_{j}^{(n)}(s-)}\mathbb N_{jk}^{(n)}(ds).
$$

</blockquote>

<blockquote class = "def">

**Definition 4.4.** _The **Aalen-Johansen estimator** for the transition probabilities is defined according to_

$$
\hat p^{(n)}(t,s)=\prod_t^s\Big(\mathbb I+\hat q(du)\Big),
$$

_employing the convention $\hat q^{(n)}_{jj}=-\sum_{k\in \mathcal J,k\ne j} \hat q_{jk}^{(n)}$._

</blockquote>

#### Convergence and consistency of the estimators

The product integral has many nice properties, most outside the scope of this note, which essentially allows one to easily extend results for the Nelson–Aalen estimator to the Aalen– Johansen estimator. In the following, we therefore focus on the Nelson–Aalen estimator.


We intend to prove strong uniform consistency. Adopting such a strong notion of convergence comes at a cost, though. Ideally, we would like to establish convergence on the maximal interval, which is $[0,\eta)$, where $\eta$ is the (possibly infinite) right endpoint of the support of $C$. In other words, we would like to show that

$$
\sup_{0\le t< \eta}\left\vert \hat q_{jk}^{(n)}-q_{jk}(t)\right\vert\stackrel{\text{a.s.}}{\to}0,\qquad n\to \infty.
$$

But this requires very advanced mathematical tools, so we settle for convergence on compact sets. That is, we fix some $\theta$ strictly below $\eta$, the right endpoint of the support of $C$, and consider the interval $[0, \theta]$. Furthermore, the fractions

$$
\frac{1}{p^c_j(s-)},\quad \frac{1}{\mathbb I^{(n)}_j(s-)},
$$

might blow up of the estimator around time $s$ whenever the event $\{Z_s = j\}$ is rare. There are multiple ways to circumvent this issue. We here settle for the simple but also quite crude assumption that

$$
\inf_{0\le t\le \theta}\mathbb P(Z_t=j)>0.
$$

We begin by establishing strong uniform consistency of the estimators for the components. The following decomposition might be of independent interest.

<blockquote class = "prop">

**Proposition 4.5.** _It holds for all $j\in\mathcal J$ that_

$$
1_{(Z_t=j)}1_{(t<C)}=1_{(Z_0=j)}-1_{(C\le t)}1_{(Z_C=j)}+\sum_{k\in \mathcal J, k\ne j} \left(N_{kj}(t\wedge C)-N_{jk}(t\wedge C)\right).
$$

</blockquote>

The proof is skipped. From the proposition above we see that
\begin{align*}
\mathbb I_{j}^{(n)}(t)&=\mathbb I_{j}^{(n)}(0)-\mathbb C_{j}^{(n)}(t)+\sum_{k\in \mathcal J, k\ne j} \left(\mathbb N_{kj}^{(n)}(t)-\mathbb N_{jk}^{(n)}(t)\right),\\
p_j^c(t)&=p_j^c(0)-C_j(t)+\sum_{k\in \mathcal J, k\ne j} \left(p_{kj}^c(t)-p_{jk}^c(t)\right),
\end{align*}
with $C_j(t)=\mathbb P(C\le t, Z_C=j)$ and

$$
\mathbb C_{j}^{(n)}(t)=\frac{1}{n}\sum_{\ell = 1}^n 1_{(C^\ell \le t)}1_{(Z^\ell_{C^\ell}=j)}.
$$

Again, by the strong law of large numbers,

$$
\mathbb C_{j}^{(n)}(t)\stackrel{\text{a.s.}}{\to}C_{j}(t),\quad n\to\infty.
$$

Note that although $C_j$ is not a distribution function, this is only true up to normalization. So by slightly adapting the proof of the Glinveko–Cantelli theorem, one may show that

$$
\sup_{0\le t<\infty}\left\vert \mathbb C_{j}^{(n)}(t) - C_{j}(t)\right\vert\stackrel{\text{a.s.}}{\to}0,\qquad n\to \infty.
$$

<blockquote class = "prop">

**Proposition 4.6.** _It holds that_
\begin{align*}
\sup_{0\le t\le \theta}\left\vert \mathbb N_{jk}^{(n)}(t) - p_{jk}^c(t)\right\vert\stackrel{\text{a.s.}}{\to}0,&\qquad n\to \infty,\\
\sup_{0\le t\le \theta}\left\vert \mathbb I_{j}^{(n)}(t) - p_{j}^c(t)\right\vert\stackrel{\text{a.s.}}{\to}0,&\qquad n\to \infty.
\end{align*}
</blockquote>
<blockquote class = "thm">
**Theorem 4.7. (Strong uniform consistency)** _Suppose that $\inf_{0\le t\le \theta}\mathbb P(Z_t=j)>0.$ holds. Then_

$$
\sup_{0\le t\le \theta}\left\vert \hat q_{jk}^{(n)}(t) - q_{jk}(t)\right\vert\stackrel{\text{a.s.}}{\to}0,\qquad n\to \infty.
$$

</blockquote>

Suppose now that $\mathbb E[N_{jk}(t)^2] < \infty$. It is then possible to derive asymptotic normality: The process

$$
\sqrt{n}\left(\hat q_{jk}^{(n)}(t)-q_{jk}(t)\right),\qquad t\in [0,\infty]
$$

converges weakly to a zero-mean Gaussian process $X_{jk}$ with independent increments and variances $V_{jk}(t) = \text{Var}[X_{jk}(t)]$, for which the following estimator turns out to be consistent:

$$
\mathbb V_{jk}^{(n)}(t)=\int_0^t\frac{n}{\mathbb I_{j}^{(n)}(s-)^2}\mathbb N^{(n)}_{jk}(ds).
$$

The theory of empirical processes provides an excellent framework for the derivation of such results. An alternative to the approach presented here, and to the application of empirical processes, is offered by martingale theory. Martingale methods yield quite easy extensions in various directions, but also come with certain limitations.

## Non-Markov modeling

The study of non-markov models revolves around a process $Z$ that is a pure jump process on a finite state space $\mathcal J$ where we assume that it is non-explosive. Thus we do not assume the classical Markov property. We do however study the usual payment model:

$$
B(dt)=\sum_{j\in \mathcal J}1_{(Z_{t-}=j)}B_j(dt)+\sum_{k\in\mathcal J,k\ne j} b_{jk}(t)N_{jk}(dt).
$$

Recall in the Markov case we assume that

$$
A(t_0,t)=\mathbb E\left[\left.B(t)-B(t_0)\ \right\vert\ \mathcal F_{t_0}^Z\right]=\mathbb E\left[\left.B(t)-B(t_0)\ \right\vert\ Z_{t_0}\right]
$$

we do not assume the second equation in non-markov models. In the non-markov studies we want to investegate the relation between $\mathbb E\left[\left.B(t)-B(t_0)\ \right\vert\ \mathcal F_{t_0}^Z\right]$ and $\mathbb E\left[\left.B(t)-B(t_0)\ \right\vert\ Z_{t_0}\right]$.

Furthermore, recall that in the Markovian case we have the cummutative transition rates given by

$$
q_{jk}(t)=\int_0^t\frac{1}{\mathbb P(Z_s=j)}\ d\mathbb E[N_{jk}(s)],
$$

and the matrix representation of the transition probabilities

$$
p(t_0,t)=\prod_{t_0}^t\Big(\mathbb I + q(du)\Big)
$$

or that $p_{jk}$ solves the kolmogorov forward equation

$$
p_{jk}(t_0,dt)=\sum_{\ell\in\mathcal J}p_{j\ell}(t_0,t-)q_{\ell k}(dt)
$$

and thus we have the cash flow

$$
A(t_0,dt)=\sum_{j\in\mathcal J}p_{Z_{t_0}j}\left(B_j(dt)+\sum_{k\in\mathcal J,k\ne j} b_{jk}(t)q_{jk}(dt)\right).
$$

Still assuming the markov model we can by Tonellis theorem using the predictable compensator we can calculate the increments
\begin{align*}
\mathbb E\left[\left.N_{jk}(t)-N_{jk}(t_0)\ \right\vert\ Z_{t_0}=i\right]&=\int_{t_0}^t \mathbb E[1_{(Z_{s-}=j)}\ \vert\ Z_{t_0}=i]q_{jk}(ds)\\
&=\int_{t_0}^t \mathbb P(Z_{s-}=j\ \vert\ Z_{t_0}=i)q_{jk}(ds),
\end{align*}
with

$$
q_{jk}(t)-q_{jk}(t_0)=\int_{t_0}^t\frac{1}{\mathbb P(Z_{s-}=j\ \vert\ Z_{t_0}=i)} d\mathbb E\left[\left.N_{jk}(s)-N_{jk}(t_0)\ \right\vert\ Z_{t_0}=i\right]
$$

because of the structure of the predictable compensator $\Lambda_{jk}(t)=\int_0^t 1_{(Z_{s-}=j)} q_{jk}(ds)$. Having this in mind we formulate the definition.

<blockquote class = "def">

**Definition 5.1. (Cummulative forward transition rates)** _Let $i,j,k\in\mathcal J$ then we define_

$$
q_{i,jk}(t_0,t)=\int_{t_0}^t\frac{1}{\mathbb P(Z_{s-}=j\ \vert\ Z_{t_0}=i)}d\mathbb E\left[\left.N_{jk}(s)-N_{jk}(t_0)\ \right\vert\ Z_{t_0}=i\right].
$$

_as the cummulative forward transition rates._

</blockquote>

Recall from the above we in the Markov case simply have

$$
q_{i,jk}(t_0,t)=q_{jk}(t)-q_{jk}(t_0).
$$

Notice also that we have

$$
d\mathbb E\left[\left.N_{jk}(s)\ \right\vert\ Z_{t_0}=i\right]=\mathbb P (Z_{s-}=j\ \vert\ Z_{t_0}=i)q_{i,jk}(t_0,ds).
$$

<blockquote class = "prop">

**Proposition 5.2.** _Let $p_{ij}(t_0,t)=\mathbb P(Z_t=j\ \vert\ Z_{t_0}=i)$ without assuming the Markov property of $Z$ we have_

$$
p_{ij}(t_0,dt)=\sum_{k\in\mathcal J}p_{ij}(t_0,t-)q_{i,kj}(t_0,dt)
$$

_with boundary condition $p_{ij}(t_0,t_0)=1_{(i=j)}$ and using the convention $q_{i,jj}=-\sum_{k\in\mathcal J,k\ne j}q_{i,jk}$._

</blockquote>
<details>
<summary>**Proof.**</summary>

We know that

$$
1_{(Z_{t}=j)}=1_{(Z_{t_0}=i)}+\sum_{k\in\mathcal J, k\ne j}\left(\Big(N_{kj}(t)-N_{kj}(t_0)\Big)-\Big(N_{jk}(t)-N_{jk}(t_0)\Big)\right).
$$

Hence taking conditional expectation on the above yields
\begin{align*}
\mathbb P(Z_t = j\ \vert\ Z_{t_0}=i)&=\mathbb E\left[\left.1_{(Z_{t_0}=i)}+\sum_{k\in\mathcal J, k\ne j}\left(\Big(N_{kj}(t)-N_{kj}(t_0)\Big)-\Big(N_{jk}(t)-N_{jk}(t_0)\Big)\right)\ \right\vert\ Z_{t_0}=i\right]\\
&=\mathbb P(Z_{t_0} = j\ \vert\ Z_{t_0}=i)\\
&+\sum_{k\in\mathcal J, k\ne j}\mathbb E\left[\left.\Big(N_{kj}(t)-N_{kj}(t_0)\Big)-\Big(N_{jk}(t)-N_{jk}(t_0)\Big)\ \right\vert\ Z_{t_0}=i\right]\\
&=1_{(j= i)}+\sum_{k\in\mathcal J, k\ne j}\int_{t_0}^t \mathbb P(Z_{s-}=k\ \vert\ Z_{t_0}=i)q_{i,kj}(t_0,ds)\\
&-\sum_{k\in\mathcal J, k\ne j}\int_{t_0}^t \mathbb P(Z_{s-}=j\ \vert\ Z_{t_0}=i)q_{i,jk}(t_0,ds)\\
&=1_{(j= i)} +\sum_{k\in\mathcal J}\int_{t_0}^t \mathbb P(Z_{s-}=k\ \vert\ Z_{t_0}=i)q_{i,kj}(t_0,ds)
\end{align*}
giving the desired result. $\blacksquare$

</details>

We see that the expected cash flow then has the form
\begin{align*}
A(t_0,dt)&=\sum_{j\in\mathcal J} \mathbb P(Z_{t-}= j\ \vert\ Z_{t_0})\left(B_j(dt)+\sum_{k\in\mathcal J,k\ne j}b_{jk}(t) d\mathbb E[N_{jk}(t)\ \vert\ Z_{t_0}]\right)\\
&=\sum_{j\in\mathcal J} p_{Z_0 j}(t_0,t-)\left(B_j(dt)+\sum_{k\in\mathcal J,k\ne j}b_{jk}(t) q_{Z_0,jk}(t_0,dt)\right).
\end{align*}

### Right censoring in the Non-Markov model

We now study how one may estimate the cumulative transition rate $q_{i,jk}(t_0,t)$. Let $C>0$ be a right-censoring time. We observe the pair

$$
((Z_t)_{0\le t\le C},\tau \wedge C),
$$

where $\tau$ is an absorption time. We define as somewhat like in the previous section the following
\begin{align*}
p_{ij}^c(t_0,t)&=\mathbb P(Z_t=j, t<C\ \vert\ Z_{t_0}=i),\\
p_{i,jk}^c(t_0,t)&=\mathbb E[N_{jk}(t\wedge C)-N_{jk}(t_0\wedge C)\ \vert\ Z_{t_0}=i].
\end{align*}
We can write the cumulative transition rate in terms of these processes as

<blockquote class = "prop">

**Proposition 5.3.** _Suppose $Z\perp\!\!\!\perp C$. Then_

$$
q_{i,jk}(t_0,t)=\int_{t_0}^t\frac{1}{p_{ij}^c(t_0,s-)}p_{i,jk}^c(t_0,ds).
$$

</blockquote>
<details>
<summary>**Proof.**</summary>

We start by calculating the integrator function. We have that
\begin{align*}
p_{i,jk}^c(t_0,t)&=\mathbb E\left[\left.\int_{t_0}^t 1_{(s\le C)}dN_{jk}(ds)\ \right\vert\ Z_{t_0}=i\right]\\
&=\int_{t_0}^t \mathbb E\left[1_{(s\le C)}\right]d\mathbb E\left[\left.N_{jk}(s)\ \right\vert\ Z_{t_0}=i\right]\\
&=\int_{t_0}^t \mathbb P(s\le C) d\mathbb E\left[\left.N_{jk}(s)-N_{jk}(t_0)\ \right\vert\ Z_{t_0}=i\right].
\end{align*}
and
\begin{align*}
p_{ij}^c(t_0,t)&=\mathbb P(Z_t=j, t<C\ \vert\ Z_{t_0}=i)\\
&=\mathbb P(Z_t=j\ \vert\ Z_{t_0}=i)\mathbb P(t\le C)
\end{align*}
Hence
\begin{align*}
\int_{t_0}^t&\frac{1}{p_{ij}^c(t_0,s-)}p_{i,jk}^c(t_0,ds)\\
&=\int_{t_0}^t\frac{1}{\mathbb P(Z_{s-}=j\ \vert\ Z_{t_0}=i)\mathbb P(s-\le C)}\mathbb P(s\le C) d\mathbb E\left[\left.N_{jk}(s)-N_{jk}(t_0)\ \right\vert\ Z_{t_0}=i\right]\\
&=\int_{t_0}^t\frac{1}{\mathbb P(Z_{s-}=j\ \vert\ Z_{t_0}=i)} d\mathbb E\left[\left.N_{jk}(s)-N_{jk}(t_0)\ \right\vert\ Z_{t_0}=i\right]\\
&=q_{i,jk}(t_0,t).
\end{align*}
as desired. $\blacksquare$

</details>

If we can accept the independece assumption required in the argument, then we can form empirical estimates of the probabilities and mean of the counting process. Assume that we have i.i.d. samples

$$
\Big((Z_t^\ell)_{0\le t\le C^\ell}, \tau^\ell \wedge C^\ell\Big)_{\ell\in \{1,...,n\}}.
$$

We form a non-parametric estimator for the cumulative transition rates by suitably combining standard non-parametric estimators for the components $p^c_{ij}$ and $p^c_{i,jk}$. To this end we define
\begin{align*}
\mathbb I_{j}^{(n)}(t_0,t)&=\frac{1}{\sum_{\ell = 1}^n 1_{(Z^\ell_{t_0}=i)}}\sum_{\ell = 1}^n 1_{(Z^\ell_{t_0}=i)}1_{(Z_t^\ell = j)}1_{(t<C^\ell)},\\
\mathbb N_{jk}^{(n)}(t_0,t)&=\frac{1}{\sum_{\ell = 1}^n 1_{(Z^\ell_{t_0}=i)}}\sum_{\ell = 1}^n 1_{(Z^\ell_{t_0}=i)}\left(N_{jk}^\ell(t\wedge C^\ell)-N_{jk}^\ell(t_0\wedge C^\ell)\right)
\end{align*}
Note that, due to the strong law of large numbers,
\begin{align*}
\mathbb I_{j}^{(n)}(t)&\stackrel{\text{a.s.}}{\to}p_{ij}^c(t_0,t),&\quad n\to\infty,\\
\mathbb N_{jk}^{(n)}(t)&\stackrel{\text{a.s.}}{\to}p_{i,jk}^c(t_0,t),&\quad n\to\infty.
\end{align*}
We possible throw away alot of observation by conditioning on the state at time $t_0$, so this introduces variance. Therefore, one should test if the underlying process is indeed non-Markov.

## Outlook