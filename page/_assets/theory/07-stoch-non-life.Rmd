# Stochastic Processes in Non-Life Insurance Mathematics

## Preliminaries

<blockquote class = "def">

**Definition 1.1. (Schmidli) (Stochastic process)** _A **stochastic process**\index{stochastic process} is a family of random variables $X_t : (\Omega,\mathbb F,\mathbb P) \to (E,\mathbb E,\mu)$ indexed by $t\in \mathcal I$, where $\mathcal I\subseteq \mathbb N$ or $\mathcal I\subseteq \mathbb R$, denoted by $\mathbf X=\{X_t\}_{t\in \mathcal I}$. In the case that $\mathcal I=\mathbb R$, we say that $\mathbf X$ is **CADLAG**\index{CADLAG stochastic process} if_

$$
X(t)=\lim_{h\to 0}X(t+h)
$$

_and $\lim_{h\to 0}X(t-h)$ exist almost surely. Furthermore, if $X(t)$ is continuous almost surely we say that $\mathbf X$ is **continuous**\index{continuous stochastic process}._

</blockquote>

We will in this chapter assume that whenever $\mathcal I=\mathbb R$ then $\mathbf X$ is CADLAG. This in non controversial in the processes we will be working on, as it is natural for real-life processes to be left deterministic and right continuous. Furthermore, if we suppress $\{X_t\}_{t\in \mathcal I}$ then $\mathcal I=\mathbb R^+=[0,\infty)$ and we simply write $\{X_t\}$ or $\mathbf X$.

<blockquote class = "def">

**Definition 1.2. (Schmidli) (Point process)** _A CADLAG stochastic process $\{N_t\}$ is called a **point process**\index{point process} if i) $N_0=0$, ii) $N_t\ge N_s$ for all $t\ge s$ (increasing) and iii) $N_t\in \mathbb N$ for all $t$. Furthermore, if for the mapping $T_k : k\mapsto\inf\{t\ge 0 : N_t\ge k\}$ (jumping times) $T_k\ne T_j$ for all $k\ne j$, then $\mathbf N$ is said to be a **simple point process**.\index{simple point process}_

</blockquote>

<blockquote class = "def">

**Definition 1.3. (Schmidli)** _Let $\{X_t\}$ be a stochastic process. If for all partitions $0=t_0<t_1<\cdots<t_n$ the random variables $X_{t_i}-X_{t_{i-1}}$ for $i=1,...,n$ are jointly independent, then $\{X_t\}$ has **independent increments**\index{independent increments}. If furthermore for all $h>0$ it holds that_ 

$$
\{X_{t_i}-X_{t_{i-1}}\}_{i=1,...n}\stackrel{d}{=}\{X_{t_i+h}-X_{t_{i-1}+h}\}_{i=1,...n}
$$

_then $\{X_t\}$ hast **stationary increments**\index{stationary increments}._

</blockquote>

We will also be needing the definition of a filtration and a martingale. These are given in the appendix. See also the **Martingale stopping theorem** and **Martingale convergence theorem**.

### Poisson process

<blockquote class = "def">

**Definition 1.9. (Schmidli) (Poisson process)** _A point process $\{N_t\}$ is called a (homogeneous) **Poisson process**\index{poisson process} with rate $\lambda >0$ if i) $\{N_t\}$ has stationary and independent increments and ii) $\mathbb P[N_j=0]=1-\lambda h+o(h)$ as $h\to 0$._

</blockquote>

<blockquote class = "prop">

**Proposition 1.10. (Schmidli) (Poisson process, alternative version)** _A point process $\{N_t\}$ is a (homogeneous) **Poisson process**\index{poisson process} with rate $\lambda >0$ if one of the following statements is true:_

  i) _$\{N_t\}$ has independent increments and $N_t\sim \text{Pois}(\lambda t)$,_
  ii) _the interarrival times $T_k-T_{k-1}$ are independent and $\text{Exp}(\lambda)$ distributed,_
  iii) _For each $t$ $N_t\sim \text{Pois}(\lambda t)$ and on the event $\{N_t=n\}$ the ordered samples $T_0,T_1,...,T_n$ are uniformly distributed on $[0,t]$ and independent,_
  iv) _$\{N_t\}$ has independent increments and $\mathbb E[N_1]=\lambda$ and on the event $\{N_t=n\}$ the ordered samples $T_0,T_1,...,T_n$ are uniformly distributed on $[0,t]$ and independent and_
  v) _$\{N_t\}$ has independent and stationary increments such that $\mathbb P[N_h\ge 2]=o(h)$ as $h\to 0$ and $\mathbb E[N_1]=\lambda$._

</blockquote>

The Poisson process has the following properties.

<blockquote class = "prop">

**Proposition 1.11. (Schmidli)** _Let $\{N_t\}$ and $\{\tilde N_t\}$ be two independent Poisson processes with rates $\lambda$ and $\tilde\lambda$ respectively. Let $\{I_i : i \in \mathbb N\}$ be an iid sequence of random variables independent of $\{N_t\}$ with $\mathbb P[I_i = 1] = 1−\mathbb P[I_i = 0] = q$ for some $q \in (0,1)$. Furthermore let $a > 0$ be a real number. Then_

  i) _$\{N_t+\tilde N_t\}$ is a Poisson process with rate $\lambda +\tilde \lambda$._
  ii) _$\sum_{i=1}^{N_t}I_i$ is a Poisson process with rate $\lambda q$._

</blockquote>

<blockquote class = "def">

**Definition 1.12. (Schmidli) (inhomogeneous Poisson process)**\index{inhomogeneous Poisson process} _Let $\Lambda(t)$ be an increasing right continuous function on $[0,\infty)$ with $\Lambda(0) = 0$. A point process $\{N_t\}$ on $[0, \infty)$ is called **inhomogeneous Poisson process** with intensity measure $\Lambda(t)$ if_

  i) $\{N_t\}$ _has independent increments,_
  ii) $N_t-N_s\sim\text{Pois}(\Lambda (s)-\Lambda(s))$.

</blockquote>

If there exists a function $\lambda(t)$ such that $\Lambda(t) = \int_0^t \lambda(s)\ ds$ then $\lambda(t)$ is called **intensity** or **rate** of the inhomogeneous Poisson process. Note that a homogeneous Poisson process is a special case with $\Lambda(t) = \lambda t$. Define $\Lambda^{−1}(x) = \sup\{t \ge 0 : \Lambda(t) \le x\}$ the inverse function of $\Lambda(t)$.

<blockquote class = "prop">

**Proposition 1.13. (Schmidli)** _Let $\tilde N_t$ be a homogeneous Poisson process with rate 1. Define $N_t=\tilde N_{\Lambda (t)}$. Then $N_t$ is an inhomogeneous Poisson process with intensity measure $\Lambda(t)$. Conversely, let $N_t$ is an inhomogeneous Poisson process with intensity measure $\Lambda(t)$. Define $\tilde N_t=N_{\Lambda^{-1}(t)}$. Then $\tilde N_t$ is a homogeneous Poisson process with rate 1._

</blockquote>

For an inhomogeneous Poisson process we can construct the following martingales.

<blockquote class = "lem">

**Lemma 1.14. (Schmidli)** _Let $r\in \mathbb R$. The following processes are martingales._

  i) $\{N_t-\Lambda(t)\}$,
  ii) $\{(N_t-\Lambda(t))^2-\Lambda(t)\}$,
  iii) $\{\exp[rN_t-\Lambda(t)(e^r-1)]\}$.

</blockquote>

### Renewal processes

<blockquote class = "def">

**Definition 1.15. (Schmidli) (Renewal process)**\index{Renewal process} _A simple point process $\{N_t\}$ is called an **ordinary renewal process** if the interarrival times $\{T_k-T_{k-1}\}_{k\in\mathbb N}$ are iid. If $T_1-T_0=T_1$ has a different distribution then $\{N_t\}$ is called **delayed renewal process**. If $\frac{1}{\lambda}=\mathbb E[T_2-T_1]$ exist and_

$$
\mathbb P[T_1\le x]=\lambda\int_0^x\mathbb P[T_1-T_1>y]\ dy
$$

_then $\{N_t\}$ is called **stationary renewal process**. If $\{N_t\}$ is an ordinary renewal process then the function $U(t)=1_{\{t\ge 0\}}+\mathbb E[N_t]$ is called the **renewal measure**._\index{renewal measure}

</blockquote>

In the rest of this section we denote by $F$ the distribution function of $T_2 − T_1$. For simplicity we let $T$ be a random variable with distribution $F$. Note that because the point process is simple we implicitly assume that $F(0) = 0$. If nothing else is said we consider in the sequel only ordinary renewal processes.

We recall that the convolution\index{convolution} $*$ is an operator defined as follows:

$$
(g * F)(x)=\int g(x-y)\ dF(y)
$$

and we write $(F * F)=F^{*1}$ and by convention we let $F^{*0}(x)=1_{\{t : F(t)>0\}}(x)$. Notice that $F$ in renewal process has support on $[0,\infty)$ and so we have assuming that $F$ is absolutely continuous 
\begin{align*}
(g*F)(x)&=\int g(x-y)f(y)\ dy=\int g(y)f(x-y)\ dy\\
&=\int_{-\infty}^x  g(y)f(x-y)\ dy.
\end{align*}
and if $g=F$ we obtain

$$
F^{*1}(x)=\int_{-\infty}^x F(y)f(x-y)\ dy=\int_0^x F(y)f(x-y)\ dy.
$$

With this in min we move to rewriting the renewal measure.

<blockquote class = "lem">

**Lemma 1.16. (Schmidli)** _The renewal measure can be written as_

$$
U(t)=\sum_{n=0}^\infty F^{*n}(t).
$$

_Moreover, $U(t)<\infty$ for all $t\ge 0$ and $U(t)\to\infty$ as $t\to \infty$._

</blockquote>

In the renewal theory one often has to solve equations of the form

$$
Z(x)=z(x)+\int_{0}^x Z(x-y)\ dF(y)
$$

where $z(x)$ is a known function and $Z(x)$ is unknown. This equation is called the **renewal equation**\index{renewal equation}. The equation can be solved explicitly.

<blockquote class = "prop">

**Proposition 1.17. (Schmidli) (Renewal equation)** _Let $z$ be given and let $F$ be the distribution of a non-negative random variable. Define $Z$ as the solution to_

$$
Z(x)=z(x)+\int_{0}^x Z(x-y)\ dF(y).
$$

_If $z$ is bounded on bounded intervals then_

$$
Z(x)=\int_{0-}^x z(x-y)\ dU(y)=z*U(x)
$$
_is the unique solution to $Z$ that is bounded on bounded intervals._

</blockquote>

Let us take some explicit solutions by defining arithmetic distributions.

<blockquote class = "def">

**Definition 1.20. (Schmidli)**\index{arithmetic distribution} _Let $F$ be a distribution function for a random variable $X$. If $\gamma$ exists such that_

$$
\mathbb P\left[X\in\bigcup_{n=0}^\infty \{2^n\gamma\}\right]=1
$$

_then $F$ is called **arithmetic**. We call the largest number $\gamma$ such that the above holds for the **span**._

</blockquote>

<blockquote class = "prop">

**Proposition 1.17. (Schmidli) (Renewal theorem)** _If $z$ is directly Riemann integrable i.e. the following holds:_
\begin{align*}
-\infty<\lim_{h\to 0^+}&h\sum_{k=1}^\infty\sup\Big\{z\big([(k-1)h,kh)\big)\Big\}=\\
&\lim_{h\to 0^+}h\sum_{k=1}^\infty\inf\Big\{z\big([(k-1)h,kh)\big)\Big\}<\infty,
\end{align*}
_then the solution to the renewal equation satisfies_

$$
\lim_{t\to\infty}Z(t)=\lambda \int_0^\infty z(y)\ dy=\frac{1}{\mathbb E[X]}\int_0^\infty z(y)\ dy
$$

_and is given by_

$$
Z(t)=\left(z*\sum_{n=0}^\infty F^{*n}\right)(t)=\sum_{n=0}^\infty\int_0^tz(t-y)dF^{*n}(t).
$$
</blockquote>

A comprehensive look at the derivation may be read in [this note](https://halweb.uc3m.es/esp/personal/personas/bdauria/1112/stoc_proc_phd/Notes/2012-02-28Tu_Notes.pdf).

### Wiener-Hopf factorisation

Let $\{X_i\}$ be a sequence of iid. random variables. Let $U(x)$ be the distribution function of $X_i$. Define

$$
S_n=\sum_{i=1}^nX_i,
$$

with $S_0=0$. Assume that $-\infty<\mathbb E[X_i]<0$. Define

$$
\tau_+=\inf\{n > 0 : S_n>0\},\qquad \tau_-=\inf\{n>0 : S_n\le 0\}.
$$

Using this we define

$$
H(x)=\mathbb P[\tau_+<\infty, S_{\tau_+}\le x],\qquad \rho(x)=\mathbb P[S_{\tau_-}\le x].
$$

Next define the functions

$$
\psi_n(x)=\mathbb P\Big[S_1>0,S_2>0,...,S_n>0,S_n\le x\Big],\quad \psi_0(x)=1_{[0,\infty)}(x)
$$

and

$$
\psi(x)=\sum_{n=0}^\infty \psi_n(x).
$$

<blockquote class = "lem">

**Lemma 1.27. (Schmidli)** _For $n\ge 1$ we have_

$$
\psi_n(x)=\mathbb P[S_n>S_j, (0\le j\le n-1),S_n\le x]
$$

_and therefore $\psi$ is the result of the renewal equation_

$$
\psi(x)=H + \psi_n* H
$$

_that is_

$$
\psi(x)=\sum_{n=0}^\infty H^{*n}(x).
$$

</blockquote>

It further may be shown that $U$ has factorisation

$$
U=H+\rho-H * \rho
$$

which is called the **Wierner-Hopf factorisation**.\index{Wierner-Hopf factorisation}

### Subexponential distributions

<blockquote class = "def">

**Definition 1.28. (Schmidli) (Subexponential distribution)**\index{Subexponential distribution} _A distribution function $F$ with $F(x)=0$ for $x<0$ is called **subexponential** if_

$$
\lim_{t\to\infty}\frac{1-F^{*2}(t)}{1-F(t)}=2.
$$

</blockquote>

We want to show that the moment generating function of such a distribution does not exist for strictly positive values. We first need the following.

<blockquote class = "lem">

**Lemma 1.29. (Schmidli)** _If $F$ is subexponential then for all $t\in\mathbb R$_

$$
\lim_{x\to\infty}\frac{1-F(x-t)}{1-F(x)}=1.
$$

</blockquote>

<blockquote class = "lem">

**Lemma 1.30. (Schmidli)** _If $F$ is subexponential then for all $r>0$_

$$
\lim_{t\to\infty}e^{rt}(1-F(t))=\infty,
$$

_in particular_

$$
\int_{0-}^\infty e^{rx}\ dF(x)=\infty.
$$

</blockquote>

The following lemma gives a condition for subexponentiality.

<blockquote class = "lem">

**Lemma 1.31. (Schmidli)** _Assume that for all $z\in (0,1]$ the limit_

$$
\gamma(z)=\lim_{x\to\infty}\frac{1-F(zx)}{1-F(x)}
$$

_exists and that $\gamma(z)$ is left-continuous at 1. Then $F$ is a subexponential distribution function._

</blockquote>

Next we give an upper bound for the tails of the convolutions.

<blockquote class = "lem">

**Lemma 1.33. (Schmidli)** _Let $F$ be subexponential. Then for any $\varepsilon>0$ there exist a $D\in\mathbb R$ such that_

$$
\frac{1-F^{*n}(t)}{1-F(t)}\le D(1+\varepsilon)^n
$$

_for all $t>0$ and $n\in\mathbb N$._

</blockquote>

<blockquote class = "lem">

**Lemma 1.34. (Schmidli)** _Let $F(x)=0$ for $x<0$. The following are equivalent:_

  i) _$F$ is subexponential._
  ii) _For all $n\in\mathbb N$_
  $$\lim_{x\to\infty}\frac{1-F^{*n}(x)}{1-F(x)}=n.$$
  iii) _There exists $n\ge 2$ such that the above holds._

</blockquote>

<blockquote class = "lem">

**Lemma 1.35. (Schmidli)** _Let $U$ and $V$ be two distribution functions with $U(x)=V(x)=0$ for all $x<0$. Assume that_

$$
1-V(x)\sim a(1-U(x)),\qquad \text{as}\ x\to\infty
$$

_for some $a>0$. If $U$ is subexponential then also $V$ is subexponential._

</blockquote>

## The Cramer-Lundberg model

### Definition of the Cramer-Lundberg process

Modelling a risk by a compound Poisson distribution is very popular. That is because the compound Poisson model has nice properties. For instance it can be derived as a limit of individual models. This was the reason for Filip Lundberg to postulate a continuous time risk model where the aggregate claims in any interval have a compound Poisson distribution. Moreover, the premium income should be modelled. In a portfolio of insurance contracts the premium payments will be spread all over the year. Thus he assumed that that the premium income is continuous over time and that the premium income in any time interval is proportional to the interval length. This leads to the following model for the surplus of an insurance portfolio

$$
C_t=u+ct-\sum_{i=1}^{N_t}Y_i.
$$

$u$ is the initial capital, $c$ is the premium rate. The number of claims in $(0,t]$ is a Poisson process $N_t$ with rate $\lambda$. The claim sizes $\{Y_i\}$ are a sequence of iid positive random variables independent of $\{N_t\}$. This model is called **Cramer-Lundberg process**\index{Cramer-Lundberg process} or **classical risk process**\index{classical risk process}.

We denote the distribution function of the claims $Y_i$ by $G$, its moments by $\mu_n = \mathbb E [Y_1^n]$ and its moment generating function by $M_Y (r) = \mathbb E [\exp\{rY_1\}]$. Let $\mu = \mu_1$. We assume that $\mu < \infty$. Otherwise no insurance company would insure such a risk. Note that $G(x) = 0$ for $x < 0$. We will see later that it is no restriction to assume that $G(0) = 0$.

For an insurance company it is important that $\{C_t\}$ stays above a certain level. This level is given by legal restrictions. By adjusting the initial capital it is no loss of generality if we assume this level to be 0. We define the ruin time

$$
\tau = \inf\{t>0 : C_t< 0\},\qquad (\inf\ \emptyset =\infty).
$$

We will mostly be interested in the probability of ruin in a time interval $(0, t]$

$$
\psi(u,t)=\mathbb P [\tau \le t\ \vert\ C_0=u]=\mathbb P\left[\inf_{0<s\le t} C_s<0\right]
$$

and the probability of ultimate ruin

$$
\psi(u)=\lim_{t\to \infty}\psi(u,t) =\mathbb P [\tau <\infty\ \vert\ C_0=u]=\mathbb P\left[\inf_{t>0} C_t<0\right].
$$

It is easy to see that $\psi(u, t)$ is decreasing in $u$ and increasing in $t$.

As in the first section we denote the claim times by $T_1,T_2,...$ and by convention $T_0 = 0$. Let $X_i = c(T_i − T_{i−1}) − Y_i$. If we only consider the process at the claim times we can see that

$$
C_{T_n}=u+\sum_{i=1}^n X_i
$$

is a random walk. Note that $\psi (u)=\mathbb P[\inf_{n\in\mathbb N}C_{T_n}<0]$. From the theory of random walks we can see that ruin occurs a.s. iff $\mathbb E [X_i] \le 0$, compare with Lemma 1.26. Hence we will assume in the sequel that
\begin{align*}
\mathbb E[X_i]>0\iff c\frac{1}{\lambda}-\mu>0\iff c> \lambda\mu\iff \mathbb E[C_t-u]>0.
\end{align*}
Recall that

$$
\mathbb E\left[\sum_{i=1}^{N_t}Y_i \right]=\lambda t\mu.
$$

The condition can be interpreted that the mean income is strictly larger than the mean outflow. Therefore the condition is also called the **net profit condition**\index{net profit condition}.

If the net profit condition is fulfilled then $C_{T_n}$ tends to infinity as $n \to\infty$. Hence

$$
\inf\{C_t-u : t>0\}=\inf \{C_{T_n}-u : n\ge 1\}
$$

is a.s. finite. So we can conclude that

$$
\lim_{u\to\infty}\psi(u)=0.
$$

#### A note on the model and reality

In reality the mean number of claims in an interval will not be the same all the time. There will be a claim rate $\lambda(t)$ which may be periodic in time. Moreover, the number of individual contracts in the portfolio may vary with time. Let $a(t)$ be the volume of the portfolio at time $t$. Then the claim number process $\{N_t\}$ is an inhomogeneous Poisson process with rate $a(t)\lambda(t)$. Let

$$
\Lambda (t)=\int_0^t a(s)\lambda(s)\ ds
$$

Then $\{\tilde N_t\}$ where $\tilde N_t = N_{\Lambda^{−1}(t)}$ is a Poisson process with rate 1. Let now the premium rate vary with $t$ such that $c_t = ca(t)\lambda(t)$ for some constant $c$. This assumption is natural for changes in the risk volume.

It is artificial for the changes in the intensity. For instance we assume that the company gets more new customers at times with a higher intensity. This effect may arise because customers withdraw from their old insurance contracts and write new contracts with another company after claims occurred because they were not satisfied by the handling of claims by their old companies.

The premium income in the interval $(0,t]$ is $c\Lambda(t)$. Let $\tilde C_t = C_{\Lambda^{−1}(t)}$. Then

$$
\tilde C_t=u+c\Lambda(\Lambda^{-1}(t))-\sum_{i=1}^{N_{\Lambda^{-1}(t)}}Y_i=u+ct-\sum_{i=1}^{\tilde N_t}Y_i
$$

is a Cramer-Lundberg process. Thus we should not consider time to be the **real time** but **operational time** since we at any point may time translate the process into a standard homogeneous Poisson process.

### A differential equation for the ruin probability

We first prove that $\{C_t\}$ is a strong Markov process.

<blockquote class = "lem">

**Lemma 2.1. (Schmidli)** _Let $\{C_t\}$ be a Cramer-Lundberg process and $T $be a finite stopping time. Then the stochastic process $\{C_{T+t}−C_T : t\ge 0\}$ is a Cramer-Lundbergp rocess with initial capital 0 and independent of $\mathcal F_T$._

</blockquote>

That is any time shift of a Cramer-Lundberg process is itself af Cramer-Lundberg process. This gives us a differential form of the ruin probability.

Let $h$ be small. If ruin does not occur in the interval $(0,h]$ then a new Cramer-Lundberg process starts at time $h$ with new initial capital $C_h$. Let $\delta(u) = 1 − \psi(u)$ denote the probability that ruin does not occur. Using the definition of the Poisson process we obtain
\begin{align*}
\delta (u)&=\mathbb P[N_{t+h}=N_t]\delta(u+ch)+\mathbb P[N_{t+h}=N_t+1]\mathbb E[\delta((u-Y_1)^+)]\\
&=\Big(1-\lambda h+o(h)\Big)\delta(u+ch)+\Big(\lambda h + o(h)\Big)\mathbb E[\delta((u-Y_1)^+)]+o(h)\\
&=\Big(1-\lambda h\Big)\delta(u+ch)+\lambda h\int_0^u\delta((u-y)^+)\ dG(y)+o(h),
\end{align*}
where $\delta((u-y)^+)$ exists because $\delta(u)$ is increasing. Letting $h$ tending to 0 shows that $\delta(u)$ is right continuous. If we rearrange we see that

$$
c\frac{\delta(u+ch)-\delta(u)}{ch}=\lambda\left[\delta(u+ch)-\int_0^u\delta(u-y)\ dG(y)\right]+o(1)
$$

Letting $h$ tend to 0 shows that $\delta(u)$ is differentiable from the right. A similar argument shows the differentiability from the left. If we let $h\to 0$ we obtain the differential form

$$
\frac{d}{du}\delta(u)=\frac{\lambda}{c}\left[\delta(u)-\int_0^u\delta(u-y)\ dG(y)\right].
$$

By integrating we have

$$
\frac{c}{\lambda}(\delta(u)-\delta(0))=\int_0^u \delta(x)-\int_0^x\delta(x-y)\ dG(y)\ dx=\int_0^u\delta(u-x)(1-G(x))\ dx
$$

Since $\delta(u)\to 1$ for $u\to\infty$ we have

$$
\frac{c}{\lambda}(1-\delta(0))= \int_0^\infty(1-G(x))\ dx=\mu
$$

hence

$$
\delta(0)=1-\frac{\lambda\mu}{c},\qquad \psi(0)=1-\delta(0)=\frac{\lambda\mu}{c}.
$$

In total we see that

$$
\frac{c}{\lambda}\left(\delta(u)-1+\frac{\lambda\mu}{c}\right)=\int_0^u\delta(u-x)(1-G(x))\ dx
$$

hence

$$
\delta(u)=1-\frac{\lambda\mu}{c}+\frac{\lambda}{c}\int_0^u\delta(u-x)(1-G(x))\ dx
$$

and
\begin{align*}
\psi(u)&=\frac{\lambda\mu}{c}-\frac{\lambda}{c}\int_0^u\delta(u-x)(1-G(x))\ dx\\
&=\frac{\lambda\mu}{c}-\frac{\lambda}{c}\int_0^u(1-\psi(u-x))(1-G(x))\ dx\\
&=\frac{\lambda\mu}{c}+\frac{\lambda}{c}\int_0^u\psi(u-x)(1-G(x))\ dx-\frac{\lambda}{c}\int_0^u(1-G(x))\ dx\\
&=\frac{\lambda}{c}\int_u^\infty (1-G(x))\ dx+\frac{\lambda}{c}\int_0^u\psi(u-x)(1-G(x))\ dx
\end{align*}
using that $\mu=\int_0^\infty (1-G(x)) dx$.

### The adjustment coefficient

Let

$$
\theta(r)=\lambda(M_Y(r)-1)-cr,
$$

where $M_Y(r)=\mathbb E\{e^{rY}\}$ is the moment generating function for $Y$. Let $\theta$ be defined on the set where $M_Y$ is exists. We have the following martingale.

<blockquote class = "lem">

**Lemma 2.3. (Schmidli)** _Let $r\in\mathbb R$ such that $M_Y(r)<\infty$. Then the stochastic process_

$$
e^{-rC_t-\theta(r)t}
$$

_is a martingale._

</blockquote>

It would be nice to have a martingale which is only dependent on the surplus and not explicitly on time. Let us therefore consider the equation $\theta(r) = 0$. This equation has obviously the solution $r = 0$:

$$
\theta(0)=\lambda(M_Y(0)-1)-c\cdot 0=\lambda\mathbb E[e^{0\cdot Y}]-\lambda=0,
$$

We differentiate $\theta(r)$ to find the *possible* other solution

$$
\theta'(r)=\lambda M_Y'(r)-c,\qquad \theta''(r)=\lambda M_Y''(r).
$$

Using that $M_Y^{(n)}(r)=\mathbb E[Y^ne^{rY}]$ we have

$$
\theta'(r)=\lambda \mathbb E[Ye^{rY}] - c,\qquad \theta''(r)=\lambda E[Y^2e^{rY}]>0.
$$

Hence we know that $\theta$ is *stricly* convex. And since $\theta'(0)=\lambda\mu-c<0$ by the net profit condition there exist another solution $R>0$ such that $\theta(R)=0$. We call this solution the **adjustment coefficient**\index{adjustment coefficient} or the **Lundberg exponent**\index{Lundberg exponent}. The Lundberg exponent will play an important role in the estimation of the ruin probabilities.

In general the adjustment coefficient is difficult to calculate. So we try to find some bounds for the adjustment coefficient. Note that the second moment $μ_2$ exists if the Lundberg exponent exists. We have by some calculations the upper bound for $R$

$$
R<\frac{2(c-\lambda\mu)}{\lambda \mu_2}.
$$

We are not able to find a lower bound in general. But in the case of bounded claims we get a lower bound for $R$. If $Y\le M$ almost surely we have

$$
\frac{1}{M}\log\frac{c}{\lambda \mu}<R<\frac{2(c-\lambda\mu)}{\lambda \mu_2},
$$

although since $M$ may in general be large or wanted to be large for modelling purposes this lower bound is somewhat useless.

### Lundberg's inequality

We will now connect the adjustment coefficient and ruin probabilities.

<blockquote class = "thm">

**Theorem 2.4. (Schmidli)** _Assume that the adjustment coefficient $R$ exists. Then_

$$
\psi(u)<e^{-Ru}.
$$

</blockquote>

We have got an upper bound for the ruin probability. We would like to know whether $R$ is the best possible exponent in an exponential upper bound. This question will be answered in the next section.

### The Cramer-Lundberg approximation

Consider the equation

$$
\psi(u)=\frac{\lambda}{c}\int_u^\infty (1-G(x))\ dx+\frac{\lambda}{c}\int_0^u\psi(u-x)(1-G(x))\ dx
$$

This equation looks almost like a renewal equation, but

$$
\frac{\lambda}{c}\int_0^\infty (1-G(x))\ dx=\frac{\lambda \mu}{c}<1
$$

by the net profit condition. Hence $\frac{\lambda}{c}(1-G(x))$ is not a probability distribution. Can we manipulate the above to get a renewal equation?

<blockquote class = "thm">

**Theorem 2.6. (Schmidli) (Cramer-Lundberg approximation)**\index{Cramer-Lundberg approximation} _Assume that the Lundberg exponent exists and that_

$$
\int_0^\infty xe^{Rx}\frac{\lambda}{c}(1-G(x))\ dx<\infty.
$$

_Then_

$$
\lim_{u\to\infty} \psi(u)e^{Ru}=\frac{c-\lambda \mu}{\lambda M'_Y(R)-c}
$$

_and we write_

$$
\psi(u)\sim e^{-Ru}\frac{c-\lambda \mu}{\lambda M'_Y(R)-c} \quad \text{for}\quad u\to\infty.
$$

</blockquote>

Thus for large u we get an approximation to $\psi(u)$. This approximation is called the Cramer-Lundberg approximation. The theorem shows that it is not possible to obtain an exponential upper bound for the ruin probability with an exponent strictly larger than $R$. 

### Reinsurance and ruin

#### Proportional reinsurance

Recall that for proportional insurance the insurer covers $Y_i^I = \alpha Y_i$ of each claim, the reinsurer covers $Y_i^R = (1 − \alpha )Y_i$. Denote by $c^I$ the insurer's premium rate. The insurer's adjustment coefficient is obtained from the equation

$$
\lambda (M_{\alpha Y} (r) − 1) − c^Ir = 0.
$$

The new moment generating function is

$$
M_{\alpha Y} (r) = \mathbb E [e^{r\alpha Y}] = M_Y (\alpha r).
$$

Assume that both insurer and reinsurer use an expected value premium principle with the same safety loading. Then $c^I = \alpha c$ and we have to solve

$$
\lambda (M_Y (\alpha r) − 1) − c\alpha r = 0.
$$

This is almost the original equation, hence $R = \alpha R^I$ where $R^I$ is the adjustment coefficient under reinsurance. The new adjustment coefficient is larger, hence the risk has become smaller.

#### Excess of loss reinsurance

Under excess of loss reinsurance with retention level $M$ the insurer has to pay $Y_i^I = \min\{Y_i,M\}$ of each claim. The adjustment coefficient is the strictly positive solution to the equation

$$
\lambda \left(\int_0^Me^{rx}\ G(x) + e^{rM}(1-G(M))-1\right)-c^Ir=0.
$$

There is no possibility to find the solution from the problem without reinsurance. We have to solve the equation for every $M$ separately. But note that $R^I$ exists in any case. Especially for heavy tailed distributions this shows that the risk has become much smaller. By the Cramer-Lundberg approximation the ruin probability decreases exponentially as the initial capital increases.

We will now show that for the insurer the excess of loss reinsurance is optimal. We assume that both insurer and reinsurer use an expected value principle.

<blockquote class = "prop">

**Proposition 2.9. (Schmidli) ** _Let all premiums be computed via the expected value principle. Under all reinsurance forms acting on individual claims with premiums rate $c^I$ and $c^R$ fixed the excess of loss reinsurance maximises the insurer's adjustment coefficient._

</blockquote>

We consider now the portfolio of the reinsurer. What is the claim number process of the claims the reinsurer is involved in? Because the claim amounts are independent of the claim arrival process we delete independently points from the Poisson process with probability $G(M)$ and do not delete them with probability $1 − G(M)$. By Proposition 1.11 this process is a Poisson process with rate $\lambda (1 − G(M))$. Because the claim sizes are iid. and independent of the claim arrival process the surplus of the reinsurer is a Cramer-Lundberg process with intensity $\lambda (1 − G(M))$ and claim size distribution

$$
\tilde G(x)=\mathbb P[Y_i-M\le x\ \vert\ Y_i> M]=\frac{G(M+x)-G(M)}{1-G(M)}.
$$

### The severity of ruin and the distribution of $\inf\{C_t : t \ge 0\}$

For an insurance company ruin is not so dramatic if $−C_\tau$ is small, but it could ruin the whole company if $−C_\tau$ is very large. So we are interested in the distribution of $−C_\tau$ if ruin occurs. Let

$$
\psi_x(u)=\mathbb P[\tau <\infty, C_\tau <-x].
$$

By using the same techniques as previously we obtain

$$
\frac{c}{\lambda}\psi_x(u)=\int_0^u\psi_x(u-y)(1-G(y))\ dy-\int_x^{x+u}(1-G(y))+\int_x^\infty(1-G(y))\ dy
$$

using that $\frac{c}{\lambda}\psi_x(0)=\int_x^\infty(1-G(y))\ dy$. Then we have

$$
\psi_x(u)=\frac{\lambda}{c}\left(\int_0^u\psi_x(u-y)(1-G(y))\ dy+\int_{x+u}^\infty(1-G(y))\ dy\right).
$$

By Bayes' theorem we have for $A,B$ that $\mathbb P(A\vert B)=\mathbb P(A\cap B)/\mathbb P(B)$ and so
\begin{align*}
\mathbb P[C_\tau <-x\ \vert\ \tau<\infty,C_0=0]&=\frac{\mathbb P[C_\tau <-x, \tau<\infty,C_0=0]}{\mathbb P[
\tau<\infty,C_0=0]}\\
&=\frac{\psi_x(0)}{\psi(0)}\\
&=\frac{c}{\lambda \mu}\frac{\lambda}{c}\int_{x}^\infty(1-G(y))\ dy\\
&=\frac{1}{\mu}\int_{x}^\infty(1-G(y))\ dy
\end{align*}
The random variable $−C_\tau$ is called **severity of ruin**\index{severity of ruin}.

In order to get an equation for the ruin probability we can consider the first time point $\tau_1$ where the surplus is below the initial capital. At this point, by the strong Markov property, a new Cramer-Lundberg process starts. We get three possibilities:

  i) The process gets never below the initial capital.
  ii) $\tau_1 <\infty$ but $C_{\tau_1}\ge 0$.
  iii) Ruin occurs at $\tau_1$.

Thus we get

$$
\psi(u)=\left(1-\frac{\lambda \mu}{c}\right)0+\frac{\lambda}{c}\int_0^u\psi(u-y)(1-G(y))\ dy+\frac{\lambda}{c}\int_u^\infty 1(1-G(y))\ dy.
$$

Let $\tau_0 = 0$ and $\tau_i = \inf\{t > \tau_{i−1} : C_t < C_{\tau_{i-1}}\}$, called the ladder times, and define $L_i = C_{\tau_{i−1}} − C_{\tau_i}$ , called the ladder heights. Note that $L_i$ only is defined if $\tau_i <\infty$. By Lemma 2.1 we find 

$$
\mathbb P[\tau_i<\infty\ \vert\ \tau_{i-1}<\infty]=\frac{\lambda \mu}{c}.
$$

Let $K = \sup\{i \in\mathbb N : \tau_i < \infty\}$ be the number of ladder epochs. One may show that

$$
K \sim \text{NB}\left(1, 1 − \frac{\lambda \mu}{c}\right)
$$

and that, given $K$, the random variables $(L_i : i \le K)$ are iid and absolutely continuous with density $(1 − G(x))/\mu$. We only have to condition on $K$ because $L_i$ is not defined for $i > K$. If we assume that all $\{L_i : i \ge 1\}$ have the same distribution, then we can drop the conditioning on $K$ and $\{L_i\}$ is independent of $K$. Then

$$
\inf\{C_t : t\ge 0\}=u-\sum_{i=1}^K L_i
$$

and

$$
\mathbb P[\tau <\infty]=\mathbb P[\inf\{C_t : t\ge 0\}<0]=\mathbb P\left[\sum_{i=1}^K L_i> u\right]
$$

Denote by

$$
B(x)=\frac{1}{\mu}\int_0^x(1-G(y))\ dy
$$

he distribution function of $L_i$. We can use Panjer recursion to approximate $\psi(u)$ by using an appropriate discretisation.

A formula that is useful for theoretical considerations, but too messy to use for the computation of $\psi(u)$ is the Pollaczek-Khintchine formula.
\begin{align*}
\psi(u)&=\mathbb P\left[\sum_{i=1}^K L_i> u\right]=\sum_{n=1}^\infty P\left[\sum_{i=1}^n L_i> u\right]\mathbb P[K=n]\\
&=\left(1 − \frac{\lambda \mu}{c}\right)\sum_{n=1}^\infty\left(\frac{\lambda \mu}{c}\right)^n(1-B^{*n}(u)).
\end{align*}

### The Laplace transform of $\psi$

<blockquote class = "def">

**Definition 2.10. (Schmidli) (Laplace transform)**\index{Laplace transform} _Let $f$ be a real valued function on $[0,\infty)$. The transform_

$$
\hat f(s):=\int_0^\infty e^{-sx}f(x)\ dx
$$

_for $s\in\mathbb R$ is called the **Laplace transform** of $f$._

</blockquote>

We want to find the Laplace transform of $\delta (u)$. We multiply

$$
\frac{d}{du}\delta(u)=\frac{\lambda}{c}\left[\delta(u)-\int_0^u\delta(u-y)\ dG(y)\right].
$$

with $e^{−su}$ and then integrate over $u$. Let $s > 0$.

$$
\frac{c}{\lambda}\int_0^\infty \delta'(u)e^{-su}\ du=\int_0^\infty e^{-su}\delta(u)\ du-\int_0^\infty e^{-su}\int_0^u\delta(u-y)\ dG(y)\ du.
$$

We have to determine the last integral.
\begin{align*}
\int_0^\infty e^{-su}\int_0^u\delta(u-y)\ dG(y)\ du&=\int_0^\infty\int_y^\infty\delta(u-y)e^{-su}\ du\ dG(y)\\
&=\int_0^\infty\int_0^\infty\delta(u)e^{-s(u+y)}\ du\ dG(y)\\
&=\hat \delta(s)M_Y(-s).
\end{align*}
Thus we get

$$
\frac{c}{\lambda}(s\hat \delta(s)-\delta(0))=\hat \delta(s)(1-M_Y(-s))
$$

rearranging

$$
\hat \delta(s)=\frac{s\delta(0)}{cs-\lambda(1-M_Y(-s))}=\frac{c-\lambda \mu}{cs-\lambda(1-M_Y(-s))}.
$$

The Laplace transform of $\psi$ can easily be found as

$$
\hat\psi(s)=\int_0^\infty(1-\delta(u))e^{-su}\ du=\frac{1}{s}-\hat\delta(s)
$$

hence

$$
\hat\psi(s)=\frac{1}{s}+\frac{\lambda \mu-c}{cs-\lambda(1-M_Y(-s))}.
$$

#### Determining moments of maximal loss of capital

Consider the variable

$$
Z=\sup\{u-C_t : t\ge 0\}\ \vert\ \tau_1<\infty
$$

or equivalently

$$
Z=\sup\left\{\sum_{i=1}^{N_t}Y_i-ct : t\ge 0\right\}\ \vert\ \tau_1<\infty
$$

that is the largest loss of capital given that $C_t<u$ for some $t>0$. Hence we obviously have that $Z>0$ almost surely. We know the distribution of $Z$:

$$
\mathbb P[Z\le x]=1-\frac{c}{\lambda \mu}\psi(x).
$$

Furthermore, the moment generating function is

$$
M_Z(r)=\int_0^\infty e^{ru}\frac{c}{\lambda \mu}\delta'(u)\ du=\frac{c}{\lambda \mu}\left(-r\frac{c-\lambda \mu}{-cr-\lambda(1-M_Y(r))}-\left(1-\frac{\lambda\mu}{c}\right)\right)
$$

that is 

$$
M_Z(r)=1-\frac{\lambda\mu}{c}+\frac{c(c-\lambda \mu)}{\lambda\mu}\frac{r}{cr-\lambda(M_Y(r)-1)}.
$$

Using derivatives of $M_Z$ we find that

$$
\mathbb E[Z]=\frac{c\mu_2}{2\mu(c-\lambda\mu)}
$$

and

$$
\mathbb E[Z^2]=\frac{c}{\mu}\left(\frac{\mu_3}{3(c-\lambda\mu)} + \frac{\lambda\mu_2^2}{2(c-\lambda\mu)^2}\right).
$$

To replicate recall that $M_Z^{(n)}(r)=\mathbb E[Z^ne^{rZ}]$ hence

$$
\left.M_Z^{(n)}(r)\right\vert_{r=0}=\mathbb E[Z^n].
$$

### Approximations to $\psi$

#### Diffusion approximations

Diffusion approximations are based on the following.

<blockquote class = "prop">

**Proposition 2.12. (Schmidli)** _Let $\{C_t^{(n)}\}$ be a sequence of Cramer-Lundberg processes with initial capital $u$, claim arrival intensities $\lambda^{(n)}=\lambda n$, claim size distributions $G^{(n)}(x)=G(x\sqrt n)$ and premium rates_

$$
c^{(n)}=\left(1 + \frac{c-\lambda \mu}{\lambda \mu\sqrt n}\right)\lambda^{(n)}\mu^{(n)}=c+(\sqrt n -1)\lambda\mu.
$$

_Let $\mu=\int_0^\infty y dG(y)$ and assume that $\mu_2 = \int_0^\infty y^2dG(y)<\infty$. Then_

$$
\left\{C_t^{(n)}\right\}\stackrel{d}{\to} \{u+ W_t\}
$$

_where $\{W_t\}$ is a $(c-\lambda \mu,\lambda\mu_2)$-Brownian motion._

</blockquote>

Intuitively we let the number of claims in a unit time interval go to infinity and make the claim sizes smaller in such a way that the distribution of $C^{(n)}_1 − u$ tends to a normal distribution and $\mathbb E [C^{(n)}_1 − u] = c − \lambda \mu$. Let $\tau^{(n)}$ denote the ruin time of $(C^{(n)}_t)$ and $\tau = \inf\{t \ge 0 : u+W_t < 0\}$ the ruin probability of the Brownian motion. Then

<blockquote class = "prop">

**Proposition 2.13. (Schmidli)** _Let $\{C_t^{(n)}\}$ and $\{W_t\}$ be given as above. Then_

$$
\lim_{n\to\infty} \mathbb P[\tau^{(n)}\le t]=\mathbb P[\tau \le t]
$$

_and_

$$
\lim_{n\to\infty} \mathbb P[\tau^{(n)}< \infty]=\mathbb P[\tau < \infty].
$$

</blockquote>

The idea of the diffusion approximation is to approximate $\mathbb P [\tau^{(1)}\le t]$ by $\mathbb P [\tau\le t]$ and $\mathbb P [\tau^{(1)} < \infty]$ by $\mathbb P [\tau < \infty]$. Thus we need the ruin probabilities of the Brownian motion.

<blockquote class = "lem">

**Lemma 2.14. (Schmidli)** _Let $\{W_t\}$ be a $(m,\mu_2)$-Brownian motion with $m > 0$ and $\tau = \inf\{t\ge 0 : u+W_t <0\}$. Then_

$$
\mathbb P[\tau<\infty]=e^{-2um/\eta^2}
$$

_and_

$$
\mathbb P[\tau\le t]=1-\Phi\left(\frac{mt+u}{\eta\sqrt t}\right)+e^{-2um/\eta^2}\Phi\left(\frac{mt-u}{\eta\sqrt t}\right).
$$

</blockquote>

Diffusion approximations only work well if $c/(\lambda \mu)$ is near 1. There also exist corrected diffusion approximations which work much better.

#### The deVylder approximation

In the case of exponentially distributed claim amounts we know the ruin probabilities explicitly. The idea of the deVylder approximation is to replace $\{C_t\}$ by $\{\tilde C_t\}$ where $\{\tilde Ct\}$ has exponentially distributed claim amounts and

$$
\mathbb E [(C_t-u)^k]=\mathbb E [(\tilde C_t-u)^k]
$$

for $k=1,2,3$. The first three centralised moments are
\begin{align*}
\mathbb E [C_t-u]&=(c-\lambda\mu)t=\left(\tilde c -\frac{\tilde \lambda}{\tilde \alpha}\right)t,\\
\text{Var}[C_t]=\text{Var}[u+ct-C_t]&=\lambda\mu_2t=\frac{2\tilde \lambda}{\tilde \alpha^2}t,\\
\mathbb E [(C_t-\mathbb E[C_t])^3]&=-\lambda\mu_3t=-\frac{6\tilde \lambda}{\tilde\alpha ^3}t.
\end{align*}
Thus the parameters are given by

$$
\tilde\alpha =\frac{3\mu_2}{\mu_3},\qquad\tilde\lambda=\frac{\lambda\mu_2\tilde \alpha^2}{2}=\frac{9\mu_2^3}{2\mu_3^2}\lambda
$$

and

$$
\tilde c=c-\lambda\mu+\frac{\tilde \lambda}{\tilde\alpha}=c-\lambda\mu+\frac{3\mu_2^2}{2\mu_3}\lambda.
$$

Thus the approximation to the probability of ultimate ruin is

$$
\psi(u)\approx \frac{\tilde \lambda}{\tilde\alpha\tilde c}e^{-\left(\tilde\alpha-\frac{\tilde \lambda}{\tilde c}\right)u}.
$$

There is also a formula for the probability of ruin within finite time. Let $\eta =\sqrt{\tilde \lambda/(\tilde \alpha\tilde c)}$. Then

$$
\psi(u,t)\approx \frac{\tilde \lambda}{\tilde\alpha\tilde c}e^{-\left(\tilde\alpha-\frac{\tilde \lambda}{\tilde c}\right)u}-\frac{1}{\pi}\int_0^\pi f(x)\ dx
$$

where

$$
f(x)=\eta\frac{e^{2\eta\tilde\alpha\tilde c t \cos x-(\tilde\alpha\tilde c + \tilde \lambda)t+\tilde\alpha u(\eta \cos x - 1)}}{1+\eta^2-2\eta \cos x}[\cos(\tilde\alpha u\eta\sin x)-\cos(\tilde \alpha u \eta \sin x + 2x)].
$$

#### The Beekman-Bowers approximation

Recall that

$$
F(u)=1-\frac{c}{\lambda \mu}\psi(u)
$$

is a distribution function and that

$$
\int_0^\infty z\ dF(z)=\frac{c\mu_2}{2\mu(c-\lambda\mu)}
$$

and that

$$
\int_0^\infty z^2\ dF(z)=\frac{c}{\mu}\left(\frac{\mu_3}{3(c-\lambda\mu)}+\frac{\lambda\mu_2^2}{2\mu(c-\lambda\mu)^2}\right).
$$

The idea is to approximate the distribution function $F$ by the distribution function $\tilde F(u)$ of a $\Gamma(\gamma,\alpha)$ distributed random variable such that the first two moments coincide. Thus the parameters $\gamma$ and $\alpha$ have to fulfill

$$
\frac{\gamma}{\alpha}=\frac{c\mu_2}{2\mu(c-\lambda \mu)}
$$

and

$$
\frac{\gamma(\gamma +1)}{\alpha^2}=\frac{c}{\mu}\left(\frac{\mu_3}{3(c-\lambda\mu)}+\frac{\lambda\mu_2^2}{2\mu(c-\lambda\mu)^2}\right).
$$

The Beekman-Bowers approximation to the ruin probability is

$$
\psi(u)=\frac{\lambda \mu}{c}(1-F(u))\approx \frac{\lambda\mu}{c}(1-\tilde F(u))
$$

### Subexponential claim size distributions

Let us now consider subexponential claim size distributions. In this case the Lund- berg exponent does not exist (Lemma 1.30).

<blockquote class = "thm">

**Theorem 2.15. (Schmidli)** _Assume that the ladder height distribution_

$$
\frac{1}{\mu}\int_0^x(1-G(y))\ dy
$$

_is subexponential. Then_

$$
\lim_{u\to\infty}\frac{\psi(u)}{\int_u^\infty (1-G(y))\ dy}=\frac{\lambda}{c-\lambda\mu}
$$

_or equivalently_

$$
\psi(u)\sim \frac{\lambda}{c-\lambda\mu}\int_u^\infty (1-G(y))\ dy.
$$

</blockquote>

Recall that the probability that ruin occurs at the first ladder time given there is a first ladder epoch

$$
\frac{1}{\mu}\int_u^\infty(1-G(y))\ dy.
$$

Hence the ruin probability is asymptotically $(c − \lambda\mu)^{-1}\lambda\mu$ times the probability of ruin at the first ladder time given there is a first ladder epoch. But $(c − \lambda\mu)^{-1}\lambda\mu$ is the expected number of ladder times. Intuitively for $u$ large ruin will occur if one of the ladder heights exceeds $u$.

The conditions of the theorem are also fulfilled for $\text{LN}(\mu,\sigma^2)$, for $\text{LG}(\gamma ,\alpha)$ and for $Wei(\alpha, c)$ $(\alpha < 1)$ distributed claims.

### The time of ruin

Consider the function

$$
f_\alpha(u)=\mathbb E[e^{-\alpha \tau}1_{\{\tau<\infty\}}\ \vert\ C_0=u].
$$

The function is defined at least for $\alpha\ge 0$. We will first find a differential equation for $f_\alpha(u)$.

<blockquote class = "lem">

**Lemma 2.17. (Schmidli)** _The function $f_\alpha(u)$ is absolutely continuous and fulfils the equation_

$$
cf_\alpha'(u)+\lambda\left[\int_0^u f_\alpha(u-y)\ dG(y) + 1 -G(u)-f_\alpha (u)\right] - \alpha f_\alpha(u)=0.
$$

</blockquote>

<blockquote class = "lem">

**Lemma 2.19. (Schmidli)** _Assume $\mu_2<\infty$. Then_

$$
\mathbb E[\tau 1_{\{\tau<\infty\}}]=\frac{1}{c-\lambda\mu}\left[\frac{\lambda\mu_2}{2(c-\lambda\mu)}\delta(u)-\int_0^u\psi(u-y)\delta(y)\ dy\right].
$$

</blockquote>

<blockquote class = "prop">

**Corollary 2.20. (Schmidli)** _Let $t>0$. Then_

$$
\mathbb P[t<\tau<\infty]<\frac{\lambda \mu_2}{2(c-\lambda\mu)^2t}.
$$

</blockquote>

### Seal's formulae

We consider now the probability of ruin within finite time $\psi(u, t)$. But first we want to find the conditional finite ruin probability given $C_t$ for some $t$ fixed.

<blockquote class = "lem">

**Lemma 2.21. (Schmidli)** _Let $t$ be fixed $u=0$ and $0< y\le ct$. Then_

$$
\mathbb P[C_s\ge 0,0\le s\le t\ \vert\ C_t=y]=\frac{y}{ct}.
$$

</blockquote>

Conditioning on $C_t$ is in fact a conditioning on the aggregate claim size $S_t$. Denote by $F(x; t)$ the distribution function of $S_t$ . For integration we use $dF (\cdot; t)$ for integration with respect to the measure $F (\cdot; t)$. Moreover, let $\delta(u, t) = 1 − \psi(u, t) = \mathbb P [\tau > t]$.

<blockquote class = "lem">

**Theorem 2.22. (Schmidli)** _For initial capital 0 we have_

$$
\delta (0,t)=\frac{1}{ct}\mathbb E[\min\{C_t,0\}]=\frac{1}{ct}\int_0^{ct}F(y;t)\ dy.
$$

</blockquote>

Let now the initial capital be arbitrary.

<blockquote class = "lem">

**Theorem 2.23. (Schmidli)** _With the notation used above we have for $u>0$_

$$
\delta (u,t)=F(u+ct; t)-\int_u^{u+ct}\delta\left(0,t-\frac{v-u}{c}\right) F\left(dv;\frac{v-u}{c}\right).
$$

</blockquote>

## The renewal risk model

### Definition of the renewal risk model

### The adjustment coefficient

### Lundberg's inequality

#### The ordinary case

#### The general case

### The Cramer-Lundberg approximation

#### The ordinary case

#### The general case

### Diffusion approximations

### Subexponential claim size distributions

### Finite time Lundberg inequalities




## Modern ruin theory

## Claims reserving